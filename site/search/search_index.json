{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#specklecn2profiler","title":"SpeckleCn2Profiler:","text":""},{"location":"#improving-satellite-communications-with-scidar-and-machine-learning","title":"Improving Satellite Communications with SCIDAR and Machine Learning","text":""},{"location":"#overview","title":"Overview","text":"<p>Optical satellite communications is a growing research field with bright commercial perspectives. One of the challenges for optical links through the atmosphere is turbulence, which is also apparent by the twinkling of stars. The reduction of the quality can be calculated, but it needs the turbulence strength over the path the optical beam is running. Estimation of the turbulence strength is done at astronomic sites, but not at rural or urban sites. To be able to do this, a simple instrument is required. We want to propose to use a single star Scintillation Detection and Ranging (SCIDAR), which is an instrument that can estimate the turbulence strength, based on the observation of a single star. Here, reliable signal processing of the received images of the star is most challenging. We propose to solve this by Machine Learning.</p>"},{"location":"#project-goals","title":"Project Goals","text":"<p>The primary objectives of this project are:</p> <ol> <li> <p>Turbulence Strength Estimation: Develop a robust algorithm using Machine Learning to estimate turbulence strength based on SCIDAR data.</p> </li> <li> <p>Signal Processing Enhancement: Implement advanced signal processing techniques to improve the accuracy and reliability of turbulence strength calculations.</p> </li> <li> <p>Adaptability to Various Sites: Ensure the proposed solution is versatile enough to be deployed in diverse environments, including rural and urban locations.</p> </li> </ol>"},{"location":"#repository-contents","title":"Repository Contents","text":"<p>This repository contains:</p> <ul> <li> <p>Machine Learning Models: Implementation of machine learning models tailored for turbulence strength estimation from SCIDAR data.</p> </li> <li> <p>Signal Processing Algorithms: Advanced signal processing algorithms aimed at enhancing the quality of received star images.</p> </li> <li> <p>Dataset: Sample datasets for training and testing the machine learning models.</p> </li> <li> <p>Documentation: In-depth documentation explaining the methodology, algorithms used, and guidelines for using the code.</p> </li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with the project, follow these steps:</p> <ol> <li> <p>Install the package: <pre><code>python -m pip install git+https://github.com/MALES-project/SpeckleCn2Profiler\n</code></pre> while the above command works, <code>speckcn2</code> will be available on pypi as soon as its dependencies get updated.</p> </li> <li> <p>Explore the Code:     Dive into the codebase to understand the implementation details and customize it according to your needs.</p> </li> </ol>"},{"location":"#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to improve and expand the capabilities of this project. If you have ideas, bug fixes, or enhancements, please submit a pull request. Check out our Contributing Guidelines to get started with development.</p>"},{"location":"#how-to-cite","title":"How to cite","text":"<p>Please consider citing this software that is published in Zenodo under the DOI 10.5281/zenodo.11447920.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at s.ciarella@esciencecenter.nl. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing guidelines","text":"<p>Welcome! SpeckCn2 is an open-source project for analysis of speckle patterns. If you're trying SpeckCn2 with your data, your experience, questions, bugs you encountered, and suggestions for improvement are important to the success of the project.</p> <p>We have a Code of Conduct, please follow it in all your interactions with the project.</p>"},{"location":"CONTRIBUTING/#questions-feedback-bugs","title":"Questions, feedback, bugs","text":"<p>Use the search function to see if someone else already ran accross the same issue. Feel free to open a new issue here to ask a question, suggest improvements/new features, or report any bugs that you ran into.</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting changes","text":"<p>Even better than a good bug report is a fix for the bug or the implementation of a new feature. We welcome any contributions that help improve the code.</p> <p>When contributing to this repository, please first discuss the change you wish to make via an issue with the owners of this repository before making a change.</p> <p>Contributions can come in the form of:</p> <ul> <li>Bug fixes</li> <li>New features</li> <li>Improvement of existing code</li> <li>Updates to the documentation</li> <li>... ?</li> </ul> <p>We use the usual GitHub pull-request flow. For more info see GitHub's own documentation.</p> <p>Typically this means:</p> <ol> <li>Forking the repository and/or make a new branch</li> <li>Making your changes</li> <li>Make sure that the tests pass and add your own</li> <li>Update the documentation is updated for new features</li> <li>Pushing the code back to Github</li> <li>Create a new Pull Request</li> </ol> <p>One of the code owners will review your code and request changes if needed. Once your changes have been approved, your contributions will become part of GEMDAT. \ud83c\udf89</p>"},{"location":"CONTRIBUTING/#getting-started-with-development","title":"Getting started with development","text":""},{"location":"CONTRIBUTING/#setup","title":"Setup","text":"<p>SpeckCn2 targets Python 3.9 or newer.</p> <p>Clone the repository into the <code>speckcn2</code> directory:</p> <pre><code>git clone https://github.com/MALES-project/SpeckleCn2Profiler speckcn2\n</code></pre> <p>Install using <code>virtualenv</code>:</p> <pre><code>cd speckcn2\npython3 -m venv env\nsource env/bin/activate\npython3 -m pip install -e .[develop]\n</code></pre> <p>Alternatively, install using Conda:</p> <pre><code>cd speckcn2\nconda create -n speckcn2 python=3.10\nconda activate speckcn2\npip install -e .[develop]\n</code></pre>"},{"location":"CONTRIBUTING/#running-tests","title":"Running tests","text":"<p>SpeckCn2 uses pytest to run the tests. You can run the tests for yourself using:</p> <pre><code>pytest\n</code></pre> <p>To check coverage:</p> <pre><code>coverage run -m pytest\ncoverage report  # to output to terminal\ncoverage html    # to generate html report\n</code></pre>"},{"location":"CONTRIBUTING/#building-the-documentation","title":"Building the documentation","text":"<p>The documentation is written in markdown, and uses mkdocs to generate the pages.</p> <p>To build the documentation for yourself:</p> <pre><code>pip install -e .[docs]\nmkdocs serve\n</code></pre> <p>You can find the documentation source in the docs directory. If you are adding new pages, make sure to update the listing in the <code>mkdocs.yml</code> under the <code>nav</code> entry.</p>"},{"location":"CONTRIBUTING/#making-a-release","title":"Making a release","text":"<ol> <li> <p>Make a new release.</p> </li> <li> <p>Under 'Choose a tag', set the tag to the new version. The versioning scheme we use is SemVer, so bump the version (major/minor/patch) as needed. Bumping the version is handled transparently by <code>bumpversion</code> in this workflow.</p> </li> <li> <p>The upload to pypi is triggered when a release is published and handled by this workflow.</p> </li> <li> <p>The upload to zenodo is triggered when a release is published.</p> </li> </ol>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#macos-m1-arm64","title":"MacOS M1 arm64","text":"<p>Some dependencies (e.g. <code>scikit</code>) do not support the latest python version (3.12). Also <code>py3nj</code>, a dependency of <code>escnn</code>, requires openmp. We've installed this via homebrew and thus explicitly specifying the C compiler (gnu) prior to installation of this package does the trick.</p> <pre><code>conda create -n speckcn2 python=3.10\nconda activate speckcn2\nCC=gcc-13 pip3 install py3nj # install py3nj before with gcc instead of clang\npip install -e .\n</code></pre>"},{"location":"api/api/","title":"speckcn2","text":"<ul> <li>speckcn2.mlops.train</li> <li>speckcn2.mlops.score</li> </ul>"},{"location":"api/io/","title":"I/O","text":""},{"location":"api/io/#speckcn2.io.load","title":"<code>load(model, datadirectory, epoch)</code>","text":"<p>Load the model state and the model itself.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to load</p> </li> <li> <code>datadirectory</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> <li> <code>epoch</code>               (<code>int</code>)           \u2013            <p>The epoch of the model</p> </li> </ul> Source code in <code>src/speckcn2/io.py</code> <pre><code>def load(model: torch.nn.Module, datadirectory: str, epoch: int) -&gt; None:\n    \"\"\"Load the model state and the model itself.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to load\n    datadirectory : str\n        The directory where the data is stored\n    epoch : int\n        The epoch of the model\n    \"\"\"\n\n    model_state = torch.load(\n        f'{datadirectory}/{model.name}_states/{model.name}_{epoch}.pth')\n\n    model.epoch = model_state['epoch']\n    model.loss = model_state['loss']\n    model.val_loss = model_state['val_loss']\n    model.time = model_state['time']\n    model.load_state_dict(model_state['model_state_dict'], strict=False)\n\n    assert model.epoch[\n        -1] == epoch, 'The epoch of the model is not the same as the one loaded'\n</code></pre>"},{"location":"api/io/#speckcn2.io.load_config","title":"<code>load_config(config_file_path)</code>","text":"<p>Load the configuration file.</p> <p>Parameters:</p> <ul> <li> <code>config_file_path</code>               (<code>str</code>)           \u2013            <p>Path to the .yaml configuration file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>config</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the configuration</p> </li> </ul> Source code in <code>src/speckcn2/io.py</code> <pre><code>def load_config(config_file_path: str) -&gt; dict:\n    \"\"\"Load the configuration file.\n\n    Parameters\n    ----------\n    config_file_path : str\n        Path to the .yaml configuration file\n\n    Returns\n    -------\n    config : dict\n        Dictionary containing the configuration\n    \"\"\"\n    with open(config_file_path, 'r') as file:\n        config = yaml.safe_load(file)\n    return config\n</code></pre>"},{"location":"api/io/#speckcn2.io.load_model_state","title":"<code>load_model_state(model, datadirectory)</code>","text":"<p>Loads the model state from the given directory.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to load the state into</p> </li> <li> <code>datadirectory</code>               (<code>str</code>)           \u2013            <p>The directory where the model states are stored</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>The model with the loaded state</p> </li> <li> <code>last_model_state</code> (              <code>int</code> )          \u2013            <p>The number of the last model state</p> </li> </ul> Source code in <code>src/speckcn2/io.py</code> <pre><code>def load_model_state(model: torch.nn.Module,\n                     datadirectory: str) -&gt; tuple[torch.nn.Module, int]:\n    \"\"\"Loads the model state from the given directory.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to load the state into\n    datadirectory : str\n        The directory where the model states are stored\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The model with the loaded state\n    last_model_state : int\n        The number of the last model state\n    \"\"\"\n\n    # Print model informations\n    print(model)\n    model.nparams = sum(p.numel() for p in model.parameters())\n    print(f'\\n--&gt; Nparams = {model.nparams}')\n\n    ensure_directory(f'{datadirectory}/{model.name}_states')\n\n    # check what is the last model state\n    try:\n        last_model_state = sorted([\n            int(file_name.split('.pth')[0].split('_')[-1])\n            for file_name in os.listdir(f'{datadirectory}/{model.name}_states')\n        ])[-1]\n    except Exception as e:\n        print(f'Warning: {e}')\n        last_model_state = 0\n\n    if last_model_state &gt; 0:\n        print(f'Loading model at epoch {last_model_state}')\n        load(model, datadirectory, last_model_state)\n        return model, last_model_state\n    else:\n        print('No pretrained model to load')\n\n        # Initialize some model state measure\n        model.loss = []\n        model.val_loss = []\n        model.time = []\n        model.epoch = []\n\n        return model, 0\n</code></pre>"},{"location":"api/io/#speckcn2.io.save","title":"<code>save(model, datadirectory)</code>","text":"<p>Save the model state and the model itself.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to save</p> </li> <li> <code>epoch</code>               (<code>int</code>)           \u2013            <p>The epoch of the model</p> </li> <li> <code>datadirectory</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> </ul> Source code in <code>src/speckcn2/io.py</code> <pre><code>def save(model: torch.nn.Module, datadirectory: str) -&gt; None:\n    \"\"\"Save the model state and the model itself.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to save\n    epoch : int\n        The epoch of the model\n    datadirectory : str\n        The directory where the data is stored\n    \"\"\"\n\n    model_state = {\n        'epoch': model.epoch,\n        'loss': model.loss,\n        'val_loss': model.val_loss,\n        'time': model.time,\n        'model_state_dict': model.state_dict(),\n    }\n\n    torch.save(\n        model_state,\n        f'{datadirectory}/{model.name}_states/{model.name}_{model.epoch[-1]}.pth'\n    )\n</code></pre>"},{"location":"api/mlops/","title":"MlOps","text":""},{"location":"api/mlops/#speckcn2.mlops.score","title":"<code>score(model, test_set, device, criterion, normalizer, nimg_plot=100)</code>","text":"<p>Tests the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to test</p> </li> <li> <code>test_set</code>               (<code>list</code>)           \u2013            <p>The testing set</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> <li> <code>criterion</code>               (<code>ComposableLoss</code>)           \u2013            <p>The composable loss function, where I can access useful parameters</p> </li> <li> <code>normalizer</code>               (<code>Normalizer</code>)           \u2013            <p>The normalizer used to recover the tags</p> </li> <li> <code>nimg_plot</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of images to plot</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>test_tags</code> (              <code>list</code> )          \u2013            <p>List of all the predicted tags of the test set</p> </li> <li> <code>test_losses</code> (              <code>list</code> )          \u2013            <p>List of all the losses of the test set</p> </li> <li> <code>test_measures</code> (              <code>list</code> )          \u2013            <p>List of all the measures of the test set</p> </li> <li> <code>test_cn2_pred</code> (              <code>list</code> )          \u2013            <p>List of all the predicted Cn2 profiles of the test set</p> </li> <li> <code>test_cn2_true</code> (              <code>list</code> )          \u2013            <p>List of all the true Cn2 profiles of the test set</p> </li> <li> <code>test_recovered_tag_pred</code> (              <code>list</code> )          \u2013            <p>List of all the recovered tags from the model prediction</p> </li> <li> <code>test_recovered_tag_true</code> (              <code>list</code> )          \u2013            <p>List of all the recovered tags</p> </li> </ul> Source code in <code>src/speckcn2/mlops.py</code> <pre><code>def score(\n        model: nn.Module,\n        test_set: list,\n        device: torch.device,\n        criterion: ComposableLoss,\n        normalizer: Normalizer,\n        nimg_plot: int = 100\n) -&gt; tuple[list, list, list, list, list, list, list]:\n    \"\"\"Tests the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to test\n    test_set : list\n        The testing set\n    device : torch.device\n        The device to use\n    criterion : ComposableLoss\n        The composable loss function, where I can access useful parameters\n    normalizer : Normalizer\n        The normalizer used to recover the tags\n    nimg_plot : int\n        Number of images to plot\n\n    Returns\n    -------\n    test_tags : list\n        List of all the predicted tags of the test set\n    test_losses : list\n        List of all the losses of the test set\n    test_measures : list\n        List of all the measures of the test set\n    test_cn2_pred : list\n        List of all the predicted Cn2 profiles of the test set\n    test_cn2_true : list\n        List of all the true Cn2 profiles of the test set\n    test_recovered_tag_pred : list\n        List of all the recovered tags from the model prediction\n    test_recovered_tag_true : list\n        List of all the recovered tags\n    \"\"\"\n    counter = 0\n    conf = normalizer.conf\n    data_dir = conf['speckle']['datadirectory']\n    batch_size = conf['hyppar']['batch_size']\n    # Setup the EnsembleModel wrapper\n    ensemble = EnsembleModel(conf, device)\n\n    # For scoring the model, I enforce to use with the same weights:\n    # 1. MAE on screen tags\n    # 2. Fried MAE\n    # 3. Isoplanatic angle MAE\n    # 4. Scintillation (weak) index MAE\n    # TODO: this should be controllable in the configuration file\n    criterion.loss_weights = {\n        'MSE': 1,\n        'MAE': 1,\n        'JMSE': 0,\n        'JMAE': 0,\n        'Cn2MSE': 0,\n        'Cn2MAE': 0,\n        'Pearson': 0,\n        'Fried': 1,\n        'Isoplanatic': 1,\n        'Rytov': 0,\n        'Scintillation_w': 1,\n        'Scintillation_ms': 0,\n    }\n    criterion._select_loss_needed()\n\n    with torch.no_grad():\n        # Put model in evaluation mode\n        model.eval()\n\n        test_tags = []\n        test_losses = []\n        test_measures = []\n        test_cn2_pred = []\n        test_cn2_true = []\n        test_recovered_tag_pred = []\n        test_recovered_tag_true = []\n        # create the directory where the images will be stored\n        ensure_directory(f'{data_dir}/{model.name}_score')\n\n        for idx in range(0, len(test_set), batch_size):\n            batch = test_set[idx:idx + batch_size]\n\n            # Forward pass\n            outputs, targets, inputs = ensemble(model, batch)\n\n            # Loop each input separately\n            for i in range(len(outputs)):\n                loss, losses = criterion(outputs[i], targets[i])\n                # Print the loss for every epoch\n                print(f'Item {counter} loss: {loss.item():.4f}')\n\n                # Get the Cn2 profile and the recovered tags\n                Cn2_pred = criterion.reconstruct_cn2(outputs[i])\n                Cn2_true = criterion.reconstruct_cn2(targets[i])\n                recovered_tag_pred = criterion.get_J(outputs[i])\n                recovered_tag_true = criterion.get_J(targets[i])\n                # and get all the measures\n                all_measures = criterion._get_all_measures(\n                    outputs[i], targets[i], Cn2_pred, Cn2_true)\n\n                if counter &lt; nimg_plot:\n                    score_plot(conf, inputs, targets, loss, losses, i, counter,\n                               all_measures, Cn2_pred, Cn2_true,\n                               recovered_tag_pred, recovered_tag_true)\n\n                # and get all the tags for statistic analysis\n                for tag in outputs:\n                    test_tags.append(tag)\n\n                # and get the other information\n                test_losses.append(losses)\n                test_measures.append(all_measures)\n                test_cn2_pred.append(Cn2_pred)\n                test_cn2_true.append(Cn2_true)\n                test_recovered_tag_pred.append(recovered_tag_pred)\n                test_recovered_tag_true.append(recovered_tag_true)\n\n                counter += 1\n\n    return (test_tags, test_losses, test_measures, test_cn2_pred,\n            test_cn2_true, test_recovered_tag_pred, test_recovered_tag_true)\n</code></pre>"},{"location":"api/mlops/#speckcn2.mlops.train","title":"<code>train(model, last_model_state, conf, train_set, test_set, device, optimizer, criterion)</code>","text":"<p>Trains the model for the given number of epochs.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to train</p> </li> <li> <code>last_model_state</code>               (<code>int</code>)           \u2013            <p>The number of the last model state</p> </li> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>train_set</code>               (<code>list</code>)           \u2013            <p>The training set</p> </li> <li> <code>test_set</code>               (<code>list</code>)           \u2013            <p>The testing set</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> <li> <code>optimizer</code>               (<code>optim</code>)           \u2013            <p>The optimizer to use</p> </li> <li> <code>criterion</code>               (<code>ComposableLoss</code>)           \u2013            <p>The loss function to use</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>The trained model</p> </li> <li> <code>average_loss</code> (              <code>float</code> )          \u2013            <p>The average loss of the last epoch</p> </li> </ul> Source code in <code>src/speckcn2/mlops.py</code> <pre><code>def train(model: nn.Module, last_model_state: int, conf: dict, train_set: list,\n          test_set: list, device: torch.device, optimizer: optim.Optimizer,\n          criterion: ComposableLoss) -&gt; tuple[nn.Module, float]:\n    \"\"\"Trains the model for the given number of epochs.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to train\n    last_model_state : int\n        The number of the last model state\n    conf : dict\n        Dictionary containing the configuration\n    train_set : list\n        The training set\n    test_set : list\n        The testing set\n    device : torch.device\n        The device to use\n    optimizer : torch.optim\n        The optimizer to use\n    criterion : ComposableLoss\n        The loss function to use\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The trained model\n    average_loss : float\n        The average loss of the last epoch\n    \"\"\"\n\n    final_epoch = conf['hyppar']['maxepochs']\n    save_every = conf['model']['save_every']\n    datadirectory = conf['speckle']['datadirectory']\n    batch_size = conf['hyppar']['batch_size']\n\n    # Setup the EnsembleModel wrapper\n    ensemble = EnsembleModel(conf, device)\n\n    print(f'Training the model from epoch {last_model_state} to {final_epoch}')\n    average_loss = 0.0\n    model.train()\n    for epoch in range(last_model_state, final_epoch):\n        total_loss = 0.0\n        model.train()\n        t_in = time.time()\n        for i in range(0, len(train_set), batch_size):\n            batch = train_set[i:i + batch_size]\n\n            # Zero out the optimizer\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs, targets, _ = ensemble(model, batch)\n            loss, _ = criterion(outputs, targets)\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n            # Accumulate the loss\n            total_loss += loss.item()\n\n        # Suffle the training set\n        random.shuffle(train_set)\n\n        # Calculate average loss for the epoch\n        average_loss = total_loss / len(train_set)\n\n        # Log the important information\n        t_fin = time.time() - t_in\n        model.loss.append(average_loss)\n        model.time.append(t_fin)\n        model.epoch.append(epoch + 1)\n\n        # And also the validation loss\n        val_loss = 0.0\n        model.eval()\n        with torch.no_grad():\n            for i in range(0, len(test_set), batch_size):\n                batch = test_set[i:i + batch_size]\n                # Forward pass\n                outputs, targets, _ = ensemble(model, batch)\n                loss, _ = criterion(outputs, targets)\n                # sum loss\n                val_loss += loss.item()\n        val_loss = val_loss / len(test_set)\n        model.val_loss.append(val_loss)\n\n        # Print the average loss for every epoch\n        message = (f'Epoch {epoch+1}/{final_epoch} '\n                   f'(in {t_fin:.3g}s),\\tTrain-Loss: {average_loss:.5f},\\t'\n                   f'Test-Loss: {val_loss:.5f}')\n        print(message, flush=True)\n\n        if (epoch + 1) % save_every == 0 or epoch == final_epoch - 1:\n            # Save the model state\n            save(model, datadirectory)\n\n    return model, average_loss\n</code></pre>"},{"location":"api/models/","title":"Models","text":""},{"location":"api/models/#speckcn2.mlmodels.EnsembleModel","title":"<code>EnsembleModel(conf, device)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper that allows any model to be used for ensembled data.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>The global configuration containing the model parameters.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def __init__(self, conf: dict, device: torch.device):\n    \"\"\"Initializes the EnsembleModel.\n\n    Parameters\n    ----------\n    conf: dict\n        The global configuration containing the model parameters.\n    device : torch.device\n        The device to use\n    \"\"\"\n    super(EnsembleModel, self).__init__()\n\n    self.ensemble_size = conf['preproc'].get('ensemble', 1)\n    self.device = device\n    self.uniform_ensemble = conf['preproc'].get('ensemble_unif', False)\n    resolution = conf['preproc']['resize']\n    self.D = conf['noise']['D']\n    self.t = conf['noise']['t']\n    self.snr = conf['noise']['snr']\n    self.dT = conf['noise']['dT']\n    self.dO = conf['noise']['dO']\n    self.rn = conf['noise']['rn']\n    self.fw = conf['noise']['fw']\n    self.bit = conf['noise']['bit']\n    self.discretize = conf['noise']['discretize']\n    self.mask_D, self.mask_d, self.mask_X, self.mask_Y = self.create_masks(\n        resolution)\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.EnsembleModel.apply_noise","title":"<code>apply_noise(image_tensor)</code>","text":"<p>Processes a tensor of 2D images.</p> <p>Parameters:</p> <ul> <li> <code>image_tensor</code>               (<code>Tensor</code>)           \u2013            <p>Tensor of 2D images with shape (batch, channels, width, height).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>processed_tensor</code> (              <code>Tensor</code> )          \u2013            <p>Tensor of processed 2D images.</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def apply_noise(self, image_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Processes a tensor of 2D images.\n\n    Parameters\n    ----------\n    image_tensor : torch.Tensor\n        Tensor of 2D images with shape (batch, channels, width, height).\n\n    Returns\n    -------\n    processed_tensor : torch.Tensor\n        Tensor of processed 2D images.\n    \"\"\"\n    batch, channels, height, width = image_tensor.shape\n    processed_tensor = torch.zeros_like(image_tensor)\n\n    # Normalize wrt optical power\n    image_tensor = image_tensor / torch.mean(\n        image_tensor, dim=(2, 3), keepdim=True)\n\n    amp = self.rn * 10**(self.snr / 20)\n\n    for i in range(batch):\n        for j in range(channels):\n            B = image_tensor[i, j]\n\n            # Apply masks\n            B[self.mask_D] = 0\n            B[self.mask_d] = 0\n            B[self.mask_X] = 0\n            B[self.mask_Y] = 0\n\n            # Add noise sources\n            A = self.rn + self.rn * torch.randn(\n                height, width, device=self.device) + amp * B + torch.sqrt(\n                    amp * B) * torch.randn(\n                        height, width, device=self.device)\n\n            # Make a discretized version\n            if self.discretize == 'on':\n                C = torch.round(A / self.fw * 2**self.bit)\n                C[A &gt; self.fw] = self.fw\n                C[A &lt; 0] = 0\n            else:\n                C = A\n\n            processed_tensor[i, j] = C\n\n    return processed_tensor\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.EnsembleModel.create_masks","title":"<code>create_masks(resolution)</code>","text":"<p>Creates the masks for the circular aperture and the spider.</p> <p>Parameters:</p> <ul> <li> <code>resolution</code>               (<code>int</code>)           \u2013            <p>Resolution of the images.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mask_D</code> (              <code>Tensor</code> )          \u2013            <p>Mask for the circular aperture.</p> </li> <li> <code>mask_d</code> (              <code>Tensor</code> )          \u2013            <p>Mask for the central obscuration.</p> </li> <li> <code>mask_X</code> (              <code>Tensor</code> )          \u2013            <p>Mask for the horizontal spider.</p> </li> <li> <code>mask_Y</code> (              <code>Tensor</code> )          \u2013            <p>Mask for the vertical spider.</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def create_masks(\n    self, resolution: int\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Creates the masks for the circular aperture and the spider.\n\n    Parameters\n    ----------\n    resolution : int\n        Resolution of the images.\n\n    Returns\n    -------\n    mask_D : torch.Tensor\n        Mask for the circular aperture.\n    mask_d : torch.Tensor\n        Mask for the central obscuration.\n    mask_X : torch.Tensor\n        Mask for the horizontal spider.\n    mask_Y : torch.Tensor\n        Mask for the vertical spider.\n    \"\"\"\n    # Coordinates\n    x = torch.linspace(-1, 1, resolution, device=self.device)\n    X, Y = torch.meshgrid(x, x, indexing='ij')  # XY grid\n    d = self.dO * self.D  # Diameter obscuration\n\n    R = torch.sqrt(X**2 + Y**2)\n\n    # Masking image\n    mask_D = R &gt; self.D\n    mask_d = R &lt; d\n    mask_X = torch.abs(X) &lt; self.t\n    mask_Y = torch.abs(Y) &lt; self.t\n\n    return mask_D, mask_d, mask_X, mask_Y\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.EnsembleModel.forward","title":"<code>forward(model, batch_ensemble)</code>","text":"<p>Forward pass through the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to use</p> </li> <li> <code>batch_ensemble</code>               (<code>list</code>)           \u2013            <p>Each element is a batch of an ensemble of samples.</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def forward(self, model, batch_ensemble):\n    \"\"\"Forward pass through the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to use\n    batch_ensemble : list\n        Each element is a batch of an ensemble of samples.\n    \"\"\"\n\n    if self.ensemble_size == 1:\n        batch = batch_ensemble\n        # If no ensembling, each element of the batch is a tuple (image, tag, ensemble_id)\n        images, tags, ensembles = zip(*batch)\n        images = torch.stack(images).to(self.device)\n        images = self.apply_noise(images)\n        tags = torch.tensor(np.stack(tags)).to(self.device)\n\n        return model(images), tags, images\n    else:\n        batch = list(itertools.chain(*batch_ensemble))\n        # Like the ensemble=1 case, I can process independently each element of the batch\n        images, tags, ensembles = zip(*batch)\n        images = torch.stack(images).to(self.device)\n        images = self.apply_noise(images)\n        tags = torch.tensor(np.stack(tags)).to(self.device)\n\n        model_output = model(images)\n\n        # To average the self.ensemble_size outputs of the model I extract the confidence weights\n        predictions = model_output[:, :-1]\n        weights = model_output[:, -1]\n        if self.uniform_ensemble:\n            weights = torch.ones_like(weights)\n        # multiply the prediction by the weights\n        weighted_predictions = predictions * weights.unsqueeze(-1)\n        # and sum over the ensembles\n        weighted_predictions = weighted_predictions.view(\n            model_output.size(0) // self.ensemble_size, self.ensemble_size,\n            -1).sum(dim=1)\n        # then normalize by the sum of the weights\n        sum_weights = weights.view(\n            weights.size(0) // self.ensemble_size,\n            self.ensemble_size).sum(dim=1)\n        ensemble_output = weighted_predictions / sum_weights.unsqueeze(-1)\n\n        # and get the tags and ensemble_id of the first element of the ensemble\n        tags = tags[::self.ensemble_size]\n        ensembles = ensembles[::self.ensemble_size]\n\n        return ensemble_output, tags, images\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.get_a_resnet","title":"<code>get_a_resnet(config)</code>","text":"<p>Returns a pretrained ResNet model, with the last layer corresponding to the number of screens.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>The model with the loaded state</p> </li> <li> <code>last_model_state</code> (              <code>int</code> )          \u2013            <p>The number of the last model state</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def get_a_resnet(config: dict) -&gt; tuple[nn.Module, int]:\n    \"\"\"Returns a pretrained ResNet model, with the last layer corresponding to\n    the number of screens.\n\n    Parameters\n    ----------\n    config : dict\n        Dictionary containing the configuration\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The model with the loaded state\n    last_model_state : int\n        The number of the last model state\n    \"\"\"\n\n    model_name = config['model']['name']\n    model_type = config['model']['type']\n    pretrained = config['model']['pretrained']\n    nscreens = config['speckle']['nscreens']\n    data_directory = config['speckle']['datadirectory']\n    ensemble = config['preproc'].get('ensemble', 1)\n\n    if model_type == 'resnet18':\n        model = torchvision.models.resnet18(\n            weights='IMAGENET1K_V1' if pretrained else None)\n        finaloutsize = 512\n    elif model_type == 'resnet50':\n        model = torchvision.models.resnet50(\n            weights='IMAGENET1K_V2' if pretrained else None)\n        finaloutsize = 2048\n    elif model_type == 'resnet152':\n        model = torchvision.models.resnet152(\n            weights='IMAGENET1K_V2' if pretrained else None)\n        finaloutsize = 2048\n    else:\n        raise ValueError(f'Unknown model {model_type}')\n\n    # If the model uses multiple images as input,\n    # add an extra channel as confidence weight\n    # to average the final prediction\n    if ensemble &gt; 1:\n        nscreens = nscreens + 1\n\n    # Give it its name\n    model.name = model_name\n\n    # Change the model to process black and white input\n    model.conv1 = torch.nn.Conv2d(1,\n                                  64,\n                                  kernel_size=(7, 7),\n                                  stride=(2, 2),\n                                  padding=(3, 3),\n                                  bias=False)\n    # Add a final fully connected piece to predict the output\n    model.fc = create_final_block(config, finaloutsize, nscreens)\n\n    return load_model_state(model, data_directory)\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.get_scnn","title":"<code>get_scnn(config)</code>","text":"<p>Returns a pretrained Spherical-CNN model, with the last layer corresponding to the number of screens.</p> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def get_scnn(config: dict) -&gt; tuple[nn.Module, int]:\n    \"\"\"Returns a pretrained Spherical-CNN model, with the last layer\n    corresponding to the number of screens.\"\"\"\n\n    model_name = config['model']['name']\n    model_type = config['model']['type']\n    datadirectory = config['speckle']['datadirectory']\n\n    model_map = {\n        'scnnC8': 'C8',\n        'scnnC16': 'C16',\n        'scnnC4': 'C4',\n        'scnnC6': 'C6',\n        'scnnC10': 'C10',\n        'scnnC12': 'C12',\n    }\n    try:\n        scnn_model = SteerableCNN(config, model_map[model_type])\n    except KeyError:\n        raise ValueError(f'Unknown model {model_type}')\n\n    scnn_model.name = model_name\n\n    return load_model_state(scnn_model, datadirectory)\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.setup_model","title":"<code>setup_model(config)</code>","text":"<p>Returns the model specified in the configuration file, with the last layer corresponding to the number of screens.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>The model with the loaded state</p> </li> <li> <code>last_model_state</code> (              <code>int</code> )          \u2013            <p>The number of the last model state</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def setup_model(config: dict) -&gt; tuple[nn.Module, int]:\n    \"\"\"Returns the model specified in the configuration file, with the last\n    layer corresponding to the number of screens.\n\n    Parameters\n    ----------\n    config : dict\n        Dictionary containing the configuration\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The model with the loaded state\n    last_model_state : int\n        The number of the last model state\n    \"\"\"\n\n    model_name = config['model']['name']\n    model_type = config['model']['type']\n\n    print(f'^^^ Initializing model {model_name} of type {model_type}')\n\n    if model_type.startswith('resnet'):\n        return get_a_resnet(config)\n    elif model_type.startswith('scnnC'):\n        return get_scnn(config)\n    else:\n        raise ValueError(f'Unknown model {model_name}')\n</code></pre>"}]}