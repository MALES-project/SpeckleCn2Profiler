{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#specklecn2profiler","title":"SpeckleCn2Profiler:","text":""},{"location":"#improving-satellite-communications-with-scidar-and-machine-learning","title":"Improving Satellite Communications with SCIDAR and Machine Learning","text":""},{"location":"#overview","title":"Overview","text":"<p>Optical satellite communications is a growing research field with bright commercial perspectives. One of the challenges for optical links through the atmosphere is turbulence, which is also apparent by the twinkling of stars. The reduction of the quality can be calculated, but it needs the turbulence strength over the path the optical beam is running. Estimation of the turbulence strength is done at astronomic sites, but not at rural or urban sites. To be able to do this, a simple instrument is required. We want to propose to use a single star Scintillation Detection and Ranging (SCIDAR), which is an instrument that can estimate the turbulence strength, based on the observation of a single star. In this setting, reliable signal processing of the received images of the star is most challenging. We propose to solve this by Machine Learning.</p>"},{"location":"#statement-of-need","title":"Statement of Need","text":"<p><code>SpeckleCn2Profiler</code> addresses the challenge of atmospheric turbulence profiling for optical satellite communications using machine learning-based analysis of SCIDAR data. The primary target users include:</p> <ul> <li>Aerospace engineers working on free-space optical communication systems</li> <li>Atmospheric scientists studying turbulence profiles and optical propagation</li> <li>Astronomers characterizing atmospheric seeing conditions</li> <li>Researchers in adaptive optics and atmospheric characterization</li> </ul> <p>While traditional SCIDAR instruments require complex signal processing, <code>SpeckleCn2Profiler</code> provides a machine learning-based approach that enables simpler, more accessible turbulence profiling. Unlike classical methods that rely on analytical inversion techniques, our ML-based approach can handle noisy data more robustly and provides uncertainty quantification through ensemble predictions.</p>"},{"location":"#repository-contents","title":"Repository Contents","text":"<p>This repository contains the workflow to implement and train machine learning models for turbulence strength estimation from SCIDAR data. Extensive Documentation is available to explain the methodology, algorithms used, and guidelines for using the code.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with the project, follow these steps:</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>To correctly install <code>speckcn2</code>, you need Python 3.9 or higher. If you don't have it installed, you can download it from the official website.</p> <p>You will also need header files required to compile Python extensions (contained in <code>python3-dev</code>):</p> <p>Ubuntu/Debian: <pre><code>sudo apt-get install python3-dev\n</code></pre></p> <p>Other systems: <pre><code># Fedora/RHEL\nsudo dnf install python3-devel\n\n# macOS\nxcode-select --install\n\n# Windows: Headers are included with the official Python installer\n</code></pre></p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#option-1-install-from-pypi-recommended","title":"Option 1: Install from PyPI (Recommended)","text":"<pre><code>python -m pip install speckcn2\n</code></pre> <p>We strongly recommend using a virtual environment:</p> <pre><code># Create virtual environment\npython -m venv speckcn2-env\n\n# Activate virtual environment\n# On Linux/macOS:\nsource speckcn2-env/bin/activate\n# On Windows:\n# speckcn2-env\\Scripts\\activate\n\n# Install the package\npython -m pip install speckcn2\n</code></pre> <p>Verify installation: <pre><code>python -c \"import speckcn2; print('Installation successful!')\"\n</code></pre></p>"},{"location":"#option-2-development-installation","title":"Option 2: Development Installation","text":"<p>For advanced users and developers who want to modify the code:</p> <pre><code>git clone https://github.com/MALES-project/SpeckleCn2Profiler.git\ncd SpeckleCn2Profiler\ngit submodule init\ngit submodule update\npip install -e .\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>To use the package, you run the commands such as:</p> <pre><code>python &lt;mycode.py&gt; &lt;path_to_config.yml&gt;\n</code></pre> <p>where <code>&lt;mycode.py&gt;</code> is the name of the script that trains/uses the <code>speckcn2</code> model and <code>&lt;path_to_config.yml&gt;</code> is the path to the configuration file.</p> <p>Here you can find a typical example run and an explanation of all the main configuration parameter. In the example submodule you can find multiple examples and multiple configuration to take inspiration from.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<p>Here's a minimal working example to train a model on sample data:</p> <pre><code>import speckcn2 as sp2\nimport torch\n\n# Load configuration\nconfig = sp2.load_config('path/to/config.yaml')\n\n# Prepare data\nall_images, all_tags, all_ensemble_ids = sp2.prepare_data(config)\n\n# Normalize tags\nnz = sp2.Normalizer(config)\n\n# Split data\ntrain_set, test_set = sp2.train_test_split(all_images, all_tags,\n                                            all_ensemble_ids, nz)\n\n# Setup model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel, last_model_state = sp2.setup_model(config)\nmodel = model.to(device)\n\n# Define loss and optimizer\ncriterion = sp2.ComposableLoss(config, nz, device)\noptimizer = sp2.setup_optimizer(config, model)\n\n# Train\nmodel, avg_loss = sp2.train(model, last_model_state, config,\n                            train_set, test_set, device,\n                            optimizer, criterion, criterion)\nprint(f'Training complete. Final loss: {avg_loss:.5f}')\n</code></pre> <p>Sample data and configuration files are available in the examples/ directory.</p>"},{"location":"#what-can-we-predict","title":"What can we predict?","text":"<p>A machine learning model trained using <code>speckcn2</code> can predict:</p>"},{"location":"#1-instantaneous-turbulence-strength","title":"1. Instantaneous turbulence strength","text":"<p> Given a speckle pattern, the model can predict the instantaneous turbulence strength and also provide an uncertainty estimate if more patterns are available.</p>"},{"location":"#2-parameters-estimation","title":"2. Parameters estimation","text":"<p>The model can also estimate important parameters that are useful for the analysis of the speckle pattern. At the moment we support: * Fried parameter <code>r0</code> * Isoplanatic angle <code>\u03b80</code> * Rytov Index <code>\u03c3</code></p> <p>We also provide histograms of the estimated parameters and the error of the estimation.</p>"},{"location":"#running-tests","title":"Running Tests","text":"<p>To verify your installation and ensure everything works correctly, you can run the test suite using pytest:</p> <pre><code>pytest\n</code></pre> <p>Note: Some tests may fail on first run because test data needs to be set up. If this happens, run:</p> <pre><code>python ./scripts/setup_test.py\npytest\n</code></pre> <p>For more details on testing and development, see our Contributing Guidelines.</p>"},{"location":"#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to improve and expand the capabilities of this project. If you have ideas, bug fixes, or enhancements, please submit a pull request. Check out our Contributing Guidelines to get started with development.</p>"},{"location":"#generative-ai-disclaimer","title":"Generative-AI Disclaimer","text":"<p>Parts of the code have been generated and/or refined using GitHub Copilot. All AI-output has been verified for correctness, accuracy and completeness, revised where needed, and approved by the author(s).</p>"},{"location":"#how-to-cite","title":"How to cite","text":"<p>Please consider citing this software that is published in Zenodo under the DOI 10.5281/zenodo.11447920.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache 2.0 License - see the LICENSE file for details.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at s.ciarella@esciencecenter.nl. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing guidelines","text":"<p>Welcome! SpeckCn2 is an open-source project for the analysis of speckle patterns. If you're trying SpeckCn2 with your data, your experience, questions, bugs you encountered, and suggestions for improvement are important to the success of the project.</p> <p>We have a Code of Conduct, please follow it in all your interactions with the project.</p>"},{"location":"CONTRIBUTING/#questions-feedback-bugs","title":"Questions, feedback, bugs","text":"<p>Use the search function to see if someone else already ran across the same issue. Feel free to open a new issue here to ask a question, suggest improvements/new features, or report any bugs that you ran into.</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting changes","text":"<p>Even better than a good bug report is a fix for the bug or the implementation of a new feature. We welcome any contributions that help improve the code.</p> <p>When contributing to this repository, please first discuss the change you wish to make via an issue with the owners of this repository before making a change.</p> <p>Contributions can come in the form of:</p> <ul> <li>Bug fixes</li> <li>New features</li> <li>Improvement of existing code</li> <li>Updates to the documentation</li> <li>... ?</li> </ul> <p>We use the usual GitHub pull request flow. For more info see GitHub's own documentation.</p> <p>Typically this means:</p> <ol> <li>Forking the repository and/or make a new branch</li> <li>Making your changes</li> <li>Make sure that the tests pass and add your own</li> <li>Update the documentation is updated for new features</li> <li>Pushing the code back to Github</li> <li>Create a new Pull Request</li> </ol> <p>One of the code owners will review your code and request changes if needed. Once your changes have been approved, your contributions will become part of SpeckCn2. \ud83c\udf89</p>"},{"location":"CONTRIBUTING/#getting-started-with-development","title":"Getting started with development","text":""},{"location":"CONTRIBUTING/#setup","title":"Setup","text":"<p>SpeckCn2 targets Python 3.9 or newer.</p> <p>Clone the repository into the <code>speckcn2</code> directory:</p> <pre><code>git clone https://github.com/MALES-project/SpeckleCn2Profiler speckcn2\ncd speckcn2\n</code></pre> <p>Initialize all submodules: <pre><code>git submodule update --recursive --init\n</code></pre></p> <p>Install using <code>virtualenv</code>:</p> <pre><code>python3 -m venv env\nsource env/bin/activate\npython3 -m pip install -e .[develop]\n</code></pre> <p>Alternatively, install using Conda:</p> <pre><code>conda create -n speckcn2 python=3.10\nconda activate speckcn2\npip install -e .[develop]\n</code></pre>"},{"location":"CONTRIBUTING/#running-tests","title":"Running tests","text":"<p>SpeckCn2 uses pytest to run the tests. You can run the tests for yourself using:</p> <pre><code>pytest\n</code></pre> <p>Notice that some of the tests will fail the first time that you run them locally. After you get this failure message you can run</p> <pre><code>python ./scripts/setup_test.py\n</code></pre> <p>to stash the test data in the correct location. After that, you can run the tests again and they should pass.</p> <p>To check coverage:</p> <pre><code>coverage run -m pytest\ncoverage report  # to output to terminal\ncoverage html    # to generate html report\n</code></pre>"},{"location":"CONTRIBUTING/#building-the-documentation","title":"Building the documentation","text":"<p>The documentation is written in markdown, and uses mkdocs to generate the pages.</p> <p>To build the documentation for yourself:</p> <pre><code>pip install -e .[docs]\nmkdocs serve\n</code></pre> <p>You can find the documentation source in the docs directory. If you are adding new pages, make sure to update the listing in the <code>mkdocs.yml</code> under the <code>nav</code> entry.</p>"},{"location":"CONTRIBUTING/#making-a-release","title":"Making a release","text":"<ol> <li> <p>Make a new release.</p> </li> <li> <p>Under 'Choose a tag', set the tag to the new version. The versioning scheme we use is SemVer, so bump the version (major/minor/patch) as needed. Bumping the version is handled transparently by <code>bumpversion</code> in this workflow.</p> </li> <li> <p>The upload to pypi is triggered when a release is published and handled by this workflow.</p> </li> <li> <p>The upload to zenodo is triggered when a release is published.</p> </li> </ol>"},{"location":"installation/","title":"Installation","text":"<p>This guide provides step-by-step instructions for installing <code>speckcn2</code> on different platforms and setups.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>To correctly install <code>speckcn2</code>, you need Python 3.9 or higher. If you don't have it installed, you can download it from the official Python website.</p> <p>You will also need header files required to compile Python extensions (contained in <code>python3-dev</code>):</p> <p>=== \"Ubuntu/Debian\"     <pre><code>sudo apt-get install python3-dev\n</code></pre></p> <p>=== \"Fedora/RHEL\"     <pre><code>sudo dnf install python3-devel\n</code></pre></p> <p>=== \"macOS\"     <pre><code>xcode-select --install\n</code></pre></p> <p>=== \"Windows\"     Headers are included with the official Python installer - no additional steps needed.</p>"},{"location":"installation/#installation-options","title":"Installation Options","text":""},{"location":"installation/#option-1-install-from-pypi-recommended","title":"Option 1: Install from PyPI (Recommended)","text":"<p>This is the simplest way to install <code>speckcn2</code> for regular usage:</p> <pre><code>python -m pip install speckcn2\n</code></pre>"},{"location":"installation/#using-virtual-environment-strongly-recommended","title":"Using Virtual Environment (Strongly Recommended)","text":"<p>We strongly recommend using a virtual environment to avoid dependency conflicts:</p> <p>=== \"Linux/macOS\"     <pre><code># Create virtual environment\npython -m venv speckcn2-env\n\n# Activate virtual environment\nsource speckcn2-env/bin/activate\n\n# Install the package\npython -m pip install speckcn2\n</code></pre></p> <p>=== \"Windows\"     <pre><code># Create virtual environment\npython -m venv speckcn2-env\n\n# Activate virtual environment\nspeckcn2-env\\Scripts\\activate\n\n# Install the package\npython -m pip install speckcn2\n</code></pre></p>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify that everything works correctly:</p> <pre><code>python -c \"import speckcn2; print('Installation successful!')\"\n</code></pre>"},{"location":"installation/#option-2-development-installation","title":"Option 2: Development Installation","text":"<p>For advanced users and developers who want to modify the code or contribute to the project:</p> <pre><code>git clone https://github.com/MALES-project/SpeckleCn2Profiler.git\ncd SpeckleCn2Profiler\ngit submodule init\ngit submodule update\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs the package in \"editable\" mode, meaning changes to the source code will be immediately reflected without reinstalling.</p>"},{"location":"installation/#platform-specific-instructions","title":"Platform-Specific Instructions","text":""},{"location":"installation/#macos-with-apple-silicon-m1m2","title":"macOS with Apple Silicon (M1/M2)","text":"<p>Apple Silicon Macs require special handling due to dependency compatibility issues:</p> <p>Python Version Limitation</p> <p>Some dependencies (e.g., <code>scikit-learn</code>) may not support the latest Python version (3.12). We recommend using Python 3.10 or 3.11.</p> <p>The <code>py3nj</code> dependency of <code>escnn</code> requires OpenMP, which needs to be installed via Homebrew with explicit compiler specification:</p> <pre><code># Create conda environment with compatible Python version\nconda create -n speckcn2 python=3.10\nconda activate speckcn2\n\n# Install py3nj with GNU compiler (instead of clang)\nCC=gcc-13 pip install py3nj\n\n# Install speckcn2\npip install -e .\n</code></pre> <p>Installing GCC</p> <p>If you don't have <code>gcc-13</code> installed, you can install it via Homebrew: <pre><code>brew install gcc\n</code></pre></p>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>This guide will help you get started with <code>speckcn2</code> quickly. Follow these steps to install the package, verify your installation, and run your first example.</p>"},{"location":"quickstart/#1-install-specklecn2profiler","title":"1. Install SpeckleCn2Profiler","text":"<p>First, install the package using pip. We recommend using a virtual environment:</p> <pre><code># Create and activate a virtual environment\npython -m venv speckcn2-env\nsource speckcn2-env/bin/activate  # On Windows: speckcn2-env\\Scripts\\activate\n\n# Install the package\npip install speckcn2\n</code></pre> <p>Need more details?</p> <p>For platform-specific instructions and troubleshooting, see the full Installation Guide.</p>"},{"location":"quickstart/#2-verify-installation","title":"2. Verify Installation","text":"<p>Verify that the package is installed correctly:</p> <pre><code>python -c \"import speckcn2; print('Installation successful!')\"\n</code></pre>"},{"location":"quickstart/#3-run-tests","title":"3. Run Tests","text":"<p>To ensure everything is working properly, run the test suite:</p> <pre><code># Install pytest if not already installed\npip install pytest\n\n# Run tests\npytest\n</code></pre> <p>First-time test setup</p> <p>Some tests may fail on the first run because test data needs to be set up. If this happens, run: <pre><code>python ./scripts/setup_test.py\npytest\n</code></pre></p>"},{"location":"quickstart/#4-try-a-minimal-example","title":"4. Try a Minimal Example","text":"<p>Here's a minimal working example to train a model on sample data. Save this as <code>quickstart_train.py</code>:</p> <pre><code>import speckcn2 as sp2\nimport torch\n\ndef main():\n    # Load configuration (you'll need to provide your own config file)\n    config = sp2.load_config('path/to/config.yaml')\n\n    # Prepare data\n    print(\"Preparing data...\")\n    all_images, all_tags, all_ensemble_ids = sp2.prepare_data(config)\n\n    # Normalize tags (helps the model work with reasonable numbers)\n    nz = sp2.Normalizer(config)\n\n    # Split data into training and testing sets\n    train_set, test_set = sp2.train_test_split(\n        all_images, all_tags, all_ensemble_ids, nz\n    )\n\n    # Setup device (GPU if available, otherwise CPU)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Using device: {device}')\n\n    # Load and configure the model\n    model, last_model_state = sp2.setup_model(config)\n    model = model.to(device)\n\n    # Define loss function and optimizer\n    criterion = sp2.ComposableLoss(config, nz, device)\n    optimizer = sp2.setup_optimizer(config, model)\n\n    # Train the model\n    print(\"Training model...\")\n    model, avg_loss = sp2.train(\n        model, last_model_state, config,\n        train_set, test_set, device,\n        optimizer, criterion, criterion\n    )\n\n    print(f'Training complete! Final loss: {avg_loss:.5f}')\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Run the script with:</p> <pre><code>python quickstart_train.py\n</code></pre>"},{"location":"quickstart/#5-working-with-example-data","title":"5. Working with Example Data","text":"<p>The repository includes sample data and configuration files. To use them:</p>"},{"location":"quickstart/#clone-the-examples-repository","title":"Clone the Examples Repository","text":"<pre><code>git clone https://github.com/MALES-project/examples_speckcn2.git\ncd examples_speckcn2\n</code></pre>"},{"location":"quickstart/#run-with-sample-configuration","title":"Run with Sample Configuration","text":"<p>Use one of the provided configuration files:</p> <pre><code>python example_train.py configurations/configuration_ResNet18.yaml\n</code></pre> <p>This will:</p> <ul> <li>Load the sample speckle pattern data</li> <li>Train a ResNet18 model to predict turbulence profiles</li> <li>Save the trained model and results</li> </ul>"},{"location":"quickstart/#expected-output","title":"Expected Output","text":"<p>You should see output similar to:</p> <pre><code>Using cuda.\nPreparing data...\nLoading images from ./data/\nTraining model...\nEpoch 1/100, Loss: 0.12345\nEpoch 2/100, Loss: 0.11234\n...\nFinished Training, Loss: 0.05432\n</code></pre>"},{"location":"quickstart/#6-next-steps","title":"6. Next Steps","text":"<p>Now that you have <code>speckcn2</code> running, here are some suggested next steps:</p>"},{"location":"quickstart/#learn-more-about-configuration","title":"Learn More About Configuration","text":"<p>The configuration file controls all aspects of training, from data preprocessing to model architecture. Learn how to customize it:</p> <ul> <li>Configuration Guide - Detailed explanation of all configuration parameters</li> </ul>"},{"location":"quickstart/#explore-different-models","title":"Explore Different Models","text":"<p><code>speckcn2</code> supports multiple model architectures:</p> <ul> <li>ResNet18, ResNet50, ResNet152</li> <li>Custom SCNN (Steerable CNN)</li> <li>Equivariant models for rotation-invariant predictions</li> </ul> <p>See the Models API for more details.</p>"},{"location":"quickstart/#understand-your-data","title":"Understand Your Data","text":"<p>Learn about the input data format and how to prepare your own SCIDAR observations:</p> <ul> <li>Input Data Guide - Data format specifications and preparation</li> </ul>"},{"location":"quickstart/#run-postprocessing-and-analysis","title":"Run Postprocessing and Analysis","text":"<p>After training, analyze your model's performance:</p> <pre><code># Score the model on test data\ntest_tags, test_losses, test_measures, test_cn2_pred, test_cn2_true, \\\n    test_recovered_tag_pred, test_recovered_tag_true = sp2.score(\n        model, test_set, device, criterion, nz\n    )\n\n# Plot results\nsp2.plot_J_error_details(config, test_recovered_tag_true, test_recovered_tag_pred)\nsp2.plot_loss(config, model, datadirectory)\n</code></pre> <p>See the Postprocessing Guide for visualization examples.</p>"},{"location":"quickstart/#predict-turbulence-parameters","title":"Predict Turbulence Parameters","text":"<p>Beyond Cn\u00b2 profiles, <code>speckcn2</code> can predict important atmospheric parameters:</p> <ul> <li>Fried parameter (<code>r\u2080</code>) - measures atmospheric coherence length</li> <li>Isoplanatic angle (<code>\u03b8\u2080</code>) - angular size of coherent atmospheric regions  </li> <li>Rytov index (<code>\u03c3</code>) - scintillation strength indicator</li> </ul> <p>Configure these in the <code>preproc</code> section of your configuration file.</p>"},{"location":"quickstart/#need-help","title":"Need Help?","text":"<ul> <li>Full Documentation: Browse the complete documentation for in-depth guides</li> <li>API Reference: Check the Python API for detailed function documentation</li> <li>Examples: Explore more examples and use cases</li> <li>Issues: Report bugs or request features on GitHub Issues</li> <li>Contributing: Want to contribute? See our Contributing Guidelines</li> </ul>"},{"location":"quickstart/#quick-reference","title":"Quick Reference","text":"Task Command Install package <code>pip install speckcn2</code> Verify installation <code>python -c \"import speckcn2\"</code> Run tests <code>pytest</code> Train a model <code>python train.py config.yaml</code> Check documentation Visit https://males-project.github.io/SpeckleCn2Profiler/ <p>Ready to dive deeper? Head to the Examples section for complete walkthroughs and detailed explanations.</p>"},{"location":"api/api/","title":"speckcn2","text":"<p>The main API of the <code>speckcn2</code> package is composed of the following modules:</p> <ul> <li> <p>speckcn2.preprocess: contains the functions to preprocess the data before training the model.</p> </li> <li> <p>speckcn2.models: contains the classes that define the model architecture. In particular we support <code>ResNet</code> and <code>SCNN</code> based architectures.</p> </li> <li> <p>speckcn2.mlops: contains the functions to perform operations involving the models, where the most important are:</p> <ul> <li>speckcn2.mlops.train</li> <li>speckcn2.mlops.score</li> </ul> </li> <li> <p>speckcn2.plot: contains the functions to plot the results of the model.</p> </li> </ul>"},{"location":"api/io/","title":"I/O","text":"<p>This module provides utility functions for loading and saving model configurations and states.</p> <p>It includes functions to load configuration files, save model states, load model states, and load the latest model state from a directory.</p>"},{"location":"api/io/#speckcn2.io.load","title":"<code>load(model, datadirectory, epoch, early_stop=False)</code>","text":"<p>Load the model state and the model itself from a specified directory and epoch.</p> <p>This function loads the model's state dictionary and other relevant information such as epoch, loss, validation loss, and time from a file in the specified directory.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to load</p> </li> <li> <code>datadirectory</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> <li> <code>epoch</code>               (<code>int</code>)           \u2013            <p>The epoch of the model</p> </li> <li> <code>early_stop</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the last state reached the early stop condition</p> </li> </ul> Source code in <code>src/speckcn2/io.py</code> <pre><code>def load(model: torch.nn.Module,\n         datadirectory: str,\n         epoch: int,\n         early_stop: bool = False) -&gt; None:\n    \"\"\"Load the model state and the model itself from a specified directory and\n    epoch.\n\n    This function loads the model's state dictionary and other relevant information\n    such as epoch, loss, validation loss, and time from a file in the specified directory.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to load\n    datadirectory : str\n        The directory where the data is stored\n    epoch : int\n        The epoch of the model\n    early_stop: bool\n        If True, the last state reached the early stop condition\n    \"\"\"\n    if early_stop:\n        model_state = torch.load(\n            f'{datadirectory}/{model.name}_states/{model.name}_{epoch}_earlystop.pth',\n            weights_only=False)\n        model.early_stop = True\n    else:\n        model_state = torch.load(\n            f'{datadirectory}/{model.name}_states/{model.name}_{epoch}.pth',\n            weights_only=False)\n\n    model.epoch = model_state['epoch']\n    model.loss = model_state['loss']\n    model.val_loss = model_state['val_loss']\n    model.time = model_state['time']\n    model.load_state_dict(model_state['model_state_dict'], strict=False)\n\n    assert model.epoch[\n        -1] == epoch, 'The epoch of the model is not the same as the one loaded'\n</code></pre>"},{"location":"api/io/#speckcn2.io.load_config","title":"<code>load_config(config_file_path)</code>","text":"<p>Load the configuration file from a given path.</p> <p>This function reads a YAML configuration file and returns its contents as a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config_file_path</code>               (<code>str</code>)           \u2013            <p>Path to the .yaml configuration file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>config</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the configuration</p> </li> </ul> Source code in <code>src/speckcn2/io.py</code> <pre><code>def load_config(config_file_path: str) -&gt; dict:\n    \"\"\"Load the configuration file from a given path.\n\n    This function reads a YAML configuration file and returns its contents as a dictionary.\n\n    Parameters\n    ----------\n    config_file_path : str\n        Path to the .yaml configuration file\n\n    Returns\n    -------\n    config : dict\n        Dictionary containing the configuration\n    \"\"\"\n    with open(config_file_path, 'r') as file:\n        config = yaml.safe_load(file)\n    return config\n</code></pre>"},{"location":"api/io/#speckcn2.io.load_model_state","title":"<code>load_model_state(model, datadirectory)</code>","text":"<p>Loads the latest model state from the given directory.</p> <p>This function checks the specified directory for the latest model state file, loads it, and updates the model with the loaded state. If no state is found, it initializes the model state. If the training was stopped after meeting an early stop condition, this function signals that the training should not be continued.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to load the state into</p> </li> <li> <code>datadirectory</code>               (<code>str</code>)           \u2013            <p>The directory where the model states are stored</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>The model with the loaded state</p> </li> <li> <code>last_model_state</code> (              <code>int</code> )          \u2013            <p>The number of the last model state</p> </li> </ul> Source code in <code>src/speckcn2/io.py</code> <pre><code>def load_model_state(model: torch.nn.Module,\n                     datadirectory: str) -&gt; tuple[torch.nn.Module, int]:\n    \"\"\"Loads the latest model state from the given directory.\n\n    This function checks the specified directory for the latest model state file,\n    loads it, and updates the model with the loaded state. If no state is found,\n    it initializes the model state.\n    If the training was stopped after meeting an early stop condition, this function\n    signals that the training should not be continued.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to load the state into\n    datadirectory : str\n        The directory where the model states are stored\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The model with the loaded state\n    last_model_state : int\n        The number of the last model state\n    \"\"\"\n    # Print model information\n    print(model)\n    model.nparams = sum(p.numel() for p in model.parameters())\n    print(f'\\n--&gt; Nparams = {model.nparams}')\n\n    fulldirname = f'{datadirectory}/{model.name}_states'\n    ensure_directory(fulldirname)\n\n    # First check if there was an early stop\n    earlystop = [\n        filename for filename in os.listdir(fulldirname)\n        if 'earlystop' in filename\n    ]\n    if len(earlystop) == 0:\n        # If there was no early stop, check what is the last model state\n        try:\n            last_model_state = sorted([\n                int(file_name.split('.pth')[0].split('_')[-1])\n                for file_name in os.listdir(fulldirname)\n            ])[-1]\n        except Exception as e:\n            print(f'Warning: {e}')\n            last_model_state = 0\n\n        if last_model_state &gt; 0:\n            print(\n                f'Loading model at epoch {last_model_state}, from {datadirectory}'\n            )\n            load(model, datadirectory, last_model_state)\n            return model, last_model_state\n        else:\n            print('No pretrained model to load')\n\n            # Initialize some model state measures\n            model.loss = []\n            model.val_loss = []\n            model.time = []\n            model.epoch = []\n\n            return model, 0\n    elif len(earlystop) == 1:\n        filename = earlystop[0]\n        print(f'Loading the early stop state {filename}')\n        last_model_state = int(filename.split('_')[-2])\n        load(model, datadirectory, last_model_state, early_stop=True)\n        return model, last_model_state\n    else:\n        print(\n            f'Error: more than one early stop state found. This is not correct. This is the list: {earlystop}'\n        )\n        sys.exit(0)\n</code></pre>"},{"location":"api/io/#speckcn2.io.save","title":"<code>save(model, datadirectory, early_stop=False)</code>","text":"<p>Save the model state and the model itself to a specified directory.</p> <p>This function saves the model's state dictionary and other relevant information such as epoch, loss, validation loss, and time to a file in the specified directory.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to save</p> </li> <li> <code>datadirectory</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> <li> <code>early_stop</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the model corresponds to the moment when early stop was triggered</p> </li> </ul> Source code in <code>src/speckcn2/io.py</code> <pre><code>def save(model: torch.nn.Module,\n         datadirectory: str,\n         early_stop: bool = False) -&gt; None:\n    \"\"\"Save the model state and the model itself to a specified directory.\n\n    This function saves the model's state dictionary and other relevant information\n    such as epoch, loss, validation loss, and time to a file in the specified directory.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to save\n    datadirectory : str\n        The directory where the data is stored\n    early_stop: bool\n        If True, the model corresponds to the moment when early stop was triggered\n    \"\"\"\n    model_state = {\n        'epoch': model.epoch,\n        'loss': model.loss,\n        'val_loss': model.val_loss,\n        'time': model.time,\n        'model_state_dict': model.state_dict(),\n    }\n\n    if not early_stop:\n        savename = f'{datadirectory}/{model.name}_states/{model.name}_{model.epoch[-1]}.pth'\n    else:\n        savename = f'{datadirectory}/{model.name}_states/{model.name}_{model.epoch[-1]}_earlystop.pth'\n\n    torch.save(model_state, savename)\n</code></pre>"},{"location":"api/loss/","title":"Loss","text":"<p>This module implements various loss functions for training machine learning models.</p> <p>It includes custom loss functions that extend PyTorch's nn.Module, allowing for flexible and efficient computation of loss values during training. The loss functions handle different scenarios such as classification, regression, and segmentation tasks. They incorporate techniques like weighted losses, focal losses, and smooth L1 losses to address class imbalances and improve model performance. The module ensures that the loss calculations are compatible with PyTorch's autograd system, enabling seamless integration into training loops.</p>"},{"location":"api/loss/#speckcn2.loss.ComposableLoss","title":"<code>ComposableLoss(config, nz, device, validation=False)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Compose the loss function using several terms. The importance of each term has to be specified in the configuration file. Each term with a &gt;0 weight will be added to the loss function.</p> <p>The loss term available are: - MSE: mean squared error between predicted and target normalized screen tags - MAE: mean absolute error between predicted and target normalized screen tags - JMSE: mean squared error between predicted and target J - JMAE: mean absolute error between predicted and target J - Pearson: Pearson correlation coefficient between predicted and target J - Fried: Fried parameter r0 - Isoplanatic: Isoplanatic angle theta0 - Rytov: Rytov variance sigma_r^2 that will be computed on log averaged Cn2 - Scintillation_w: scintillation index for weak turbulence - Scintillation_m: scintillation index for moderate-strong turbulence</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>nz</code>               (<code>Normalizer</code>)           \u2013            <p>Normalizer object to be used to extract J in its original scale</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use for the computation</p> </li> <li> <code>validation</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If true, use the validation parameters from config</p> </li> </ul> Source code in <code>src/speckcn2/loss.py</code> <pre><code>def __init__(self,\n             config: dict,\n             nz: Normalizer,\n             device: torch.device,\n             validation: bool = False):\n    super(ComposableLoss, self).__init__()\n    if validation:\n        if 'val_loss' in config:\n            config['loss'] = config['val_loss']\n        else:\n            print('[!] Warning: Validation loss not found in config.yaml,',\n                  'keeping track of training loss instead')\n    self.device = device\n    self.loss_functions: dict[str, Callable] = {\n        'MSE': torch.nn.MSELoss(),\n        'MAE': torch.nn.L1Loss(),\n        'JMSE': self._MSELoss,\n        'JMAE': self._L1Loss,\n        'Cn2MSE': self._do_nothing,\n        'Cn2MAE': self._do_nothing,\n        'Pearson': self._PearsonCorrelationLoss,\n        'Fried': self._FriedLoss,\n        'Isoplanatic': self._IsoplanaticLoss,\n        'Rytov': self._RytovLoss,\n        'Scintillation_w': self._ScintillationWeakLoss,\n        'Scintillation_ms': self._ScintillationModerateStrongLoss,\n    }\n    self.loss_weights = {\n        loss_name: config['loss'].get(loss_name, 0)\n        for loss_name in self.loss_functions.keys()\n    }\n    self.total_weight = sum(self.loss_weights.values())\n    self._select_loss_needed()\n\n    # And get some useful parameters for the loss functions\n    # the parameters are explained at:\n    # https://males-project.github.io/SpeckleCn2Profiler/example/#configuration-file-explanation\n    self.h = torch.Tensor([float(x) for x in config['speckle']['hArray']])\n    self.k = 2 * torch.pi / (config['speckle'].get('lambda', 550) * 1e-9)\n    self.cosz = np.cos(np.deg2rad(config['speckle'].get('z', 0)))\n    self.secz = 1 / self.cosz\n    self.p_fr = 0.423 * self.k**2 * self.secz\n    self.p_iso = self.cosz**(8. / 5.) / ((2.91 * self.k**2)**(3. / 5.))\n    self.p_scw = 2.25 * self.k**(7. / 6.) * self.secz**(11. / 6.)\n\n    # We need to ba able to recover the tags\n    self.recover_tag = nz.recover_tag\n    # Move tensors to the device\n    self.h = self.h.to(self.device)\n</code></pre>"},{"location":"api/loss/#speckcn2.loss.ComposableLoss.forward","title":"<code>forward(pred, target)</code>","text":"<p>Forward pass of the loss function.</p> <p>Parameters:</p> <ul> <li> <code>pred</code>               (<code>Tensor</code>)           \u2013            <p>The predicted screen tags</p> </li> <li> <code>target</code>               (<code>Tensor</code>)           \u2013            <p>The target screen tags</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            <p>The composed loss</p> </li> <li> <code>losses</code> (              <code>dict</code> )          \u2013            <p>Dictionary containing the individual losses</p> </li> </ul> Source code in <code>src/speckcn2/loss.py</code> <pre><code>def forward(self, pred: torch.Tensor,\n            target: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    \"\"\"Forward pass of the loss function.\n\n    Parameters\n    ----------\n    pred : torch.Tensor\n        The predicted screen tags\n    target : torch.Tensor\n        The target screen tags\n\n    Returns\n    -------\n    loss : torch.Tensor\n        The composed loss\n    losses : dict\n        Dictionary containing the individual losses\n    \"\"\"\n    total_loss = 0\n    losses = {}\n\n    if self.Cn2required:\n        Cn2_pred = self.reconstruct_cn2(pred)\n        Cn2_target = self.reconstruct_cn2(target)\n\n    for loss_name, loss_fn in self.loss_needed.items():\n        weight = self.loss_weights[loss_name]\n        if loss_name in ['MAE', 'MSE']:\n            this_loss = loss_fn(pred, target)\n        else:\n            this_loss = loss_fn(pred, target, Cn2_pred, Cn2_target)\n        total_loss += weight * this_loss\n        losses[loss_name] = this_loss\n\n    return total_loss / self.total_weight, losses\n</code></pre>"},{"location":"api/loss/#speckcn2.loss.ComposableLoss.get_FriedParameter","title":"<code>get_FriedParameter(Jnorm)</code>","text":"<p>Compute the Fried parameter r0 from the screen tags.</p> Source code in <code>src/speckcn2/loss.py</code> <pre><code>def get_FriedParameter(self, Jnorm: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Fried parameter r0 from the screen tags.\"\"\"\n    J = torch.Tensor(self.get_J(Jnorm))\n    return (self.p_fr * torch.sum(J))**(-3 / 5)\n</code></pre>"},{"location":"api/loss/#speckcn2.loss.ComposableLoss.get_IsoplanaticAngle","title":"<code>get_IsoplanaticAngle(Cn2)</code>","text":"<p>Compute the isoplanatic angle theta0 from the screen tags.</p> Source code in <code>src/speckcn2/loss.py</code> <pre><code>def get_IsoplanaticAngle(self, Cn2: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the isoplanatic angle theta0 from the screen tags.\"\"\"\n    # Integrate Cn2*z^(5/3)\n    integral = torch.sum(\n        Cn2 * (self.h[1:]**(8 / 3) - self.h[:-1]**(8 / 3))) * 3 / 8\n    # Then I can compute theta0\n    return self.p_iso / (integral**(3 / 5))\n</code></pre>"},{"location":"api/loss/#speckcn2.loss.ComposableLoss.get_J","title":"<code>get_J(Jnorm)</code>","text":"<p>Recover J from the normalized tags. This needs to be done to compute Cn2.</p> <p>Parameters:</p> <ul> <li> <code>Jnorm</code>               (<code>Tensor</code>)           \u2013            <p>The normalized screen tags between 0 and 1</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>J</code> (              <code>Tensor</code> )          \u2013            <p>The recovered screen tags</p> </li> </ul> Source code in <code>src/speckcn2/loss.py</code> <pre><code>def get_J(self, Jnorm: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Recover J from the normalized tags. This needs to be done to compute\n    Cn2.\n\n    Parameters\n    ----------\n    Jnorm : torch.Tensor\n        The normalized screen tags between 0 and 1\n\n    Returns\n    -------\n    J : torch.Tensor\n        The recovered screen tags\n    \"\"\"\n\n    if Jnorm.ndim == 1:\n        Jnorm = Jnorm[None, :]\n\n    J = []\n    for i in range(Jnorm.shape[0]):\n        J.append(\n            torch.tensor([\n                10**self.recover_tag[j](Jnorm[i][j], i)\n                for j in range(len(Jnorm[i]))\n            ],\n                         requires_grad=True).to(Jnorm.device))\n    J = torch.stack(J)\n    return J\n</code></pre>"},{"location":"api/loss/#speckcn2.loss.ComposableLoss.get_ScintillationModerateStrong","title":"<code>get_ScintillationModerateStrong(x)</code>","text":"<p>Compute the scintillation index for moderate-strong turbulence sigma^2 from the screen tags.</p> Source code in <code>src/speckcn2/loss.py</code> <pre><code>def get_ScintillationModerateStrong(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the scintillation index for moderate-strong turbulence\n    sigma^2 from the screen tags.\"\"\"\n    wsigma2 = self.get_ScintillationWeak(x)\n    return torch.exp(wsigma2 * 0.49 / (1 + 1.11 * wsigma2**(6 / 5)) +\n                     0.51 * wsigma2 / (1 + 0.69 * wsigma2**(6 / 5)))\n</code></pre>"},{"location":"api/loss/#speckcn2.loss.ComposableLoss.get_ScintillationWeak","title":"<code>get_ScintillationWeak(Cn)</code>","text":"<p>Compute the scintillation index for weak turbulence sigma^2 from the screen tags.</p> Source code in <code>src/speckcn2/loss.py</code> <pre><code>def get_ScintillationWeak(self, Cn: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the scintillation index for weak turbulence sigma^2 from the\n    screen tags.\"\"\"\n    # Integrate Cn2*z^(5/6)\n    integral = torch.sum(\n        Cn * (self.h[1:]**(11 / 6) - self.h[:-1]**(11 / 6))) * 6 / 11\n    # Then I can compute sigma^2\n    return self.p_scw * integral\n</code></pre>"},{"location":"api/loss/#speckcn2.loss.ComposableLoss.reconstruct_cn2","title":"<code>reconstruct_cn2(Jnorm)</code>","text":"<p>Reconstruct Cn2 from screen tags c_i = J_i / (h[i+1] - h[i])</p> <p>Parameters:</p> <ul> <li> <code>Jnorm</code>               (<code>Tensor</code>)           \u2013            <p>The screen tags normalized between 0 and 1</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Cn2</code> (              <code>Tensor</code> )          \u2013            <p>The Cn2 reconstructed from the screen tags, assuming a uniform profile</p> </li> </ul> Source code in <code>src/speckcn2/loss.py</code> <pre><code>def reconstruct_cn2(self, Jnorm: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\" Reconstruct Cn2 from screen tags\n    c_i = J_i / (h[i+1] - h[i])\n\n    Parameters\n    ----------\n    Jnorm : torch.Tensor\n        The screen tags normalized between 0 and 1\n\n    Returns\n    -------\n    Cn2 : torch.Tensor\n        The Cn2 reconstructed from the screen tags, assuming a uniform profile\n    \"\"\"\n    J = self.get_J(Jnorm)\n    Cn2 = J / (self.h[1:] - self.h[:-1])\n    return Cn2\n</code></pre>"},{"location":"api/mlops/","title":"MLOps","text":""},{"location":"api/mlops/#speckcn2.mlops.score","title":"<code>score(model, test_set, device, criterion, normalizer, nimg_plot=100)</code>","text":"<p>Tests the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to test</p> </li> <li> <code>test_set</code>               (<code>list</code>)           \u2013            <p>The testing set</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> <li> <code>criterion</code>               (<code>ComposableLoss</code>)           \u2013            <p>The composable loss function, where I can access useful parameters</p> </li> <li> <code>normalizer</code>               (<code>Normalizer</code>)           \u2013            <p>The normalizer used to recover the tags</p> </li> <li> <code>nimg_plot</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of images to plot</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>test_tags</code> (              <code>list</code> )          \u2013            <p>List of all the predicted tags of the test set</p> </li> <li> <code>test_losses</code> (              <code>list</code> )          \u2013            <p>List of all the losses of the test set</p> </li> <li> <code>test_measures</code> (              <code>list</code> )          \u2013            <p>List of all the measures of the test set</p> </li> <li> <code>test_cn2_pred</code> (              <code>list</code> )          \u2013            <p>List of all the predicted Cn2 profiles of the test set</p> </li> <li> <code>test_cn2_true</code> (              <code>list</code> )          \u2013            <p>List of all the true Cn2 profiles of the test set</p> </li> <li> <code>test_recovered_tag_pred</code> (              <code>list</code> )          \u2013            <p>List of all the recovered tags from the model prediction</p> </li> <li> <code>test_recovered_tag_true</code> (              <code>list</code> )          \u2013            <p>List of all the recovered tags</p> </li> </ul> Source code in <code>src/speckcn2/mlops.py</code> <pre><code>def score(\n        model: nn.Module,\n        test_set: list,\n        device: torch.device,\n        criterion: ComposableLoss,\n        normalizer: Normalizer,\n        nimg_plot: int = 100\n) -&gt; tuple[list, list, list, list, list, list, list]:\n    \"\"\"Tests the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to test\n    test_set : list\n        The testing set\n    device : torch.device\n        The device to use\n    criterion : ComposableLoss\n        The composable loss function, where I can access useful parameters\n    normalizer : Normalizer\n        The normalizer used to recover the tags\n    nimg_plot : int\n        Number of images to plot\n\n    Returns\n    -------\n    test_tags : list\n        List of all the predicted tags of the test set\n    test_losses : list\n        List of all the losses of the test set\n    test_measures : list\n        List of all the measures of the test set\n    test_cn2_pred : list\n        List of all the predicted Cn2 profiles of the test set\n    test_cn2_true : list\n        List of all the true Cn2 profiles of the test set\n    test_recovered_tag_pred : list\n        List of all the recovered tags from the model prediction\n    test_recovered_tag_true : list\n        List of all the recovered tags\n    \"\"\"\n    counter = 0\n    conf = normalizer.conf\n    data_dir = conf['speckle']['datadirectory']\n    batch_size = conf['hyppar']['batch_size']\n    # Setup the EnsembleModel wrapper\n    ensemble = EnsembleModel(conf, device)\n\n    # For scoring the model, it is possible to compose the loss\n    # in the same way as you did during training adn validation.\n    # However, at the moment we are forcing the computation\n    # of all the quantities of interest.\n    # This can be changed in the future to save a bit of time.\n    criterion.loss_weights = {\n        'MSE': 1,\n        'MAE': 1,\n        'JMSE': 0,\n        'JMAE': 0,\n        'Cn2MSE': 0,\n        'Cn2MAE': 0,\n        'Pearson': 0,\n        'Fried': 1,\n        'Isoplanatic': 1,\n        'Rytov': 0,\n        'Scintillation_w': 1,\n        'Scintillation_ms': 0,\n    }\n    criterion._select_loss_needed()\n\n    with torch.no_grad():\n        # Put model in evaluation mode\n        model.eval()\n\n        test_tags = []\n        test_losses = []\n        test_measures = []\n        test_cn2_pred = []\n        test_cn2_true = []\n        test_recovered_tag_pred = []\n        test_recovered_tag_true = []\n        # Initialize the loss max and min. They are used to plot the images with the\n        # highest and lowest loss. We skip examples with a common average value of the loss.\n        loss_max = 0\n        loss_min = 1e6\n        # create the directory where the images will be stored\n        ensure_directory(f'{data_dir}/{model.name}_score')\n\n        for idx in range(0, len(test_set), batch_size):\n            batch = test_set[idx:idx + batch_size]\n\n            # Forward pass\n            outputs, targets, inputs = ensemble(model, batch)\n\n            # Loop each input separately\n            for i in range(len(outputs)):\n                loss, losses = criterion(outputs[i], targets[i])\n\n                # Get the Cn2 profile and the recovered tags\n                Cn2_pred = criterion.reconstruct_cn2(outputs[i])\n                Cn2_true = criterion.reconstruct_cn2(targets[i])\n                recovered_tag_pred = criterion.get_J(outputs[i])\n                recovered_tag_true = criterion.get_J(targets[i])\n                # and get all the measures\n                all_measures = criterion._get_all_measures(\n                    targets[i], Cn2_true, outputs[i], Cn2_pred)\n                this_loss = loss.item()\n\n                if counter &lt; nimg_plot and (this_loss &gt; loss_max\n                                            or this_loss &lt; loss_min):\n                    loss_max = max(this_loss, loss_max)\n                    loss_min = min(this_loss, loss_min)\n                    print(f'Plotting item {counter} loss: {this_loss:.4f}')\n                    score_plot(conf, inputs, targets, loss, losses, i, counter,\n                               all_measures, Cn2_pred, Cn2_true,\n                               recovered_tag_pred, recovered_tag_true)\n                    counter += 1\n\n                # and get all the tags for statistic analysis\n                for tag in outputs:\n                    test_tags.append(tag)\n\n                # and get the other information\n                test_losses.append(losses)\n                test_measures.append(all_measures)\n                test_cn2_pred.append(Cn2_pred)\n                test_cn2_true.append(Cn2_true)\n                test_recovered_tag_pred.append(recovered_tag_pred)\n                test_recovered_tag_true.append(recovered_tag_true)\n\n    return (test_tags, test_losses, test_measures, test_cn2_pred,\n            test_cn2_true, test_recovered_tag_pred, test_recovered_tag_true)\n</code></pre>"},{"location":"api/mlops/#speckcn2.mlops.train","title":"<code>train(model, last_model_state, conf, train_set, test_set, device, optimizer, criterion, criterion_val)</code>","text":"<p>Trains the model for the given number of epochs.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to train</p> </li> <li> <code>last_model_state</code>               (<code>int</code>)           \u2013            <p>The number of the last model state</p> </li> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>train_set</code>               (<code>list</code>)           \u2013            <p>The training set</p> </li> <li> <code>test_set</code>               (<code>list</code>)           \u2013            <p>The testing set</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> <li> <code>optimizer</code>               (<code>optim</code>)           \u2013            <p>The optimizer to use</p> </li> <li> <code>criterion</code>               (<code>ComposableLoss</code>)           \u2013            <p>The loss function to use</p> </li> <li> <code>criterion_val</code>               (<code>ComposableLoss</code>)           \u2013            <p>The loss function to use for validation</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>The trained model</p> </li> <li> <code>average_loss</code> (              <code>float</code> )          \u2013            <p>The average loss of the last epoch</p> </li> </ul> Source code in <code>src/speckcn2/mlops.py</code> <pre><code>def train(model: nn.Module, last_model_state: int, conf: dict, train_set: list,\n          test_set: list, device: torch.device, optimizer: optim.Optimizer,\n          criterion: ComposableLoss,\n          criterion_val: ComposableLoss) -&gt; tuple[nn.Module, float]:\n    \"\"\"Trains the model for the given number of epochs.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to train\n    last_model_state : int\n        The number of the last model state\n    conf : dict\n        Dictionary containing the configuration\n    train_set : list\n        The training set\n    test_set : list\n        The testing set\n    device : torch.device\n        The device to use\n    optimizer : torch.optim\n        The optimizer to use\n    criterion : ComposableLoss\n        The loss function to use\n    criterion_val : ComposableLoss\n        The loss function to use for validation\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The trained model\n    average_loss : float\n        The average loss of the last epoch\n    \"\"\"\n\n    final_epoch = conf['hyppar']['maxepochs']\n    save_every = conf['model']['save_every']\n    datadirectory = conf['speckle']['datadirectory']\n    batch_size = conf['hyppar']['batch_size']\n\n    if getattr(model, 'early_stop', False):\n        print(\n            'Warning: Training reached early stop in a previous training instance'\n        )\n        return model, 0\n\n    print(f'Training the model from epoch {last_model_state} to {final_epoch}')\n\n    # Setup the EnsembleModel wrapper\n    ensemble = EnsembleModel(conf, device)\n\n    # Early stopper\n    early_stopping = conf['hyppar'].get('early_stopping', -1)\n    if early_stopping &gt; 0:\n        print('Using early stopping (patience = {})'.format(early_stopping))\n        min_delta = conf['hyppar'].get('early_stop_delta', 0.1)\n        early_stopper = EarlyStopper(patience=early_stopping,\n                                     min_delta=min_delta)\n\n    average_loss = 0.0\n    model.train()\n    for epoch in range(last_model_state, final_epoch):\n        total_loss = 0.0\n        model.train()\n        t_in = time.time()\n        for i in range(0, len(train_set), batch_size):\n            batch = train_set[i:i + batch_size]\n\n            # Zero out the optimizer\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs, targets, _ = ensemble(model, batch)\n            loss, _ = criterion(outputs, targets)\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n            # Accumulate the loss\n            total_loss += loss.item()\n\n        # Shuffle the training set\n        random.shuffle(train_set)\n\n        # Calculate average loss for the epoch\n        average_loss = total_loss / len(train_set)\n\n        # Log the important information\n        t_fin = time.time() - t_in\n        model.loss.append(average_loss)\n        model.time.append(t_fin)\n        model.epoch.append(epoch + 1)\n\n        # And also the validation loss\n        val_loss = 0.0\n        model.eval()\n        with torch.no_grad():\n            for i in range(0, len(test_set), batch_size):\n                batch = test_set[i:i + batch_size]\n                # Forward pass\n                outputs, targets, _ = ensemble(model, batch)\n                loss, _ = criterion_val(outputs, targets)\n                # sum loss\n                val_loss += loss.item()\n        val_loss = val_loss / len(test_set)\n        model.val_loss.append(val_loss)\n\n        # Print the average loss for every epoch\n        message = (f'Epoch {epoch+1}/{final_epoch} '\n                   f'(in {t_fin:.3g}s),\\tTrain-Loss: {average_loss:.5f},\\t'\n                   f'Test-Loss: {val_loss:.5f}')\n        print(message, flush=True)\n\n        if early_stopper.early_stop(val_loss):\n            print('Early stopping triggered')\n            save(model, datadirectory, early_stop=True)\n\n        if (epoch + 1) % save_every == 0 or epoch == final_epoch - 1:\n            save(model, datadirectory)\n\n    return model, average_loss\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>This module contains the definition of the EnsembleModel class and a setup_model function.</p> <p>The EnsembleModel class is a wrapper that allows any model to be used for ensembled data. The setup_model function initializes and returns a model based on the provided configuration.</p>"},{"location":"api/models/#speckcn2.mlmodels.EarlyStopper","title":"<code>EarlyStopper(patience=1, min_delta=0)</code>","text":"<p>Early stopping to stop the training when the validation loss does not decrease anymore.</p> <p>Parameters:</p> <ul> <li> <code>patience</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of epochs of tolerance before stopping.</p> </li> <li> <code>min_delta</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Percentage of tolerance in considering the loss acceptable.</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def __init__(self, patience: int = 1, min_delta: float = 0):\n    \"\"\"Initializes the EarlyStopper.\n\n    Parameters\n    ----------\n    patience: int\n        Number of epochs of tolerance before stopping.\n    min_delta: float\n        Percentage of tolerance in considering the loss acceptable.\n    \"\"\"\n\n    self.patience = patience\n    self.min_delta = min_delta\n    self.counter = 0\n    self.min_validation_loss = float('inf')\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.EarlyStopper.early_stop","title":"<code>early_stop(validation_loss)</code>","text":"<p>Computes if the early stop condition is met at the current step.</p> <p>Parameters:</p> <ul> <li> <code>validation_loss</code>               (<code>float</code>)           \u2013            <p>Current value of the validation loss</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>It returns True if the training has met the stop condition.</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def early_stop(self, validation_loss: float) -&gt; bool:\n    \"\"\" Computes if the early stop condition is met at the current step.\n\n    Parameters\n    ----------\n    validation_loss: float\n        Current value of the validation loss\n\n    Returns\n    -------\n    bool\n        It returns True if the training has met the stop condition.\n    \"\"\"\n    if validation_loss &lt; self.min_validation_loss:\n        self.min_validation_loss = validation_loss\n        self.counter = 0\n    elif validation_loss &gt; self.min_validation_loss * (1 + self.min_delta):\n        self.counter += 1\n        if self.counter &gt;= self.patience:\n            return True\n    return False\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.EnsembleModel","title":"<code>EnsembleModel(conf, device)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper that allows any model to be used for ensembled data.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>The global configuration containing the model parameters.</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def __init__(self, conf: dict, device: torch.device):\n    \"\"\"Initializes the EnsembleModel.\n\n    Parameters\n    ----------\n    conf: dict\n        The global configuration containing the model parameters.\n    device : torch.device\n        The device to use\n    \"\"\"\n    super(EnsembleModel, self).__init__()\n\n    self.ensemble_size = conf['preproc'].get('ensemble', 1)\n    self.device = device\n    self.uniform_ensemble = conf['preproc'].get('ensemble_unif', False)\n    resolution = conf['preproc']['resize']\n    self.D = conf['noise']['D']\n    self.t = conf['noise']['t']\n    self.snr = conf['noise']['snr']\n    self.dT = conf['noise']['dT']\n    self.dO = conf['noise']['dO']\n    self.rn = conf['noise']['rn']\n    self.fw = conf['noise']['fw']\n    self.bit = conf['noise']['bit']\n    self.discretize = conf['noise']['discretize']\n    self.rot_sym = conf['noise'].get('rotation_sym', 0)\n    if self.rot_sym &gt; 0:\n        self.rot_fold = 360 // self.rot_sym\n    self.apply_masks = conf['noise'].get('apply_masks', False)\n    if self.apply_masks:\n        self.mask_D, self.mask_d, self.mask_X, self.mask_Y = self.create_masks(\n            resolution)\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.EnsembleModel.apply_noise","title":"<code>apply_noise(image_tensor)</code>","text":"<p>Processes a tensor of 2D images.</p> <p>Parameters:</p> <ul> <li> <code>image_tensor</code>               (<code>Tensor</code>)           \u2013            <p>Tensor of 2D images with shape (batch, channels, width, height).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>processed_tensor</code> (              <code>Tensor</code> )          \u2013            <p>Tensor of processed 2D images.</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def apply_noise(self, image_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Processes a tensor of 2D images.\n\n    Parameters\n    ----------\n    image_tensor : torch.Tensor\n        Tensor of 2D images with shape (batch, channels, width, height).\n\n    Returns\n    -------\n    processed_tensor : torch.Tensor\n        Tensor of processed 2D images.\n    \"\"\"\n    batch, channels, height, width = image_tensor.shape\n    processed_tensor = torch.zeros_like(image_tensor)\n\n    # Apply rotation symmetry\n    if self.rot_sym &gt; 0:\n        angle = random.randint(0, self.rot_fold)\n        image_tensor = torch.rot90(image_tensor, angle, (2, 3))\n\n    # Normalize wrt optical power\n    image_tensor = image_tensor / torch.mean(\n        image_tensor, dim=(2, 3), keepdim=True)\n\n    amp = self.rn * 10**(self.snr / 20)\n\n    for i in range(batch):\n        for j in range(channels):\n            B = image_tensor[i, j]\n\n            ## Apply masks\n            if self.apply_masks:\n                B[self.mask_D] = 0\n                B[self.mask_d] = 0\n                B[self.mask_X] = 0\n                B[self.mask_Y] = 0\n\n            # Add noise sources\n            A = self.rn + self.rn * torch.randn(\n                height, width, device=self.device) + amp * B + torch.sqrt(\n                    amp * B) * torch.randn(\n                        height, width, device=self.device)\n\n            # Make a discretized version\n            if self.discretize == 'on':\n                C = torch.round(A / self.fw * 2**self.bit)\n                C[A &gt; self.fw] = self.fw\n                C[A &lt; 0] = 0\n            else:\n                C = A\n\n            processed_tensor[i, j] = C\n\n    return processed_tensor\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.EnsembleModel.create_masks","title":"<code>create_masks(resolution)</code>","text":"<p>Creates the masks for the circular aperture and the spider.</p> <p>Parameters:</p> <ul> <li> <code>resolution</code>               (<code>int</code>)           \u2013            <p>Resolution of the images.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mask_D</code> (              <code>Tensor</code> )          \u2013            <p>Mask for the circular aperture.</p> </li> <li> <code>mask_d</code> (              <code>Tensor</code> )          \u2013            <p>Mask for the central obscuration.</p> </li> <li> <code>mask_X</code> (              <code>Tensor</code> )          \u2013            <p>Mask for the horizontal spider.</p> </li> <li> <code>mask_Y</code> (              <code>Tensor</code> )          \u2013            <p>Mask for the vertical spider.</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def create_masks(\n    self, resolution: int\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Creates the masks for the circular aperture and the spider.\n\n    Parameters\n    ----------\n    resolution : int\n        Resolution of the images.\n\n    Returns\n    -------\n    mask_D : torch.Tensor\n        Mask for the circular aperture.\n    mask_d : torch.Tensor\n        Mask for the central obscuration.\n    mask_X : torch.Tensor\n        Mask for the horizontal spider.\n    mask_Y : torch.Tensor\n        Mask for the vertical spider.\n    \"\"\"\n    # Coordinates\n    x = torch.linspace(-1, 1, resolution, device=self.device)\n    X, Y = torch.meshgrid(x, x, indexing='ij')\n    d = self.dO * self.D  # Diameter obscuration\n\n    R = torch.sqrt(X**2 + Y**2)\n\n    # Masking image\n    mask_D = R &gt; self.D\n    mask_d = R &lt; d\n    mask_X = torch.abs(X) &lt; self.t / 2\n    mask_Y = torch.abs(Y) &lt; self.t / 2\n\n    return mask_D, mask_d, mask_X, mask_Y\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.EnsembleModel.forward","title":"<code>forward(model, batch_ensemble)</code>","text":"<p>Forward pass through the model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to use</p> </li> <li> <code>batch_ensemble</code>               (<code>list</code>)           \u2013            <p>Each element is a batch of an ensemble of samples.</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def forward(self, model, batch_ensemble):\n    \"\"\"Forward pass through the model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to use\n    batch_ensemble : list\n        Each element is a batch of an ensemble of samples.\n    \"\"\"\n\n    if self.ensemble_size == 1:\n        batch = batch_ensemble\n        # If no ensembling, each element of the batch is a tuple (image, tag, ensemble_id)\n        images, tags, ensembles = zip(*batch)\n        images = torch.stack(images).to(self.device)\n        images = self.apply_noise(images)\n        tags = torch.tensor(np.stack(tags)).to(self.device)\n\n        return model(images), tags, images\n    else:\n        batch = list(itertools.chain(*batch_ensemble))\n        # Like the ensemble=1 case, I can process independently each element of the batch\n        images, tags, ensembles = zip(*batch)\n        images = torch.stack(images).to(self.device)\n        images = self.apply_noise(images)\n        tags = torch.tensor(np.stack(tags)).to(self.device)\n\n        model_output = model(images)\n\n        # To average the self.ensemble_size outputs of the model I extract the confidence weights\n        predictions = model_output[:, :-1]\n        weights = model_output[:, -1]\n        if self.uniform_ensemble:\n            weights = torch.ones_like(weights)\n        # multiply the prediction by the weights\n        weighted_predictions = predictions * weights.unsqueeze(-1)\n        # and sum over the ensembles\n        weighted_predictions = weighted_predictions.view(\n            model_output.size(0) // self.ensemble_size, self.ensemble_size,\n            -1).sum(dim=1)\n        # then normalize by the sum of the weights\n        sum_weights = weights.view(\n            weights.size(0) // self.ensemble_size,\n            self.ensemble_size).sum(dim=1)\n        ensemble_output = weighted_predictions / sum_weights.unsqueeze(-1)\n\n        # and get the tags and ensemble_id of the first element of the ensemble\n        tags = tags[::self.ensemble_size]\n        ensembles = ensembles[::self.ensemble_size]\n\n        return ensemble_output, tags, images\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.get_a_resnet","title":"<code>get_a_resnet(config)</code>","text":"<p>Returns a pretrained ResNet model, with the last layer corresponding to the number of screens.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>The model with the loaded state</p> </li> <li> <code>last_model_state</code> (              <code>int</code> )          \u2013            <p>The number of the last model state</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def get_a_resnet(config: dict) -&gt; tuple[nn.Module, int]:\n    \"\"\"Returns a pretrained ResNet model, with the last layer corresponding to\n    the number of screens.\n\n    Parameters\n    ----------\n    config : dict\n        Dictionary containing the configuration\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The model with the loaded state\n    last_model_state : int\n        The number of the last model state\n    \"\"\"\n\n    model_name = config['model']['name']\n    model_type = config['model']['type']\n    pretrained = config['model']['pretrained']\n    nscreens = config['speckle']['nscreens']\n    data_directory = config['speckle']['datadirectory']\n    ensemble = config['preproc'].get('ensemble', 1)\n\n    if model_type == 'resnet18':\n        model = torchvision.models.resnet18(\n            weights='IMAGENET1K_V1' if pretrained else None)\n        finaloutsize = 512\n    elif model_type == 'resnet50':\n        model = torchvision.models.resnet50(\n            weights='IMAGENET1K_V2' if pretrained else None)\n        finaloutsize = 2048\n    elif model_type == 'resnet152':\n        model = torchvision.models.resnet152(\n            weights='IMAGENET1K_V2' if pretrained else None)\n        finaloutsize = 2048\n    else:\n        raise ValueError(f'Unknown model {model_type}')\n\n    # If the model uses multiple images as input,\n    # add an extra channel as confidence weight\n    # to average the final prediction\n    if ensemble &gt; 1:\n        nscreens = nscreens + 1\n\n    # Give it its name\n    model.name = model_name\n\n    # Change the model to process black and white input\n    model.conv1 = torch.nn.Conv2d(1,\n                                  64,\n                                  kernel_size=(7, 7),\n                                  stride=(2, 2),\n                                  padding=(3, 3),\n                                  bias=False)\n    # Add a final fully connected piece to predict the output\n    model.fc = create_final_block(config, finaloutsize, nscreens)\n\n    return load_model_state(model, data_directory)\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.get_scnn","title":"<code>get_scnn(config)</code>","text":"<p>Returns a pretrained Spherical-CNN model, with the last layer corresponding to the number of screens.</p> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def get_scnn(config: dict) -&gt; tuple[nn.Module, int]:\n    \"\"\"Returns a pretrained Spherical-CNN model, with the last layer\n    corresponding to the number of screens.\"\"\"\n\n    model_name = config['model']['name']\n    model_type = config['model']['type']\n    datadirectory = config['speckle']['datadirectory']\n\n    model_map = {\n        'scnnC8': 'C8',\n        'scnnC16': 'C16',\n        'scnnC4': 'C4',\n        'scnnC6': 'C6',\n        'scnnC10': 'C10',\n        'scnnC12': 'C12',\n    }\n    try:\n        scnn_model = SteerableCNN(config, model_map[model_type])\n    except KeyError:\n        raise ValueError(f'Unknown model {model_type}')\n\n    scnn_model.name = model_name\n\n    return load_model_state(scnn_model, datadirectory)\n</code></pre>"},{"location":"api/models/#speckcn2.mlmodels.setup_model","title":"<code>setup_model(config)</code>","text":"<p>Returns the model specified in the configuration file, with the last layer corresponding to the number of screens.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>The model with the loaded state</p> </li> <li> <code>last_model_state</code> (              <code>int</code> )          \u2013            <p>The number of the last model state</p> </li> </ul> Source code in <code>src/speckcn2/mlmodels.py</code> <pre><code>def setup_model(config: dict) -&gt; tuple[nn.Module, int]:\n    \"\"\"Returns the model specified in the configuration file, with the last\n    layer corresponding to the number of screens.\n\n    Parameters\n    ----------\n    config : dict\n        Dictionary containing the configuration\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The model with the loaded state\n    last_model_state : int\n        The number of the last model state\n    \"\"\"\n\n    model_name = config['model']['name']\n    model_type = config['model']['type']\n\n    print(f'^^^ Initializing model {model_name} of type {model_type}')\n\n    if model_type.startswith('resnet'):\n        return get_a_resnet(config)\n    elif model_type.startswith('scnnC'):\n        return get_scnn(config)\n    else:\n        raise ValueError(f'Unknown model {model_name}')\n</code></pre>"},{"location":"api/normalizer/","title":"Normalizer","text":"<p>This module defines the Normalizer class, which handles the normalization of images and tags based on a given configuration.</p> <p>The class includes methods to precompile normalization functions, normalize images and tags, and define specific normalization strategies such as Z-score and uniform normalization. The normalization process involves replacing NaN values, creating masks, and scaling values to a specified range. The module also provides functions to recover the original values from the normalized data. The Normalizer class ensures that both images and tags are consistently normalized according to the specified configuration, facilitating further processing and analysis.</p>"},{"location":"api/normalizer/#speckcn2.normalizer.Normalizer","title":"<code>Normalizer(conf)</code>","text":"<p>Class to handle the normalization of images and tags.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> </ul> Source code in <code>src/speckcn2/normalizer.py</code> <pre><code>def __init__(self, conf: dict):\n    self.conf = conf\n</code></pre>"},{"location":"api/normalizer/#speckcn2.normalizer.Normalizer.normalize_imgs_and_tags","title":"<code>normalize_imgs_and_tags(all_images, all_tags, all_ensemble_ids)</code>","text":"<p>Normalize both the input images and the tags to be between 0 and 1.</p> <p>Parameters:</p> <ul> <li> <code>all_images</code>               (<code>list</code>)           \u2013            <p>List of all images</p> </li> <li> <code>all_tags</code>               (<code>list</code>)           \u2013            <p>List of all tags</p> </li> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dataset</code> (              <code>list</code> )          \u2013            <p>List of tuples (image, normalized_tag)</p> </li> </ul> Source code in <code>src/speckcn2/normalizer.py</code> <pre><code>def normalize_imgs_and_tags(\n    self,\n    all_images: list[torch.tensor],\n    all_tags: list[np.ndarray],\n    all_ensemble_ids: list[int],\n) -&gt; list[tuple[torch.tensor, np.ndarray, int]]:\n    \"\"\"Normalize both the input images and the tags to be between 0 and 1.\n\n    Parameters\n    ----------\n    all_images : list\n        List of all images\n    all_tags : list\n        List of all tags\n    conf : dict\n        Dictionary containing the configuration\n\n    Returns\n    -------\n    dataset : list\n        List of tuples (image, normalized_tag)\n    \"\"\"\n    self._normalizing_functions(all_images, all_tags, all_ensemble_ids)\n\n    # Normalize the images\n    normalized_images = [self.normalize_img(image) for image in all_images]\n\n    # And normalize the tags\n    normalized_tags = np.array([[\n        self.normalize_tag[j](tag, tag_id) for j, tag in enumerate(tags)\n    ] for tag_id, tags in enumerate(all_tags)],\n                               dtype=np.float32)\n\n    # I can now create the dataset\n    dataset = [(image, tag, ensemble_id) for image, tag, ensemble_id in\n               zip(normalized_images, normalized_tags, all_ensemble_ids)]\n\n    return dataset\n</code></pre>"},{"location":"api/plot/","title":"Plot","text":""},{"location":"api/plot/#speckcn2.plots.plot_J_error_details","title":"<code>plot_J_error_details(conf, tags_true, tags_pred, nbins=10, linear_bins=False)</code>","text":"<p>Function to plot the histograms per single bin of each single screen tag to quantify the relative error as a function of J.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>tags_true</code>               (<code>list</code>)           \u2013            <p>The true tags of the validation set</p> </li> <li> <code>tags_pred</code>               (<code>list</code>)           \u2013            <p>The predicted tags of the validation set</p> </li> <li> <code>nbins</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The number of bins in which to partition the data</p> </li> <li> <code>linear_bins</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the bins are linearly spaced, otherwise they are log spaced</p> </li> </ul> Source code in <code>src/speckcn2/plots.py</code> <pre><code>def plot_J_error_details(conf: dict,\n                         tags_true: list,\n                         tags_pred: list,\n                         nbins: int = 10,\n                         linear_bins: bool = False) -&gt; None:\n    \"\"\"Function to plot the histograms per single bin of each single screen tag\n    to quantify the relative error as a function of J.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    tags_true : list\n        The true tags of the validation set\n    tags_pred : list\n        The predicted tags of the validation set\n    nbins : int\n        The number of bins in which to partition the data\n    linear_bins : bool\n        If True, the bins are linearly spaced, otherwise they are log spaced\n    \"\"\"\n\n    nscreens = conf['speckle']['nscreens']\n    data_dir = conf['speckle']['datadirectory']\n    model_name = conf['model']['name']\n\n    dirname = f'{data_dir}/{model_name}_score/J_bin_details'\n    ensure_directory(dirname)\n\n    if conf['preproc'].get('J_details', False):\n        for screen_id in range(nscreens):\n            print(f'\\nComputing screen-{screen_id} details')\n\n            # Collect the data\n            params = []\n            loss = []\n            for i in range(len(tags_true)):\n                params.append(tags_true[i][0,\n                                           screen_id].detach().cpu().numpy())\n                loss.append((\n                    (tags_pred[i][0, screen_id] - tags_true[i][0, screen_id]) /\n                    (tags_true[i][0, screen_id])).detach().cpu().numpy())\n            params = np.array(params)\n            loss = np.array(loss)\n\n            if linear_bins:\n                bins = np.linspace(min(params), max(params), num=nbins)\n            else:\n                bins = np.logspace(np.log10(min(params)),\n                                   np.log10(max(params)),\n                                   num=nbins)\n            bin_indices = np.digitize(params, bins)\n            # get the average and std of the error per bin of J[screen_id]\n            bin_centers = 0.5 * (bins[:-1] + bins[1:])\n\n            for idx, single_bin in enumerate(bin_centers):\n                l_data = loss[bin_indices == idx]\n                if len(l_data) == 0:\n                    continue\n                fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n                axs.hist(l_data, bins=50, alpha=0.5, density=True)\n                mu = np.mean(l_data)\n                sigma = np.std(l_data)\n                print(\n                    f'J-{screen_id} = {single_bin:.3g} -&gt; mu = {mu:.3f}, sigma = {sigma:.3f}'\n                )\n                if sigma &gt; 0:\n                    x = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 100)\n                    x = x[x &gt; min(l_data)]\n                    x = x[x &lt; max(l_data)]\n                    axs.plot(x,\n                             stats.norm.pdf(x, mu, sigma),\n                             label=f'Average err: {mu:.3f}, Std: {sigma:.3f}')\n                axs.set_xlabel(f'Relative error J (screen-{screen_id})')\n                axs.set_ylabel('Frequency')\n                axs.legend()\n                plt.title(f'J (screen-{screen_id}) value = {single_bin:.3g}')\n                plt.tight_layout()\n                figure_format = conf.get('figure_format', 'png')\n                plt.savefig(\n                    f'{dirname}/Jscreen{screen_id}_bin{idx}.{figure_format}')\n                plt.close()\n</code></pre>"},{"location":"api/plot/#speckcn2.plots.plot_histo_losses","title":"<code>plot_histo_losses(conf, test_losses, data_dir)</code>","text":"<p>Plots the histogram of the losses.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>test_losses</code>               (<code>list[dict]</code>)           \u2013            <p>List of all the losses of the test set</p> </li> <li> <code>data_dir</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> </ul> Source code in <code>src/speckcn2/plots.py</code> <pre><code>def plot_histo_losses(conf: dict, test_losses: list[dict],\n                      data_dir: str) -&gt; None:\n    \"\"\"Plots the histogram of the losses.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    test_losses : list[dict]\n        List of all the losses of the test set\n    data_dir : str\n        The directory where the data is stored\n    \"\"\"\n    model_name = conf['model']['name']\n    data_dir = conf['speckle']['datadirectory']\n\n    dirname = f'{data_dir}/{model_name}_score/histo_losses'\n    ensure_directory(dirname)\n\n    fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n    for key in ['MAE', 'Fried', 'Isoplanatic', 'Scintillation_w']:\n        loss = [d[key].detach().cpu() for d in test_losses]\n        bins = np.logspace(np.log10(min(loss)), np.log10(max(loss)),\n                           num=50).tolist()\n        axs.hist(loss, bins=bins, alpha=0.5, label=key, density=True)\n    axs.set_xlabel('Loss')\n    axs.set_ylabel('Frequency')\n    axs.set_yscale('log')\n    axs.set_xscale('log')\n    axs.legend()\n    plt.title(f'Model: {model_name}')\n    plt.tight_layout()\n    figure_format = conf.get('figure_format', 'png')\n    plt.savefig(f'{dirname}/histo_losses_{model_name}.{figure_format}')\n    plt.close()\n</code></pre>"},{"location":"api/plot/#speckcn2.plots.plot_loss","title":"<code>plot_loss(conf, model, data_dir)</code>","text":"<p>Plots the loss of the model.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to plot the loss of</p> </li> <li> <code>data_dir</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> </ul> Source code in <code>src/speckcn2/plots.py</code> <pre><code>def plot_loss(conf: dict, model, data_dir: str) -&gt; None:\n    \"\"\"Plots the loss of the model.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    model : torch.nn.Module\n        The model to plot the loss of\n    data_dir : str\n        The directory where the data is stored\n    \"\"\"\n\n    model_name = conf['model']['name']\n    data_dir = conf['speckle']['datadirectory']\n\n    dirname = f'{data_dir}/{model_name}_score'\n    ensure_directory(dirname)\n\n    fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n    axs.plot(model.epoch, model.loss, label='Training loss')\n    axs.plot(model.epoch, model.val_loss, label='Validation loss')\n    axs.set_xlabel('Epoch')\n    axs.set_ylabel('Loss')\n    axs.set_yscale('log')\n    axs.legend()\n    plt.title(f'Model: {model_name}')\n    plt.tight_layout()\n    figure_format = conf.get('figure_format', 'png')\n    plt.savefig(f'{dirname}/loss_{model_name}.{figure_format}')\n    plt.close()\n</code></pre>"},{"location":"api/plot/#speckcn2.plots.plot_param_histo","title":"<code>plot_param_histo(conf, test_losses, data_dir, measures)</code>","text":"<p>Plots the histograms of different parameters.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>test_losses</code>               (<code>list[dict]</code>)           \u2013            <p>List of all the losses of the test set</p> </li> <li> <code>data_dir</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> <li> <code>measures</code>               (<code>list</code>)           \u2013            <p>The measures of the model</p> </li> </ul> Source code in <code>src/speckcn2/plots.py</code> <pre><code>def plot_param_histo(conf: dict, test_losses: list[dict], data_dir: str,\n                     measures: list) -&gt; None:\n    \"\"\"Plots the histograms of different parameters.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    test_losses : list[dict]\n        List of all the losses of the test set\n    data_dir : str\n        The directory where the data is stored\n    measures : list\n        The measures of the model\n    \"\"\"\n    model_name = conf['model']['name']\n    data_dir = conf['speckle']['datadirectory']\n\n    dirname = f'{data_dir}/{model_name}_score'\n    ensure_directory(dirname)\n\n    for param_model, param_true, name, units in zip(\n        ['Fried_pred', 'Isoplanatic_pred', 'Scintillation_w_pred'],\n        ['Fried_true', 'Isoplanatic_true', 'Scintillation_w_true'],\n        ['Fried parameter', 'Isoplanatic angle', 'Rytov index'],\n        ['[m]', '[rad]', '[1]'],\n    ):\n        fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n\n        params_model = [d[param_model].detach().cpu() for d in measures]\n        params_true = [d[param_true].detach().cpu() for d in measures]\n\n        pairs = sorted(zip(params_true, params_model))\n        params_true, params_model = zip(*pairs)\n        params_true = np.array(params_true)\n        params_model = np.array(params_model)\n\n        bins = np.logspace(np.log10(min(params_true)),\n                           np.log10(max(params_true)),\n                           num=50).tolist()\n        axs.hist(params_true,\n                 bins=bins,\n                 alpha=0.5,\n                 label=param_true,\n                 density=True)\n\n        bins = np.logspace(np.log10(min(params_model)),\n                           np.log10(max(params_model)),\n                           num=50).tolist()\n        axs.hist(params_model,\n                 bins=bins,\n                 alpha=0.5,\n                 label=param_model,\n                 density=True)\n\n        axs.set_xlabel(f'{name} {units}')\n        axs.set_xscale('log')\n        axs.set_yscale('log')\n        axs.set_ylabel('Frequency')\n        axs.legend()\n        plt.title(f'Model: {model_name}')\n        plt.tight_layout()\n        figure_format = conf.get('figure_format', 'png')\n        plt.savefig(\n            f'{dirname}/histo_{param_true}_{model_name}.{figure_format}')\n        plt.close()\n</code></pre>"},{"location":"api/plot/#speckcn2.plots.plot_param_vs_loss","title":"<code>plot_param_vs_loss(conf, test_losses, data_dir, measures, no_sign=False, nbins=10, linear_bins=False)</code>","text":"<p>Plots the parameter vs the loss. Optionally, it also plots the detailed histo for all the bins for the desired metrics.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>test_losses</code>               (<code>list[dict]</code>)           \u2013            <p>List of all the losses of the test set</p> </li> <li> <code>data_dir</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> <li> <code>measures</code>               (<code>list</code>)           \u2013            <p>The measures of the model</p> </li> <li> <code>no_sign</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, it will plot the abs of the relative error</p> </li> <li> <code>nbins</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The number of bins in which to partition the data</p> </li> <li> <code>linear_bins</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the bins are linearly spaced, otherwise they are log spaced</p> </li> </ul> Source code in <code>src/speckcn2/plots.py</code> <pre><code>def plot_param_vs_loss(conf: dict,\n                       test_losses: list[dict],\n                       data_dir: str,\n                       measures: list,\n                       no_sign: bool = False,\n                       nbins: int = 10,\n                       linear_bins: bool = False) -&gt; None:\n    \"\"\"Plots the parameter vs the loss. Optionally, it also plots the detailed\n    histo for all the bins for the desired metrics.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    test_losses : list[dict]\n        List of all the losses of the test set\n    data_dir : str\n        The directory where the data is stored\n    measures : list\n        The measures of the model\n    no_sign : bool\n        If True, it will plot the abs of the relative error\n    nbins : int\n        The number of bins in which to partition the data\n    linear_bins : bool\n        If True, the bins are linearly spaced, otherwise they are log spaced\n    \"\"\"\n    model_name = conf['model']['name']\n    data_dir = conf['speckle']['datadirectory']\n\n    dirname = f'{data_dir}/{model_name}_score'\n    ensure_directory(dirname)\n\n    for param, lname, name, units in zip(\n        ['Fried_true', 'Isoplanatic_true', 'Scintillation_w_true'],\n        ['Fried', 'Isoplanatic', 'Scintillation_w'],\n        ['Fried parameter', 'Isoplanatic angle', 'Rytov index'],\n        ['[m]', '[rad]', '[1]'],\n    ):\n\n        dirname = f'{data_dir}/{model_name}_score'\n        p_data = [d[param].detach().cpu() for d in measures]\n        if no_sign:\n            l_data = [d[lname].detach().cpu() for d in test_losses]\n        else:\n            pname = lname.split('_true')[0] + '_pred'\n            l_data = [((d[pname] - d[param]) / d[param]).detach().cpu()\n                      for d in measures]\n\n        pairs = sorted(zip(p_data, l_data))\n        params, loss = zip(*pairs)\n        params = np.array(params)\n        loss = np.array(loss)\n\n        if linear_bins:\n            bins = np.linspace(min(params), max(params), num=nbins)\n        else:\n            bins = np.logspace(np.log10(min(params)),\n                               np.log10(max(params)),\n                               num=nbins)\n        bin_indices = np.digitize(params, bins)\n        bin_means = [\n            loss[bin_indices == i].mean() if np.any(bin_indices == i) else 0\n            for i in range(1, len(bins))\n        ]\n        bin_stds = [\n            loss[bin_indices == i].std() if np.any(bin_indices == i) else 0\n            for i in range(1, len(bins))\n        ]\n        bin_centers = 0.5 * (bins[:-1] + bins[1:])\n\n        # Plotting the results\n        fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n        axs.errorbar(bin_centers,\n                     bin_means,\n                     yerr=bin_stds,\n                     marker='o',\n                     linestyle='-',\n                     alpha=0.75)\n\n        # Plot error reference lines+shade\n        axs.axhline(y=1.0, linestyle='--', color='tab:red', label='100% error')\n        axs.axhline(y=0.5,\n                    linestyle='--',\n                    color='tab:orange',\n                    label='50% error')\n        axs.axhline(y=0.1,\n                    linestyle='--',\n                    color='tab:green',\n                    label='10% error')\n        axs.axhline(y=-1.0, linestyle='--', color='tab:red')\n        axs.axhline(\n            y=-0.5,\n            linestyle='--',\n            color='tab:orange',\n        )\n        axs.axhline(\n            y=-0.1,\n            linestyle='--',\n            color='tab:green',\n        )\n        axs.axhline(\n            y=0,\n            linestyle='--',\n            color='black',\n        )\n        plt.tight_layout()\n        x_min, x_max = axs.get_xlim()\n        axs.fill_between([x_min, x_max],\n                         -0.1,\n                         0.1,\n                         color='tab:green',\n                         alpha=0.1)\n        axs.fill_between([x_min, x_max],\n                         0.1,\n                         0.5,\n                         color='tab:orange',\n                         alpha=0.1)\n        axs.fill_between([x_min, x_max],\n                         -0.5,\n                         -0.1,\n                         color='tab:orange',\n                         alpha=0.1)\n        axs.fill_between([x_min, x_max], 0.5, 1.0, color='tab:red', alpha=0.1)\n        axs.fill_between([x_min, x_max],\n                         -1.0,\n                         -0.5,\n                         color='tab:red',\n                         alpha=0.1)\n        axs.set_xlabel(f'{name} {units}', fontsize=12)\n        axs.set_xscale('log')\n        axs.set_yscale('symlog', linthresh=0.1)\n        axs.set_ylabel('Relative error', fontsize=12)\n        yticks = [-1, -0.5, -0.1, 0, 0.1, 0.5, 1]\n        plt.yticks(yticks)\n        yticklabels = ['-100%', '-50%', '-10%', '0', '10%', '50%', '100%']\n        plt.gca().set_yticklabels(yticklabels)\n        plt.title(f'{name} error', fontsize=15)\n        plt.tight_layout()\n        figure_format = conf.get('figure_format', 'png')\n        plt.savefig(f'{dirname}/{param}_vs_sum_{model_name}.{figure_format}')\n        plt.close()\n\n        # If specified, plot the histogram per single bin\n        if conf['preproc'].get(lname + '_details', False):\n            print(f'\\nComputing {lname} details')\n            dirname = f'{data_dir}/{model_name}_score/{lname}_bin_details'\n            ensure_directory(dirname)\n\n            for idx, single_bin in enumerate(bin_centers):\n                l_data = loss[bin_indices == idx]\n                if len(l_data) == 0:\n                    continue\n                fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n                axs.hist(l_data, bins=50, alpha=0.5, density=True)\n                mu = np.mean(l_data)\n                sigma = np.std(l_data)\n                if no_sign:\n                    print(\n                        'Warning: you are requesting the analysis of absolute value using'\n                        + ' normal gaussian assumption.' +\n                        'This is not correct and the error will be overestimated.'\n                    )\n                print(\n                    f'{lname} = {single_bin:.3f} -&gt; mu = {mu:.3f}, sigma = {sigma:.3f}'\n                )\n                if sigma &gt; 0:\n                    x = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 100)\n                    x = x[x &gt; min(l_data)]\n                    x = x[x &lt; max(l_data)]\n                    axs.plot(x,\n                             stats.norm.pdf(x, mu, sigma),\n                             label=f'Average err: {mu:.3f}, Std: {sigma:.3f}')\n                axs.set_xlabel(f'Relative error {lname}')\n                axs.set_ylabel('Frequency')\n                axs.legend()\n                plt.title(f'{lname} value = {single_bin:.3f} {units}')\n                plt.tight_layout()\n                plt.savefig(f'{dirname}/{lname}_bin{idx}.png')\n                plt.close()\n</code></pre>"},{"location":"api/plot/#speckcn2.plots.plot_samples_in_ensemble","title":"<code>plot_samples_in_ensemble(conf, test_set, device, model, criterion, trimming=0.2, n_max_plots=100)</code>","text":"<p>Plot the prediction over a sample and compare it with the ones from its ensemble.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>test_set</code>               (<code>list</code>)           \u2013            <p>The test set</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> <li> <code>model</code>               (<code>Torch</code>)           \u2013            <p>The trained model</p> </li> <li> <code>criterion</code>               (<code>ComposableLoss</code>)           \u2013            <p>The loss function</p> </li> <li> <code>trimming</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>The trimming to use for the mean</p> </li> <li> <code>n_max_plots</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The maximum number of plots</p> </li> </ul> Source code in <code>src/speckcn2/plots.py</code> <pre><code>def plot_samples_in_ensemble(conf: dict,\n                             test_set: list,\n                             device: Device,\n                             model: nn.Torch,\n                             criterion: ComposableLoss,\n                             trimming: float = 0.2,\n                             n_max_plots: int = 100) -&gt; None:\n    \"\"\"Plot the prediction over a sample and compare it with the ones\n    from its ensemble.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    test_set : list\n        The test set\n    device : torch.device\n        The device to use\n    model : nn.Torch\n        The trained model\n    criterion : ComposableLoss\n        The loss function\n    trimming : float\n        The trimming to use for the mean\n    n_max_plots : int\n        The maximum number of plots\n    \"\"\"\n\n    data_directory = conf['speckle']['datadirectory']\n    model_name = conf['model']['name']\n    n_screens = conf['speckle']['nscreens']\n\n    dirname = f'{data_directory}/{model_name}_score/single-shot_predictions'\n    ensure_directory(dirname)\n\n    # group the sets that have the same n[1]\n    grouped_test_set: Dict = {}\n    for n in test_set:\n        key = tuple(n[1])\n        if key not in grouped_test_set:\n            grouped_test_set[key] = []\n        grouped_test_set[key].append(n)\n    print('\\nAnalysis of single shot predictions')\n    print(f'Number of samples: {len(test_set)}')\n    print(f'Number of speckle groups: {len(grouped_test_set)}')\n    # Define a random probability to plot each ensemble\n    p_plot = n_max_plots / len(grouped_test_set)\n\n    # In the end, we will plot groups that have uncommon values of loss\n    loss_min = 1e10\n    loss_max = 0\n    ensemble_count = 0\n\n    ensemble = EnsembleModel(conf, device)\n    with torch.no_grad():\n        model.eval()\n\n        for key, value in grouped_test_set.items():\n            _outputs = []\n            _all_tags_pred = []\n\n            for count, speckle in enumerate(value, 1):\n                output, target, _ = ensemble(model, [speckle])\n                _outputs.append(output.detach().cpu().numpy())\n                loss, losses = criterion(output, target)\n\n                # Get the Cn2 profile and the recovered tags\n                Cn2_pred = criterion.reconstruct_cn2(output)\n                Cn2_true = criterion.reconstruct_cn2(target)\n                recovered_tag_pred = criterion.get_J(output)\n                _all_tags_pred.append(recovered_tag_pred.detach().cpu())\n                # and get all the measures\n                all_measures = criterion._get_all_measures(\n                    target, Cn2_true, output, Cn2_pred)\n\n                if count == 1:\n                    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n                    loss_0 = loss\n\n                    # (0) Plot the speckle pattern\n                    ax[0].axis('off')  # Hide axis\n                    ax[0].imshow(speckle[0][0, :, :], cmap='bone')\n\n                    # (1) Plot J vs nscreens\n                    recovered_tag_true = criterion.get_J(target)\n                    ax[1].plot(recovered_tag_true.squeeze(0).detach().cpu(),\n                               np.arange(recovered_tag_true.shape[1]),\n                               '*',\n                               label='True',\n                               color='tab:green',\n                               markersize=10,\n                               markeredgecolor='black',\n                               zorder=100)\n                    ax[1].plot(recovered_tag_pred.squeeze(0).detach().cpu(),\n                               np.arange(recovered_tag_pred.shape[1]),\n                               'o',\n                               label='This speckle',\n                               color='tab:red',\n                               markersize=7,\n                               markeredgecolor='black',\n                               zorder=90)\n\n                    # (2) Plot the parameters of this speckle prediction\n                    ax[2].axis('off')  # Hide axis\n                    recap_info = f'LOSS TERMS:\\nTotal Loss: {loss.item():.4g}\\n'\n                    # the individual losses\n                    for key, value in losses.items():\n                        recap_info += f'{key}: {value.item():.4g}\\n'\n                    recap_info += '-------------------\\nPARAMETERS:\\n'\n                    # then the single parameters\n                    for key, value in all_measures.items():\n                        recap_info += f'{key}: {value:.4g}\\n'\n                    ax[2].text(0.5,\n                               0.5,\n                               recap_info,\n                               horizontalalignment='center',\n                               verticalalignment='center',\n                               fontsize=10,\n                               color='black')\n\n            # Now at the end of the loop, we decide if this set needs to plotted or not\n            # by checking that the loss is uncommon, or via a random probability\n            if loss_0 &gt; loss_max or loss_0 &lt; loss_min or np.random.rand(\n            ) &lt; p_plot:\n                avg_tags_trim = stats.trim_mean(_all_tags_pred,\n                                                trimming).squeeze()\n                percentiles_50 = np.percentile(_all_tags_pred, [25, 75],\n                                               axis=0).squeeze()\n                percentiles_68 = np.percentile(_all_tags_pred, [16, 84],\n                                               axis=0).squeeze()\n                percentiles_95 = np.percentile(_all_tags_pred, [2.5, 97.5],\n                                               axis=0).squeeze()\n\n                y_vals = np.arange(n_screens)\n                alp = 0.3\n                ax[1].plot(avg_tags_trim,\n                           y_vals,\n                           label='Mean',\n                           color='tab:red',\n                           zorder=50)\n                ax[1].fill_betweenx(y_vals,\n                                    percentiles_50[0],\n                                    percentiles_50[1],\n                                    color='gold',\n                                    alpha=alp,\n                                    label='50% CI',\n                                    zorder=5)\n                ax[1].fill_betweenx(y_vals,\n                                    percentiles_68[0],\n                                    percentiles_50[0],\n                                    color='cadetblue',\n                                    alpha=alp,\n                                    label='68% CI',\n                                    zorder=4)\n                ax[1].fill_betweenx(y_vals,\n                                    percentiles_50[1],\n                                    percentiles_68[1],\n                                    color='cadetblue',\n                                    alpha=alp,\n                                    zorder=4)\n                ax[1].fill_betweenx(y_vals,\n                                    percentiles_95[0],\n                                    percentiles_68[0],\n                                    color='blue',\n                                    label='95% CI',\n                                    alpha=alp,\n                                    zorder=3)\n                ax[1].fill_betweenx(y_vals,\n                                    percentiles_68[1],\n                                    percentiles_95[1],\n                                    color='blue',\n                                    alpha=alp,\n                                    zorder=3)\n\n                ax[1].set_xscale('log')\n                ax[1].set_ylabel('# screen')\n                ax[1].set_xlabel('J')\n                ax[1].legend()\n                fig.tight_layout()\n                plt.subplots_adjust(top=0.92)\n                plt.suptitle(\n                    'Prediction from a single speckle, compared to similar')\n                figure_format = conf.get('figure_format', 'png')\n                plt.savefig(\n                    f'{dirname}/single_speckle_loss{loss_0.item():.4g}.{figure_format}'\n                )\n                loss_max = max(loss_0, loss_max)\n                loss_min = min(loss_0, loss_min)\n                ensemble_count += 1\n\n            plt.close()\n\n            if ensemble_count &gt;= n_max_plots:\n                break\n</code></pre>"},{"location":"api/plot/#speckcn2.plots.plot_time","title":"<code>plot_time(conf, model, data_dir)</code>","text":"<p>Plots the time per epoch of the model.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to plot the loss of</p> </li> <li> <code>data_dir</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> </ul> Source code in <code>src/speckcn2/plots.py</code> <pre><code>def plot_time(conf: dict, model, data_dir: str) -&gt; None:\n    \"\"\"Plots the time per epoch of the model.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    model : torch.nn.Module\n        The model to plot the loss of\n    data_dir : str\n        The directory where the data is stored\n    \"\"\"\n\n    model_name = conf['model']['name']\n    data_dir = conf['speckle']['datadirectory']\n\n    dirname = f'{data_dir}/{model_name}_score'\n    ensure_directory(dirname)\n\n    fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n    axs.plot(model.epoch, model.time, label='Time per epoch')\n    axs.set_xlabel('Epoch')\n    axs.set_ylabel('Time [s]')\n    axs.legend()\n    plt.title(f'Model: {model_name}')\n    plt.tight_layout()\n    figure_format = conf.get('figure_format', 'png')\n    plt.savefig(f'{dirname}/time_{model_name}.{figure_format}')\n    plt.close()\n</code></pre>"},{"location":"api/plot/#speckcn2.plots.plot_total_statistics","title":"<code>plot_total_statistics(conf, train_set, criterion, device, bins=50, alpha=0.5)</code>","text":"<p>Plot the total statistics of:     1) Total turbulence integral     2) Fried parameter     3) Isoplanatic angle     4) Rytov index as a 4 panel plot.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>train_set</code>               (<code>list</code>)           \u2013            <p>The full dataset used for training</p> </li> <li> <code>criterion</code>               (<code>ComposableLoss</code>)           \u2013            <p>The loss function</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> <li> <code>bins</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>The number of bins in which to partition the data</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The transparency of the histogram</p> </li> </ul> Source code in <code>src/speckcn2/plots.py</code> <pre><code>def plot_total_statistics(\n    conf: dict,\n    train_set: list,\n    criterion: ComposableLoss,\n    device: Device,\n    bins: int = 50,\n    alpha: float = 0.5,\n) -&gt; None:\n    \"\"\"Plot the total statistics of:\n        1) Total turbulence integral\n        2) Fried parameter\n        3) Isoplanatic angle\n        4) Rytov index\n    as a 4 panel plot.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    train_set : list\n        The full dataset used for training\n    criterion : ComposableLoss\n        The loss function\n    device : torch.device\n        The device to use\n    bins : int\n        The number of bins in which to partition the data\n    alpha : float\n        The transparency of the histogram\n    \"\"\"\n    data_directory = conf['speckle']['datadirectory']\n    model_name = conf['model']['name']\n    ensemble = conf['preproc'].get('ensemble', 1)\n\n    dirname = f'{data_directory}/{model_name}_score'\n    ensure_directory(dirname)\n\n    # Get the tags from the training set\n    if ensemble &gt; 1:\n        train_set = list(itertools.chain(*train_set))\n    _, tags, _ = zip(*train_set)\n    tags = np.stack(tags)\n    train_tags = np.array([n for n in tags])\n\n    # The total turbulence integral is the sum of J accross all screens\n    tti = np.sum(train_tags, axis=1)\n    # and prepare storage for the other parameters\n    fried = np.zeros_like(tti)\n    iso = np.zeros_like(tti)\n    rytov = np.zeros_like(tti)\n\n    for i in range(tti.shape[0]):\n        tags_tensor = torch.tensor(tags[i, :], device=device).unsqueeze(0)\n        Cn2 = criterion.reconstruct_cn2(tags_tensor)\n        J = criterion.get_J(tags_tensor)\n        tti[i] = J.sum()\n        all_measures = criterion._get_all_measures(tags_tensor, Cn2)\n        fried[i] = all_measures['Fried_true']\n        iso[i] = all_measures['Isoplanatic_true']\n        rytov[i] = all_measures['Scintillation_w_true']\n\n    # Compute the logarithm of the data\n    log_tti = np.log10(tti)\n    log_fried = np.log10(fried)\n    log_iso = np.log10(iso)\n    log_rytov = np.log10(rytov)\n\n    # And now plot a 4 panel figure histogram\n    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n    axs[0, 0].hist(log_tti, bins=bins, alpha=alpha, density=True)\n    axs[0, 0].set_xlabel('Total turbulence integral (logscale)')\n    axs[0, 0].set_ylabel('Frequency')\n    axs[0, 1].hist(log_fried, bins=bins, alpha=alpha, density=True)\n    axs[0, 1].set_xlabel('Fried parameter [m] (logscale)')\n    axs[0, 1].set_ylabel('Frequency')\n    axs[1, 0].hist(log_iso, bins=bins, alpha=alpha, density=True)\n    axs[1, 0].set_xlabel('Isoplanatic angle [rad] (logscale)')\n    axs[1, 0].set_ylabel('Frequency')\n    axs[1, 1].hist(log_rytov, bins=bins, alpha=alpha, density=True)\n    axs[1, 1].set_xlabel('Rytov index (logscale)')\n    axs[1, 1].set_ylabel('Frequency')\n    plt.tight_layout()\n    figure_format = conf.get('figure_format', 'png')\n    plt.savefig(f'{dirname}/total_statistics_{model_name}.{figure_format}')\n    plt.close()\n</code></pre>"},{"location":"api/plot/#speckcn2.plots.score_plot","title":"<code>score_plot(conf, inputs, tags, loss, losses, i, counter, measures, Cn2_pred, Cn2_true, recovered_tag_pred, recovered_tag_true)</code>","text":"<p>Plots side by side: - [0:Nensemble] the input images (single or ensemble) - [-3] the predicted/exact tags J - [-2] the Cn2 profile - [-1] the different information of the loss normalize value in model units.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>inputs</code>               (<code>Tensor</code>)           \u2013            <p>The input speckle patterns</p> </li> <li> <code>tags</code>               (<code>list</code>)           \u2013            <p>The exact tags of the data</p> </li> <li> <code>loss</code>               (<code>Tensor</code>)           \u2013            <p>The total loss of the model (for this prediction)</p> </li> <li> <code>losses</code>               (<code>dict</code>)           \u2013            <p>The individual losses of the model</p> </li> <li> <code>i</code>               (<code>int</code>)           \u2013            <p>The batch index of the image</p> </li> <li> <code>counter</code>               (<code>int</code>)           \u2013            <p>The global index of the image</p> </li> <li> <code>measures</code>               (<code>dict</code>)           \u2013            <p>The different measures of the model</p> </li> <li> <code>Cn2_pred</code>               (<code>Tensor</code>)           \u2013            <p>The predicted Cn2 profile</p> </li> <li> <code>Cn2_true</code>               (<code>Tensor</code>)           \u2013            <p>The true Cn2 profile</p> </li> <li> <code>recovered_tag_pred</code>               (<code>Tensor</code>)           \u2013            <p>The predicted tags</p> </li> <li> <code>recovered_tag_true</code>               (<code>Tensor</code>)           \u2013            <p>The true tags</p> </li> </ul> Source code in <code>src/speckcn2/plots.py</code> <pre><code>def score_plot(\n    conf: dict,\n    inputs: torch.Tensor,\n    tags: list,\n    loss: torch.Tensor,\n    losses: dict,\n    i: int,\n    counter: int,\n    measures: dict,\n    Cn2_pred: torch.Tensor,\n    Cn2_true: torch.Tensor,\n    recovered_tag_pred: torch.Tensor,\n    recovered_tag_true: torch.Tensor,\n) -&gt; None:\n    \"\"\"Plots side by side:\n    - [0:Nensemble] the input images (single or ensemble)\n    - [-3] the predicted/exact tags J\n    - [-2] the Cn2 profile\n    - [-1] the different information of the loss\n    normalize value in model units.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    inputs : torch.Tensor\n        The input speckle patterns\n    tags : list\n        The exact tags of the data\n    loss : torch.Tensor\n        The total loss of the model (for this prediction)\n    losses : dict\n        The individual losses of the model\n    i : int\n        The batch index of the image\n    counter : int\n        The global index of the image\n    measures : dict\n        The different measures of the model\n    Cn2_pred : torch.Tensor\n        The predicted Cn2 profile\n    Cn2_true : torch.Tensor\n        The true Cn2 profile\n    recovered_tag_pred : torch.Tensor\n        The predicted tags\n    recovered_tag_true : torch.Tensor\n        The true tags\n    \"\"\"\n    model_name = conf['model']['name']\n    data_dir = conf['speckle']['datadirectory']\n    ensemble = conf['preproc'].get('ensemble', 1)\n    hs = conf['speckle']['splits']\n    nscreens = conf['speckle']['nscreens']\n    if len(hs) != nscreens:\n        print(\n            'WARNING: The number of screens does not match the number of splits'\n        )\n        return\n\n    dirname = f'{data_dir}/{model_name}_score/single-shot_predictions'\n    ensure_directory(dirname)\n\n    fig, axs = plt.subplots(1, 3 + ensemble, figsize=(4 * (2 + ensemble), 3.5))\n\n    # (1) Plot the input images\n    for n in range(ensemble):\n        img = inputs[ensemble * i + n].detach().cpu().squeeze().abs()\n        axs[n].imshow(img, cmap='bone')\n    title_string = f'Input {ensemble} images' if ensemble &gt; 1 else 'Input single image'\n    axs[1].set_title(title_string)\n\n    # (2) Plot J vs nscreens\n    axs[-3].plot(recovered_tag_true.squeeze(0).detach().cpu(),\n                 'o',\n                 label='True')\n    axs[-3].plot(recovered_tag_pred.squeeze(0).detach().cpu(),\n                 '.',\n                 color='tab:red',\n                 label='Predicted')\n    axs[-3].set_yscale('log')\n    axs[-3].set_ylabel('J')\n    axs[-3].set_xlabel('# screen')\n    axs[-3].legend()\n\n    # (3) Plot Cn2 vs altitude\n    axs[-2].plot(hs, Cn2_true.squeeze(0).detach().cpu(), 'o', label='True')\n    axs[-2].plot(hs,\n                 Cn2_pred.squeeze(0).detach().cpu(),\n                 '.',\n                 color='tab:red',\n                 label='Predicted')\n    axs[-2].set_xscale('log')\n    axs[-2].set_yscale('log')\n    axs[-2].set_ylabel(r'$Cn^2$')\n    axs[-2].set_xlabel('Altitude [m]')\n\n    # (4) Plot the recap information\n    axs[-1].axis('off')  # Hide axis\n    recap_info = f'LOSS TERMS:\\nTotal Loss: {loss.item():.4g}\\n'\n    # the individual losses\n    for key, value in losses.items():\n        recap_info += f'{key}: {value.item():.4g}\\n'\n    recap_info += '-------------------\\nPARAMETERS:\\n'\n    # then the single parameters\n    for key, value in measures.items():\n        recap_info += f'{key}: {value:.4g}\\n'\n    axs[-1].text(0.5,\n                 0.5,\n                 recap_info,\n                 horizontalalignment='center',\n                 verticalalignment='center',\n                 fontsize=10,\n                 color='black')\n\n    plt.tight_layout()\n    figure_format = conf.get('figure_format', 'png')\n    plt.savefig(\n        f'{dirname}/single_speckle_loss{loss.item():.4g}.{figure_format}')\n    plt.close()\n</code></pre>"},{"location":"api/postprocess/","title":"Postprocess","text":""},{"location":"api/postprocess/#speckcn2.postprocess.average_speckle_input","title":"<code>average_speckle_input(conf, test_set, device, model, criterion, n_ensembles_to_plot=100)</code>","text":"<p>Test to see if averaging the speckle patterns (before the prediction) improves the results. This function is then going to plot the relative error over the screen tags and the Fried parameter to make this evaluation.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>test_set</code>               (<code>list</code>)           \u2013            <p>The test set</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> <li> <code>model</code>               (<code>Torch</code>)           \u2013            <p>The trained model</p> </li> <li> <code>criterion</code>               (<code>ComposableLoss</code>)           \u2013            <p>The loss function</p> </li> <li> <code>n_ensembles_to_plot</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The number of ensembles to plot</p> </li> </ul> Source code in <code>src/speckcn2/postprocess.py</code> <pre><code>def average_speckle_input(conf: dict,\n                          test_set: list,\n                          device: Device,\n                          model: nn.Torch,\n                          criterion: ComposableLoss,\n                          n_ensembles_to_plot: int = 100) -&gt; None:\n    \"\"\"Test to see if averaging the speckle patterns (before the prediction)\n    improves the results. This function is then going to plot the relative\n    error over the screen tags and the Fried parameter to make this evaluation.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    test_set : list\n        The test set\n    device : torch.device\n        The device to use\n    model : nn.Torch\n        The trained model\n    criterion : ComposableLoss\n        The loss function\n    n_ensembles_to_plot : int\n        The number of ensembles to plot\n    \"\"\"\n\n    data_directory = conf['speckle']['datadirectory']\n    model_name = conf['model']['name']\n    figure_format = conf.get('figure_format', 'png')\n\n    dirname = f'{data_directory}/{model_name}_score/effect_averaging'\n    ensure_directory(dirname)\n\n    # group the sets that have the same n[1]\n    grouped_test_set: Dict = {}\n    for n in test_set:\n        key = tuple(n[1])\n        if key not in grouped_test_set:\n            grouped_test_set[key] = []\n        grouped_test_set[key].append(n)\n    print('\\nChecking if averaging speckle patterns improves results')\n    print(f'Number of samples: {len(test_set)}')\n    print(f'Number of speckle groups: {len(grouped_test_set)}')\n\n    # For each group compare the model prediction to the exact tag\n    ensemble = EnsembleModel(conf, device)\n    with torch.no_grad():\n        model.eval()\n\n        for ensemble_count, (key,\n                             value) in enumerate(grouped_test_set.items()):\n            avg_speckle = None\n            cmap = plt.get_cmap('coolwarm')\n            norm = plt.Normalize(1, len(value))\n\n            if ensemble_count &gt; n_ensembles_to_plot:\n                continue\n\n            for count, speckle in enumerate(value, 1):\n                color = cmap(norm(count))\n\n                if avg_speckle is None:\n                    avg_speckle = speckle\n                else:\n                    avg_speckle = (torch.add(avg_speckle[0],\n                                             speckle[0]), *avg_speckle[1:])\n\n                # Average only the speckle pattern (first element)\n                avg_speckle_divided = (torch.div(avg_speckle[0],\n                                                 count), *avg_speckle[1:])\n\n                output, target, _ = ensemble(model, [avg_speckle_divided])\n                loss, losses = criterion(output, target)\n\n                if count == 1:\n                    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n\n                    recovered_tag_true = criterion.get_J(target)\n                    ax[0].plot(recovered_tag_true.squeeze(0).detach().cpu(),\n                               '*',\n                               label='True',\n                               color='tab:green',\n                               zorder=100)\n                    blue_patch = mpatches.Patch(color=color,\n                                                label='One speckle')\n\n                ax[1].plot((torch.abs(output - target) /\n                            (target + 1e-7)).flatten().detach().cpu(),\n                           color=color)\n\n                # Get the Cn2 profile and the recovered tags\n                Cn2_pred = criterion.reconstruct_cn2(output)\n                Cn2_true = criterion.reconstruct_cn2(target)\n                recovered_tag_pred = criterion.get_J(output / count)\n                ax[0].plot(recovered_tag_pred.squeeze(0).detach().cpu(),\n                           '.',\n                           color=color)\n                # and get all the measures\n                all_measures = criterion._get_all_measures(\n                    target, Cn2_true, output, Cn2_pred)\n\n                Fried_err = torch.abs(\n                    all_measures['Fried_true'] -\n                    all_measures['Fried_pred']) / all_measures['Fried_true']\n                ax[2].scatter(count, Fried_err.detach().cpu(), color=color)\n                if count == 1:\n                    ax[2].plot(\n                        [], [],\n                        ' ',\n                        label='(True) Fried = {:.3f}'.format(\n                            all_measures['Fried_true'].detach().cpu().numpy()))\n\n            ax[0].set_yscale('log')\n            ax[0].set_ylabel('J')\n            ax[0].set_xlabel('# screen')\n            red_patch = mpatches.Patch(color=color, label='All speckles')\n            handles, labels = ax[0].get_legend_handles_labels()\n            handles.extend([blue_patch, red_patch])\n            ax[0].legend(handles=handles)\n            ax[1].set_xlabel('# screen')\n            ax[1].set_ylabel('J relative error ')\n            ax[2].set_xlabel('# averaged speckles')\n            ax[2].set_ylabel('Fried relative error')\n            ax[1].set_yscale('log')\n            ax[2].set_yscale('log')\n            ax[2].legend(frameon=False)\n            fig.tight_layout()\n            plt.subplots_adjust(top=0.92)\n            plt.suptitle('Effect of averaging speckle patterns')\n            plt.savefig(\n                f'{dirname}/average_speckle_loss{loss.item():.4g}.{figure_format}'\n            )\n            plt.close()\n</code></pre>"},{"location":"api/postprocess/#speckcn2.postprocess.average_speckle_output","title":"<code>average_speckle_output(conf, test_set, device, model, criterion, trimming=0.1, n_ensembles_to_plot=100)</code>","text":"<p>Test to see if averaging the prediction of multiple speckle patterns improves the results. This function is then going to plot the relative error over the screen tags and the Fried parameter to make this evaluation.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>test_set</code>               (<code>list</code>)           \u2013            <p>The test set</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> <li> <code>model</code>               (<code>Torch</code>)           \u2013            <p>The trained model</p> </li> <li> <code>criterion</code>               (<code>ComposableLoss</code>)           \u2013            <p>The loss function</p> </li> <li> <code>n_ensembles_to_plot</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The number of ensembles to plot</p> </li> </ul> Source code in <code>src/speckcn2/postprocess.py</code> <pre><code>def average_speckle_output(conf: dict,\n                           test_set: list,\n                           device: Device,\n                           model: nn.Torch,\n                           criterion: ComposableLoss,\n                           trimming: float = 0.1,\n                           n_ensembles_to_plot: int = 100) -&gt; None:\n    \"\"\"Test to see if averaging the prediction of multiple speckle patterns\n    improves the results. This function is then going to plot the relative\n    error over the screen tags and the Fried parameter to make this evaluation.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    test_set : list\n        The test set\n    device : torch.device\n        The device to use\n    model : nn.Torch\n        The trained model\n    criterion : ComposableLoss\n        The loss function\n    n_ensembles_to_plot : int\n        The number of ensembles to plot\n    \"\"\"\n\n    data_directory = conf['speckle']['datadirectory']\n    model_name = conf['model']['name']\n    n_screens = conf['speckle']['nscreens']\n    figure_format = conf.get('figure_format', 'png')\n\n    dirname = f'{data_directory}/{model_name}_score/effect_averaging'\n    ensure_directory(dirname)\n\n    # group the sets that have the same n[1]\n    grouped_test_set: Dict = {}\n    for n in test_set:\n        key = tuple(n[1])\n        if key not in grouped_test_set:\n            grouped_test_set[key] = []\n        grouped_test_set[key].append(n)\n    print('\\nChecking if averaging speckle predictions improves results')\n    print(f'Number of samples: {len(test_set)}')\n    print(f'Number of speckle groups: {len(grouped_test_set)}')\n\n    # In the end, we will plot groups that have uncommon values of loss\n    loss_min = 1e10\n    loss_max = 0\n    ensemble_count = 0\n\n    ensemble = EnsembleModel(conf, device)\n    with torch.no_grad():\n        model.eval()\n\n        for key, value in grouped_test_set.items():\n            _outputs = []\n            _all_tags_pred = []\n            cmap = plt.get_cmap('coolwarm')\n            norm = plt.Normalize(1, len(value))\n\n            for count, speckle in enumerate(value, 1):\n                color = cmap(norm(count))\n                output, target, _ = ensemble(model, [speckle])\n\n                if count == 1:\n                    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n\n                    # (1) Plot J vs nscreens\n                    recovered_tag_true = criterion.get_J(target)\n                    ax[0].plot(recovered_tag_true.squeeze(0).detach().cpu(),\n                               '*',\n                               label='True',\n                               color='tab:green',\n                               markersize=10,\n                               markeredgecolor='black',\n                               zorder=100)\n                    ax[1].plot(recovered_tag_true.squeeze(0).detach().cpu(),\n                               '*',\n                               label='True',\n                               color='tab:green',\n                               markersize=10,\n                               markeredgecolor='black',\n                               zorder=100)\n                    blue_patch = mpatches.Patch(color=color,\n                                                label='One speckle')\n\n                # Use the trimmed mean to get the average output\n                # (trim_mean works only on cpu so you have to move back and forth)\n                _outputs.append(output.detach().cpu().numpy())\n                avg_output = torch.tensor(stats.trim_mean(_outputs,\n                                                          trimming)).to(device)\n\n                loss, losses = criterion(avg_output, target)\n\n                # Get the Cn2 profile and the recovered tags\n                Cn2_pred = criterion.reconstruct_cn2(avg_output)\n                Cn2_true = criterion.reconstruct_cn2(target)\n                recovered_tag_pred = criterion.get_J(avg_output)\n                _all_tags_pred.append(recovered_tag_pred.detach().cpu())\n                ax[1].plot(recovered_tag_pred.squeeze(0).detach().cpu(),\n                           'o',\n                           color=color)\n                # and get all the measures\n                all_measures = criterion._get_all_measures(\n                    target, Cn2_true, avg_output, Cn2_pred)\n\n                Fried_err = torch.abs(\n                    all_measures['Fried_true'] -\n                    all_measures['Fried_pred']) / all_measures['Fried_true']\n                ax[2].scatter(count, Fried_err.detach().cpu(), color=color)\n                if count == 1:\n                    ax[2].plot(\n                        [], [],\n                        ' ',\n                        label='(True) Fried = {:.3f}'.format(\n                            all_measures['Fried_true'].detach().cpu().numpy()))\n\n            # Now at the end of the loop, we decide if this set needs to plotted or not\n            # by checking that the loss\n            if loss &gt; loss_max or loss &lt; loss_min:\n                avg_tags_trim = stats.trim_mean(_all_tags_pred,\n                                                trimming).squeeze()\n                percentiles_50 = np.percentile(_all_tags_pred, [25, 75],\n                                               axis=0).squeeze()\n                percentiles_68 = np.percentile(_all_tags_pred, [16, 84],\n                                               axis=0).squeeze()\n                percentiles_95 = np.percentile(_all_tags_pred, [2.5, 97.5],\n                                               axis=0).squeeze()\n\n                x_vals = np.arange(n_screens)\n                alp = 1\n                ax[0].plot(avg_tags_trim,\n                           label='Mean',\n                           color='tab:red',\n                           zorder=50)\n                ax[0].fill_between(x_vals,\n                                   percentiles_50[0],\n                                   percentiles_50[1],\n                                   color='gold',\n                                   alpha=alp,\n                                   label='50% CI',\n                                   zorder=5)\n                ax[0].fill_between(x_vals,\n                                   percentiles_68[0],\n                                   percentiles_50[0],\n                                   color='cadetblue',\n                                   alpha=alp,\n                                   label='68% CI',\n                                   zorder=4)\n                ax[0].fill_between(x_vals,\n                                   percentiles_50[1],\n                                   percentiles_68[1],\n                                   color='cadetblue',\n                                   alpha=alp,\n                                   zorder=4)\n                ax[0].fill_between(x_vals,\n                                   percentiles_95[0],\n                                   percentiles_68[0],\n                                   color='blue',\n                                   label='95% CI',\n                                   alpha=alp,\n                                   zorder=3)\n                ax[0].fill_between(x_vals,\n                                   percentiles_68[1],\n                                   percentiles_95[1],\n                                   color='blue',\n                                   alpha=alp,\n                                   zorder=3)\n\n                ax[3].axis('off')  # Hide axis\n                recap_info = f'LOSS TERMS:\\nTotal Loss: {loss.item():.4g}\\n'\n                # the individual losses\n                for key, value in losses.items():\n                    recap_info += f'{key}: {value.item():.4g}\\n'\n                recap_info += '-------------------\\nPARAMETERS:\\n'\n                # then the single parameters\n                for key, value in all_measures.items():\n                    recap_info += f'{key}: {value:.4g}\\n'\n                ax[3].text(0.5,\n                           0.5,\n                           recap_info,\n                           horizontalalignment='center',\n                           verticalalignment='center',\n                           fontsize=10,\n                           color='black')\n\n                ax[0].set_yscale('log')\n                ax[0].set_ylabel('J')\n                ax[0].set_xlabel('# screen')\n                ax[0].legend()\n                ax[1].set_yscale('log')\n                ax[1].set_ylabel('J')\n                ax[1].set_xlabel('# screen')\n                red_patch = mpatches.Patch(color=color, label='All speckles')\n                handles, labels = ax[1].get_legend_handles_labels()\n                handles.extend([blue_patch, red_patch])\n                ax[1].legend(handles=handles)\n                ax[2].set_xlabel('# averaged speckles')\n                ax[2].set_ylabel('Fried relative error')\n                ax[2].legend(frameon=False)\n                ax[2].set_yscale('log')\n                fig.tight_layout()\n                plt.subplots_adjust(top=0.92)\n                plt.suptitle('Effect of averaging speckle predictions')\n                plt.savefig(\n                    f'{dirname}/average_ensemble_loss{loss.item():.4g}.{figure_format}'\n                )\n                loss_max = max(loss, loss_max)\n                loss_min = min(loss, loss_min)\n                ensemble_count += 1\n\n            plt.close()\n\n            if ensemble_count &gt; n_ensembles_to_plot:\n                break\n</code></pre>"},{"location":"api/postprocess/#speckcn2.postprocess.screen_errors","title":"<code>screen_errors(conf, device, J_pred, J_true, nbins=20, trimming=0.1)</code>","text":"<p>Plot the relative error of Cn2 for each screen.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device on which the data are stored</p> </li> <li> <code>J_pred</code>               (<code>Tensor</code>)           \u2013            <p>The predicted J profile</p> </li> <li> <code>J_true</code>               (<code>Tensor</code>)           \u2013            <p>The true J profile</p> </li> <li> <code>nbins</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Number of bins to use for the histograms</p> </li> <li> <code>trimming</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of data to trim from each end of the distribution</p> </li> </ul> Source code in <code>src/speckcn2/postprocess.py</code> <pre><code>def screen_errors(conf: dict,\n                  device: Device,\n                  J_pred: Tensor,\n                  J_true: Tensor,\n                  nbins: int = 20,\n                  trimming: float = 0.1) -&gt; None:\n    \"\"\"Plot the relative error of Cn2 for each screen.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    device : torch.device\n        The device on which the data are stored\n    J_pred : torch.Tensor\n        The predicted J profile\n    J_true : torch.Tensor\n        The true J profile\n    nbins : int, optional\n        Number of bins to use for the histograms\n    trimming : float, optional\n        The fraction of data to trim from each end of the distribution\n    \"\"\"\n    data_directory = conf['speckle']['datadirectory']\n    model_name = conf['model']['name']\n    n_screens = conf['speckle']['nscreens']\n    figure_format = conf.get('figure_format', 'png')\n\n    dirname = f'{data_directory}/{model_name}_score'\n    ensure_directory(dirname)\n\n    # Plot the distribution of each tag element\n    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n    for si in range(n_screens):\n\n        if device == 'cpu':\n            data_pred = np.asarray(J_pred)[:, 0, si]\n            data_true = np.asarray(J_true)[:, 0, si]\n        else:\n            data_pred = np.asarray([d.detach().cpu().numpy()\n                                    for d in J_pred])[:, 0, si]\n            data_true = np.asarray([d.detach().cpu().numpy()\n                                    for d in J_true])[:, 0, si]\n        # Define the bins\n        bins = np.linspace(min(data_true), max(data_true), nbins + 1)\n\n        # Digitize the data to get bin indices\n        bin_indices = np.digitize(data_true, bins)\n\n        # Compute the relative error for each bin\n        relative_errors = []\n        percentiles_50 = []\n        percentiles_68 = []\n        percentiles_95 = []\n\n        for bi in range(1, len(bins)):\n            model_bin_values = data_pred[bin_indices == bi]\n            true_bin_values = data_true[bin_indices == bi]\n\n            if len(true_bin_values) &gt; 0:\n                relative_error = np.abs(\n                    (model_bin_values - true_bin_values) / true_bin_values)\n                relative_errors.append(\n                    stats.trim_mean(relative_error, trimming))\n                percentiles_50.append(\n                    np.percentile(relative_error, [25, 75], axis=0))\n                percentiles_68.append(\n                    np.percentile(relative_error, [16, 84], axis=0))\n                percentiles_95.append(\n                    np.percentile(relative_error, [2.5, 97.5], axis=0))\n            else:\n                relative_errors.append(0)\n                percentiles_50.append([0, 0])\n                percentiles_68.append([0, 0])\n                percentiles_95.append([0, 0])\n\n        percentiles_50 = np.asarray(percentiles_50)\n        percentiles_68 = np.asarray(percentiles_68)\n        percentiles_95 = np.asarray(percentiles_95)\n        # Plot the relative errors\n        axs[si // 4, si % 4].plot(bins[:-1],\n                                  relative_errors,\n                                  label='Mean',\n                                  color='tab:red',\n                                  zorder=50)\n        axs[si // 4, si % 4].set_title(f'Layer {si}')\n        # and the percentiles\n        axs[si // 4, si % 4].fill_between(bins[:-1],\n                                          percentiles_50[:, 0],\n                                          percentiles_50[:, 1],\n                                          color='gold',\n                                          alpha=0.5,\n                                          label='50% CI',\n                                          zorder=5)\n        axs[si // 4, si % 4].fill_between(bins[:-1],\n                                          percentiles_68[:, 0],\n                                          percentiles_50[:, 0],\n                                          color='cadetblue',\n                                          alpha=0.5,\n                                          label='68% CI',\n                                          zorder=4)\n        axs[si // 4, si % 4].fill_between(bins[:-1],\n                                          percentiles_50[:, 1],\n                                          percentiles_68[:, 1],\n                                          color='cadetblue',\n                                          alpha=0.5,\n                                          zorder=4)\n        axs[si // 4, si % 4].fill_between(bins[:-1],\n                                          percentiles_95[:, 0],\n                                          percentiles_68[:, 0],\n                                          color='blue',\n                                          label='95% CI',\n                                          alpha=0.5,\n                                          zorder=3)\n        axs[si // 4, si % 4].fill_between(bins[:-1],\n                                          percentiles_68[:, 1],\n                                          percentiles_95[:, 1],\n                                          color='blue',\n                                          alpha=0.5,\n                                          zorder=3)\n        axs[si // 4, si % 4].set_yscale('symlog', linthresh=0.1)\n    axs[0, 1].legend()\n    for ax in axs.flatten():\n        ax.axhline(\n            y=0,\n            linestyle='--',\n            color='black',\n        )\n    vals = axs[0, 0].get_yticks()\n    axs[0, 0].set_yticklabels(['{:.0f}'.format(x * 100) + '%' for x in vals])\n    vals = axs[-1, -1].get_yticks()\n    axs[-1, -1].set_yticklabels(['{:.0f}'.format(x * 100) + '%' for x in vals])\n    for i in range(1, n_screens - 1):\n        axs.flat[i].sharey(axs.flat[0])\n    plt.suptitle('Relative Error of J')\n    plt.tight_layout()\n    figname = f'{dirname}/{model_name}_Jerrors'\n    plt.savefig(f'{figname}.{figure_format}')\n    plt.close()\n</code></pre>"},{"location":"api/postprocess/#speckcn2.postprocess.tags_distribution","title":"<code>tags_distribution(conf, train_set, test_tags, device, nbins=20, rescale=False, recover_tag=None)</code>","text":"<p>Function to plot the following: - distribution of the tags for unscaled results - distribution of the tags for rescaled results - distribution of the sum of the tags.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>train_set</code>               (<code>list</code>)           \u2013            <p>The training set</p> </li> <li> <code>test_tags</code>               (<code>Tensor</code>)           \u2013            <p>The predicted tags for the test dataset</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>The device to use</p> </li> <li> <code>data_directory</code>               (<code>str</code>)           \u2013            <p>The directory where the data is stored</p> </li> <li> <code>nbins</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>Number of bins to use for the histograms</p> </li> <li> <code>rescale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to rescale the tags using recover_tag() or leave them between 0 and 1</p> </li> <li> <code>recover_tag</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>List of functions to recover each tag</p> </li> </ul> Source code in <code>src/speckcn2/postprocess.py</code> <pre><code>def tags_distribution(conf: dict,\n                      train_set: list,\n                      test_tags: Tensor,\n                      device: Device,\n                      nbins: int = 20,\n                      rescale: bool = False,\n                      recover_tag: Optional[list[Callable]] = None) -&gt; None:\n    \"\"\"Function to plot the following:\n    - distribution of the tags for unscaled results\n    - distribution of the tags for rescaled results\n    - distribution of the sum of the tags.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    train_set : list\n        The training set\n    test_tags : torch.Tensor\n        The predicted tags for the test dataset\n    device : torch.device\n        The device to use\n    data_directory : str\n        The directory where the data is stored\n    nbins : int, optional\n        Number of bins to use for the histograms\n    rescale : bool, optional\n        Whether to rescale the tags using recover_tag() or leave them between 0 and 1\n    recover_tag : list, optional\n        List of functions to recover each tag\n    \"\"\"\n\n    data_directory = conf['speckle']['datadirectory']\n    model_name = conf['model']['name']\n    ensemble = conf['preproc'].get('ensemble', 1)\n    figure_format = conf.get('figure_format', 'png')\n\n    dirname = f'{data_directory}/{model_name}_score'\n    ensure_directory(dirname)\n\n    # Get the tags from the training set\n    if ensemble &gt; 1:\n        train_set = list(itertools.chain(*train_set))\n    _, tags, _ = zip(*train_set)\n    tags = np.stack(tags)\n    train_tags = np.array([n for n in tags])\n\n    # Get the tags from the test set\n    predic_tags = np.array([n.cpu().numpy() for n in test_tags])\n\n    # Keep track of J=sum(tags) for each sample\n    J_pred = np.zeros(predic_tags.shape[0])\n    J_true = np.zeros(train_tags.shape[0])\n\n    # Plot the distribution of each tag element\n    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n    for i in range(train_tags.shape[1]):\n        if rescale and recover_tag is not None:\n            recovered_tag_model = np.asarray(\n                [recover_tag[i](predic_tags[:, i], i)]).squeeze(0)\n            recovered_tag_true = np.asarray(\n                [recover_tag[i](train_tags[:, i], i)]).squeeze(0)\n            J_pred += 10**recovered_tag_model\n            J_true += 10**recovered_tag_true\n            axs[i // 4, i % 4].hist(recovered_tag_model,\n                                    bins=nbins,\n                                    color='tab:red',\n                                    density=True,\n                                    alpha=0.5,\n                                    label='Model prediction')\n            axs[i // 4, i % 4].hist(recovered_tag_true,\n                                    bins=nbins,\n                                    color='tab:blue',\n                                    density=True,\n                                    alpha=0.5,\n                                    label='Training data')\n        else:\n            axs[i // 4, i % 4].hist(predic_tags[:, i],\n                                    bins=nbins,\n                                    color='tab:red',\n                                    density=True,\n                                    alpha=0.5,\n                                    label='Model prediction')\n            axs[i // 4, i % 4].hist(train_tags[:, i],\n                                    bins=nbins,\n                                    color='tab:blue',\n                                    density=True,\n                                    alpha=0.5,\n                                    label='Training data')\n        axs[i // 4, i % 4].set_title(f'Layer {i}', fontsize=15)\n        axs[i // 4, i % 4].set_xlabel('Turbulence integral', fontsize=12)\n        axs[i // 4, i % 4].set_ylabel('Frequency', fontsize=12)\n    axs[0, 1].legend()\n    plt.tight_layout()\n    figname = f'{dirname}/{model_name}_tags'\n    if not rescale:\n        figname += '_unscaled'\n    plt.savefig(f'{figname}.{figure_format}')\n    plt.close()\n\n    if rescale and recover_tag is not None:\n        # Also plot the distribution of the sum of the tags\n        fig, axs = plt.subplots(1, 1, figsize=(6, 6))\n        axs.hist(np.log10(J_pred),\n                 bins=nbins,\n                 color='tab:red',\n                 density=True,\n                 alpha=0.5,\n                 label='Model prediction')\n        axs.hist(np.log10(J_true),\n                 bins=nbins,\n                 color='tab:blue',\n                 density=True,\n                 alpha=0.5,\n                 label='Training data')\n        axs.set_title('Sum of J')\n        axs.legend()\n        plt.tight_layout()\n        plt.savefig(f'{dirname}/{model_name}_sumJ.{figure_format}')\n        plt.close()\n</code></pre>"},{"location":"api/preprocess/","title":"Preprocess","text":"<p>This module contains functions for training and evaluating a neural network model using PyTorch. It includes the following key components:</p> <ol> <li><code>train</code>: Trains the model for a specified number of epochs, logs training    and validation losses, and saves the model state at specified intervals.</li> <li><code>score</code>: Evaluates the model on a test dataset, calculates various metrics,    and generates plots for a specified number of test samples.</li> </ol> <p>The module relies on several external utilities and models from the <code>speckcn2</code> package, including <code>EnsembleModel</code>, <code>ComposableLoss</code>, and <code>Normalizer</code>.</p>"},{"location":"api/preprocess/#speckcn2.preprocess.assemble_transform","title":"<code>assemble_transform(conf)</code>","text":"<p>Assembles the transformation to apply to each image.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>transform</code> (              <code>Compose</code> )          \u2013            <p>Transformation to apply to the images</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def assemble_transform(conf: dict) -&gt; transforms.Compose:\n    \"\"\"Assembles the transformation to apply to each image.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n\n    Returns\n    -------\n    transform : torchvision.transforms.Compose\n        Transformation to apply to the images\n    \"\"\"\n    list_transforms = []\n\n    if conf['preproc']['centercrop'] &gt; 0:\n        # Take only the center of the image\n        list_transforms.append(\n            transforms.CenterCrop(conf['preproc']['centercrop']))\n\n    if conf['preproc']['polarize']:\n        # make the image larger for better polar conversion\n        list_transforms.append(transforms.Resize(conf['preproc']['polresize']))\n        # Convert to polar coordinates\n        list_transforms.append(PolarCoordinateTransform())\n\n    if conf['preproc']['randomrotate'] and not conf['preproc']['polarize']:\n        # Randomly rotate the image, since it is symmetric (not if it is polarized)\n        list_transforms.append(transforms.RandomRotation(degrees=(-180, 180)))\n\n    if conf['preproc']['resize']:\n        # Optionally, downscale it\n        list_transforms.append(\n            transforms.Resize(\n                (conf['preproc']['resize'], conf['preproc']['resize'])))\n\n    if conf['preproc']['equivariant'] and conf['preproc']['polarize']:\n        # Apply the equivariant transform, which makes sense only in polar coordinates\n        list_transforms.append(ShiftRowsTransform())\n\n    list_transforms.append(ToUnboundTensor())\n\n    return transforms.Compose(list_transforms)\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.create_average_dataset","title":"<code>create_average_dataset(dataset, average_size)</code>","text":"<p>Creates a dataset of averages from a dataset of single images. The averages are created by grouping together average_size images.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>list</code>)           \u2013            <p>List of single images</p> </li> <li> <code>average_size</code>               (<code>int</code>)           \u2013            <p>The number of images that will be averaged together</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>average_dataset</code> (              <code>list</code> )          \u2013            <p>List of averages</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def create_average_dataset(dataset: list, average_size: int) -&gt; list:\n    \"\"\"Creates a dataset of averages from a dataset of single images. The\n    averages are created by grouping together average_size images.\n\n    Parameters\n    ----------\n    dataset : list\n        List of single images\n    average_size : int\n        The number of images that will be averaged together\n\n    Returns\n    -------\n    average_dataset : list\n        List of averages\n    \"\"\"\n    split_averages: dict = {}\n    for item in dataset:\n        key = item[-1]\n        split_averages.setdefault(key, []).append(item)\n\n    average_dataset: list = []\n    for average in split_averages.values():\n        # * In each average, take n_groups groups of average_size datapoints\n        n_groups = len(average) // average_size\n        if n_groups &lt; 1:\n            raise ValueError(f'Average size {average_size} is too large '\n                             f'for groups with size {len(average)}')\n        # Extract the averages randomly\n        sample = random.sample(average, n_groups * average_size)\n        # Split the sample into groups of average_size\n        list_to_avg = [\n            sample[i:i + average_size]\n            for i in range(0, n_groups * average_size, average_size)\n        ]\n        # Average the groups\n        averages = [\n            tuple(\n                sum(element) / average_size\n                for i, element in enumerate(zip(*group)))\n            for group in list_to_avg\n        ]\n        average_dataset.extend(averages)\n\n    random.shuffle(average_dataset)\n    return average_dataset\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.create_ensemble_dataset","title":"<code>create_ensemble_dataset(dataset, ensemble_size)</code>","text":"<p>Creates a dataset of ensembles from a dataset of single images. The ensembles are created by grouping together ensemble_size images. These images will be used to train the model in parallel.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>list</code>)           \u2013            <p>List of single images</p> </li> <li> <code>ensemble_size</code>               (<code>int</code>)           \u2013            <p>The number of images that will be processed together as an ensemble</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ensemble_dataset</code> (              <code>list</code> )          \u2013            <p>List of ensembles</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def create_ensemble_dataset(dataset: list, ensemble_size: int) -&gt; list:\n    \"\"\"Creates a dataset of ensembles from a dataset of single images. The\n    ensembles are created by grouping together ensemble_size images. These\n    images will be used to train the model in parallel.\n\n    Parameters\n    ----------\n    dataset : list\n        List of single images\n    ensemble_size : int\n        The number of images that will be processed together as an ensemble\n\n    Returns\n    -------\n    ensemble_dataset : list\n        List of ensembles\n    \"\"\"\n    split_ensembles: dict = {}\n    for item in dataset:\n        key = item[-1]\n        split_ensembles.setdefault(key, []).append(item)\n\n    ensemble_dataset: list = []\n    for ensemble in split_ensembles.values():\n        # * In each ensemble, take n_groups groups of ensemble_size datapoints\n        n_groups = len(ensemble) // ensemble_size\n        if n_groups &lt; 1:\n            raise ValueError(f'Ensemble size {ensemble_size} is too large '\n                             f'for ensembles with size {len(ensemble)}')\n        # Extract the ensembles randomly\n        sample = random.sample(ensemble, n_groups * ensemble_size)\n        # Split the sample into groups of ensemble_size\n        ensemble_dataset.extend(sample[i:i + ensemble_size]\n                                for i in range(0, n_groups *\n                                               ensemble_size, ensemble_size))\n\n    random.shuffle(ensemble_dataset)\n    return ensemble_dataset\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.get_ensemble_dict","title":"<code>get_ensemble_dict(tag_files)</code>","text":"<p>Function to associate each Cn2 profile to an ensemble ID for parallel processing.</p> <p>Parameters:</p> <ul> <li> <code>tag_files</code>               (<code>dict</code>)           \u2013            <p>Dictionary of image files and their corresponding tag files</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ensemble_dict</code> (              <code>dict</code> )          \u2013            <p>Dictionary of image files and their corresponding ensemble IDs</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def get_ensemble_dict(tag_files: dict) -&gt; dict:\n    \"\"\"Function to associate each Cn2 profile to an ensemble ID for parallel\n    processing.\n\n    Parameters\n    ----------\n    tag_files : dict\n        Dictionary of image files and their corresponding tag files\n\n    Returns\n    -------\n    ensemble_dict : dict\n        Dictionary of image files and their corresponding ensemble IDs\n    \"\"\"\n    ensembles = {}\n    ensemble_counter = 1\n    for value in tag_files.values():\n        # Check if the value is already assigned an ID\n        if value not in ensembles:\n            # Assign a new ID if it's a new value\n            ensembles[value] = ensemble_counter\n            ensemble_counter += 1\n    return {key: ensembles[value] for key, value in tag_files.items()}\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.get_tag_files","title":"<code>get_tag_files(file_list, datadirectory)</code>","text":"<p>Function to check the existence of tag files for each image file.</p> <p>Parameters:</p> <ul> <li> <code>file_list</code>               (<code>list</code>)           \u2013            <p>List of image files</p> </li> <li> <code>datadirectory</code>               (<code>str</code>)           \u2013            <p>The directory containing the data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tag_files</code> (              <code>dict</code> )          \u2013            <p>Dictionary of image files and their corresponding tag files</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def get_tag_files(file_list: list, datadirectory: str) -&gt; dict:\n    \"\"\"Function to check the existence of tag files for each image file.\n\n    Parameters\n    ----------\n    file_list : list\n        List of image files\n    datadirectory : str\n        The directory containing the data\n\n    Returns\n    -------\n    tag_files : dict\n        Dictionary of image files and their corresponding tag files\n    \"\"\"\n    tag_files = {}\n    for file_name in file_list:\n        if 'MALES' in file_name:\n            ftagname = file_name.replace('.h5', '_tag.h5')\n        elif 'sample' in file_name:\n            ftagname = file_name.rpartition('-')[0] + '_tag.h5'\n        else:\n            ftagname = file_name.rpartition('_')[0] + '_tag.h5'\n\n        tag_path = os.path.join(datadirectory, ftagname)\n        if os.path.exists(tag_path):\n            tag_files[file_name] = tag_path\n        else:\n            print(f'*** Warning: tag file {ftagname} not found.')\n    return tag_files\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.imgs_as_single_datapoint","title":"<code>imgs_as_single_datapoint(conf, nimg_print=5)</code>","text":"<p>Preprocesses the data by loading images and tags from the given directory, applying a transformation to the images. Each image is treated as a single data point.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>nimg_print</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of images to print</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>all_images</code> (              <code>list</code> )          \u2013            <p>List of all images</p> </li> <li> <code>all_tags</code> (              <code>list</code> )          \u2013            <p>List of all tags</p> </li> <li> <code>all_ensemble_ids</code> (              <code>list</code> )          \u2013            <p>List of all ensemble ids, representing images from the same Cn2 profile</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def imgs_as_single_datapoint(\n    conf: dict,\n    nimg_print: int = 5,\n) -&gt; tuple[list, list, list]:\n    \"\"\"Preprocesses the data by loading images and tags from the given\n    directory, applying a transformation to the images. Each image is treated\n    as a single data point.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    nimg_print: int\n        Number of images to print\n\n    Returns\n    -------\n    all_images : list\n        List of all images\n    all_tags : list\n        List of all tags\n    all_ensemble_ids : list\n        List of all ensemble ids, representing images from the same Cn2 profile\n    \"\"\"\n    time_start = time.time()\n    datadirectory = conf['speckle']['datadirectory']\n    mname = conf['model']['name']\n    dataname = conf['preproc']['dataname']\n    tagname = dataname.replace('images', 'tags')\n    ensemblename = dataname.replace('images', 'ensemble')\n    nreps = conf['preproc']['speckreps']\n\n    # Dummy transformation to get the original image\n    transform_orig = transforms.Compose([\n        transforms.ToTensor(),\n    ])\n\n    # and get the transformation to apply to each image\n    transform = assemble_transform(conf)\n\n    # Get the list of images\n    file_list = [\n        file_name for file_name in os.listdir(datadirectory)\n        if '.h5' in file_name and 'tag' not in file_name\n    ]\n    np.random.shuffle(file_list)\n    # Optionally, do data augmentation by using each file multiple times\n    # (It makes sense only in combination with random rotations)\n    file_list = file_list * nreps\n\n    all_images, all_tags, all_ensemble_ids = [], [], []\n    show_image = nimg_print &gt; 0\n    if show_image:\n        ensure_directory(f'{datadirectory}/imgs_to_{mname}')\n\n    tag_files = get_tag_files(file_list, datadirectory)\n    ensemble_dict = get_ensemble_dict(tag_files)\n\n    # Load each text file as an image\n    for counter, file_name in enumerate(file_list):\n        # Process only if tags available\n        if file_name in tag_files:\n\n            # Construct the full path to the file\n            file_path = os.path.join(datadirectory, file_name)\n\n            # Open the HDF5 file\n            print(file_path, flush=True)\n            with h5py.File(file_path, 'r') as f:\n                # Load the data from the 'data' dataset\n                pixel_values = np.float32(f['data'][:])\n                # and replace nans with 0\n                np.nan_to_num(pixel_values, copy=False)\n\n            # Create the image\n            image_orig = Image.fromarray(pixel_values, mode='F')\n\n            # Apply the transformation\n            image = transform(image_orig)\n            if show_image:\n                image_orig = transform_orig(image_orig)\n\n            # and add the img to the collection\n            all_images.append(image)\n\n            # Load the tags\n            with h5py.File(tag_files[file_name], 'r') as f:\n                if 'sample' in file_name:\n                    tags = f['J'][:].reshape(8, 1)\n                else:\n                    tags = f['data'][:]\n\n            # Plot the image using maplotlib\n            if counter &gt; nimg_print:\n                show_image = False\n            if show_image:\n                plot_preprocessed_image(image_orig, image, tags, counter,\n                                        datadirectory, mname, file_name)\n\n            # Preprocess the tags\n            np.log10(tags, out=tags)\n            # Add the tag to the collection\n            all_tags.append(tags.squeeze())\n\n            # Get the ensemble ID\n            ensemble_id = ensemble_dict[file_name]\n            # and add it to the collection\n            all_ensemble_ids.append(ensemble_id)\n\n    # Finally, store them before returning\n    torch.save(all_images, os.path.join(datadirectory, dataname))\n    torch.save(all_tags, os.path.join(datadirectory, tagname))\n    torch.save(all_ensemble_ids, os.path.join(datadirectory, ensemblename))\n\n    print('*** Preprocessing complete.', flush=True)\n    print('It took',\n          time.time() - time_start, 'seconds to preprocess the data.')\n\n    return all_images, all_tags, all_ensemble_ids\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.prepare_data","title":"<code>prepare_data(conf, nimg_print=5)</code>","text":"<p>If not already available, preprocesses the data by loading images and tags from the given directory, applying a transformation to the images.</p> <p>Parameters:</p> <ul> <li> <code>conf</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>nimg_print</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of images to print</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>all_images</code> (              <code>list</code> )          \u2013            <p>List of all images</p> </li> <li> <code>all_tags</code> (              <code>list</code> )          \u2013            <p>List of all tags</p> </li> <li> <code>all_ensemble_ids</code> (              <code>list</code> )          \u2013            <p>List of all ensemble ids, representing images from the same Cn2 profile</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def prepare_data(\n    conf: dict,\n    nimg_print: int = 5,\n) -&gt; tuple[list, list, list]:\n    \"\"\"If not already available, preprocesses the data by loading images and\n    tags from the given directory, applying a transformation to the images.\n\n    Parameters\n    ----------\n    conf : dict\n        Dictionary containing the configuration\n    nimg_print: int\n        Number of images to print\n\n    Returns\n    -------\n    all_images : list\n        List of all images\n    all_tags : list\n        List of all tags\n    all_ensemble_ids : list\n        List of all ensemble ids, representing images from the same Cn2 profile\n    \"\"\"\n    datadirectory = conf['speckle']['datadirectory']\n    dataname = conf['preproc']['dataname']\n    tagname = dataname.replace('images', 'tags')\n    ensemblename = dataname.replace('images', 'ensemble')\n\n    # First, check if the data has already been preprocessed\n    if os.path.exists(os.path.join(datadirectory, dataname)):\n        print(f'*** Loading preprocessed data from {dataname}')\n        # If so, load it\n        all_images = torch.load(os.path.join(datadirectory, dataname),\n                                weights_only=False)\n        all_tags = torch.load(os.path.join(datadirectory, tagname),\n                              weights_only=False)\n        all_ensemble_ids = torch.load(os.path.join(datadirectory,\n                                                   ensemblename),\n                                      weights_only=False)\n        # print the info about the dataset\n        print(f'*** There are {len(all_images)} images in the dataset.')\n    else:\n        # Check if there is at least one image file in the directory\n        if not any('.h5' in file_name\n                   for file_name in os.listdir(datadirectory)):\n            raise FileNotFoundError(\n                'No image files found in the directory. Please provide the '\n                'correct path to the data directory.')\n        # Otherwise, preprocess the raw data separating the single images\n        all_images, all_tags, all_ensemble_ids = imgs_as_single_datapoint(\n            conf, nimg_print)\n\n    # Get the average value of the pixels, excluding the 0 values\n    non_zero_pixels = 0\n    sum_pixels = 0\n    for image in all_images:\n        non_zero_pixels_in_image = image[image != 0]\n        non_zero_pixels += non_zero_pixels_in_image.numel()\n        sum_pixels += torch.sum(non_zero_pixels_in_image)\n    pixel_average = sum_pixels / non_zero_pixels\n    print('*** Pixel average:', pixel_average)\n    # and store it in the config\n    conf['preproc']['pixel_average'] = pixel_average\n\n    return all_images, all_tags, all_ensemble_ids\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.print_average_info","title":"<code>print_average_info(dataset, average_size, ttsplit)</code>","text":"<p>Prints the information about the average dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>list</code>)           \u2013            <p>The average dataset</p> </li> <li> <code>average_size</code>               (<code>int</code>)           \u2013            <p>The number of images in each average</p> </li> <li> <code>ttsplit</code>               (<code>int</code>)           \u2013            <p>The train-test split</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def print_average_info(dataset: list, average_size: int, ttsplit: int):\n    \"\"\"Prints the information about the average dataset.\n\n    Parameters\n    ----------\n    dataset : list\n        The average dataset\n    average_size : int\n        The number of images in each average\n    ttsplit : int\n        The train-test split\n    \"\"\"\n    train_size = int(ttsplit * len(dataset))\n    print(\n        f'*** There are {len(dataset)} average groups in the dataset, '\n        f'that I split in {train_size} for training and '\n        f'{len(dataset) - train_size} for testing. Each average is composed by '\n        f'{average_size} images. This corresponds to {train_size * average_size} '\n        f'for training and {(len(dataset) - train_size) * average_size} for testing.'\n    )\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.print_dataset_info","title":"<code>print_dataset_info(dataset, ttsplit)</code>","text":"<p>Prints the information about the dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>list</code>)           \u2013            <p>The dataset</p> </li> <li> <code>ttsplit</code>               (<code>int</code>)           \u2013            <p>The train-test split</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def print_dataset_info(dataset: list, ttsplit: int):\n    \"\"\"Prints the information about the dataset.\n\n    Parameters\n    ----------\n    dataset : list\n        The dataset\n    ttsplit : int\n        The train-test split\n    \"\"\"\n    train_size = int(ttsplit * len(dataset))\n    print(f'*** There are {len(dataset)} images in the dataset, '\n          f'{train_size} for training and '\n          f'{len(dataset) - train_size} for testing.')\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.print_ensemble_info","title":"<code>print_ensemble_info(dataset, ensemble_size, ttsplit)</code>","text":"<p>Prints the information about the ensemble dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>list</code>)           \u2013            <p>The ensemble dataset</p> </li> <li> <code>ensemble_size</code>               (<code>int</code>)           \u2013            <p>The number of images in each ensemble</p> </li> <li> <code>ttsplit</code>               (<code>int</code>)           \u2013            <p>The train-test split</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def print_ensemble_info(dataset: list, ensemble_size: int, ttsplit: int):\n    \"\"\"Prints the information about the ensemble dataset.\n\n    Parameters\n    ----------\n    dataset : list\n        The ensemble dataset\n    ensemble_size : int\n        The number of images in each ensemble\n    ttsplit : int\n        The train-test split\n    \"\"\"\n\n    train_size = int(ttsplit * len(dataset))\n    print(\n        f'*** There are {len(dataset)} ensemble groups in the dataset, '\n        f'that I split in {train_size} for training and '\n        f'{len(dataset) - train_size} for testing. Each ensemble is composed by '\n        f'{ensemble_size} images. This corresponds to {train_size * ensemble_size} '\n        f'for training and {(len(dataset) - train_size) * ensemble_size} for testing.'\n    )\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.split_dataset","title":"<code>split_dataset(dataset, ttsplit)</code>","text":"<p>Splits the dataset into training and testing sets.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>list</code>)           \u2013            <p>The dataset</p> </li> <li> <code>ttsplit</code>               (<code>int</code>)           \u2013            <p>The train-test split</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>train_set</code> (              <code>list</code> )          \u2013            <p>The training set</p> </li> <li> <code>test_set</code> (              <code>list</code> )          \u2013            <p>The testing set</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def split_dataset(dataset: list, ttsplit: int) -&gt; tuple[list, list]:\n    \"\"\"Splits the dataset into training and testing sets.\n\n    Parameters\n    ----------\n    dataset : list\n        The dataset\n    ttsplit : int\n        The train-test split\n\n    Returns\n    -------\n    train_set : list\n        The training set\n    test_set : list\n        The testing set\n    \"\"\"\n    # First shuffle the dataset\n    random.shuffle(dataset)\n    train_size = int(ttsplit * len(dataset))\n    return dataset[:train_size], dataset[train_size:]\n</code></pre>"},{"location":"api/preprocess/#speckcn2.preprocess.train_test_split","title":"<code>train_test_split(all_images, all_tags, all_ensemble_ids, nz)</code>","text":"<p>Splits the data into training and testing sets.</p> <p>Parameters:</p> <ul> <li> <code>all_images</code>               (<code>list</code>)           \u2013            <p>List of images</p> </li> <li> <code>all_tags</code>               (<code>list</code>)           \u2013            <p>List of tags</p> </li> <li> <code>all_ensemble_ids</code>               (<code>list</code>)           \u2013            <p>List of ensemble ids</p> </li> <li> <code>nz</code>               (<code>Normalizer</code>)           \u2013            <p>The normalizer object to preprocess the data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>train_set</code> (              <code>list</code> )          \u2013            <p>Training dataset</p> </li> <li> <code>test_set</code> (              <code>list</code> )          \u2013            <p>Testing dataset</p> </li> </ul> Source code in <code>src/speckcn2/preprocess.py</code> <pre><code>def train_test_split(\n    all_images: list[torch.tensor],\n    all_tags: list[np.ndarray],\n    all_ensemble_ids: list[int],\n    nz: Normalizer,\n) -&gt; tuple[list, list]:\n    \"\"\"Splits the data into training and testing sets.\n\n    Parameters\n    ----------\n    all_images : list\n        List of images\n    all_tags : list\n        List of tags\n    all_ensemble_ids : list\n        List of ensemble ids\n    nz: Normalizer\n        The normalizer object to preprocess the data\n\n    Returns\n    -------\n    train_set : list\n        Training dataset\n    test_set : list\n        Testing dataset\n    \"\"\"\n    # Get the config dict\n    config = nz.conf\n    # extract the model parameters\n    modelname = config['model']['name']\n    datadirectory = config['speckle']['datadirectory']\n    ttsplit = config['hyppar'].get('ttsplit', 0.8)\n    ensemble_size = config['preproc'].get('ensemble', 1)\n    average_size = config['preproc'].get('average', 0)\n\n    # Check if the training and test set are already prepared\n    train_file = f'{datadirectory}/train_set_{modelname}.pickle'\n    test_file = f'{datadirectory}/test_set_{modelname}.pickle'\n\n    if os.path.isfile(train_file) and os.path.isfile(test_file):\n        print('Loading the training and testing set...', flush=True)\n        nz._normalizing_functions(all_images, all_tags, all_ensemble_ids)\n        train_data = pickle.load(open(train_file, 'rb'))\n        test_data = pickle.load(open(test_file, 'rb'))\n        print(f'*** There are {len(train_data)} images in the training set, ')\n        print(f'*** and {len(test_data)} images in the testing set.')\n        return train_data, test_data\n\n    # If the data are not already prepared, first I normalize them using the Normalizer object\n    print('Normalizing the images and tags...', flush=True)\n    dataset = nz.normalize_imgs_and_tags(all_images, all_tags,\n                                         all_ensemble_ids)\n\n    if average_size &gt; 1 and ensemble_size &gt; 1:\n        raise ValueError(\n            'The average_size and ensemble_size cannot be set at the same time.'\n        )\n    elif average_size &gt; 1:\n        dataset = create_average_dataset(dataset, average_size)\n        print_average_info(dataset, average_size, ttsplit)\n    elif ensemble_size &gt; 1:\n        dataset = create_ensemble_dataset(dataset, ensemble_size)\n        print_ensemble_info(dataset, ensemble_size, ttsplit)\n    else:\n        print_dataset_info(dataset, ttsplit)\n\n    train_set, test_set = split_dataset(dataset, ttsplit)\n\n    pickle.dump(train_set, open(train_file, 'wb'))\n    pickle.dump(test_set, open(test_file, 'wb'))\n\n    return train_set, test_set\n</code></pre>"},{"location":"api/scnn/","title":"SCNN","text":"<p>This module defines a Steerable Convolutional Neural Network (SteerableCNN) using the escnn library for equivariant neural networks. The primary class, SteerableCNN, allows for the creation of a convolutional neural network that handles various symmetries, making it useful for tasks requiring rotational invariance.</p> <p>The module imports necessary libraries, including torch and escnn, and defines utility functions such as <code>create_block</code>, <code>compute_new_features</code>, <code>create_pool</code>, and <code>create_final_block</code>. These functions help construct convolutional blocks, calculate feature map sizes, create anti-aliased pooling layers, and build fully connected layers.</p> <p>The SteerableCNN class initializes with a configuration dictionary and a symmetry parameter, setting up parameters like kernel sizes, paddings, strides, and feature fields. It determines the symmetry group and initializes the input type for the network.</p> <p>The network is built by iterating through specified kernel sizes, creating convolutional blocks and pooling layers, adding a group pooling layer for invariance, and creating final fully connected layers using <code>create_final_block</code>.</p> <p>The forward method processes the input tensor through the network, applying each equivariant block, performing group pooling, and classifying the output using the fully connected layers.</p> <p>Overall, this module provides a flexible framework for building steerable convolutional neural networks with configurable symmetries and architectures.</p>"},{"location":"api/scnn/#speckcn2.scnn.create_final_block","title":"<code>create_final_block(config, n_initial, nscreens)</code>","text":"<p>Creates a fully connected neural network block based on a predefined configuration.</p> <p>This function dynamically creates a sequence of PyTorch layers for a fully connected neural network. The configuration for the layers is read from a global <code>config</code> dictionary which should contain a 'final_block' key with a list of layer configurations. Each layer configuration is a dictionary that must include a 'type' key with the name of the layer class (e.g., 'Linear', 'Dropout', etc.) and can include additional keys for the layer parameters.</p> <p>The first 'Linear' layer in the configuration has its number of input features set to <code>n_in</code>, and any 'Linear' layer with 'out_features' set to 'nscreens' has its number of output features set to <code>nscreens</code>.</p> <p>Args:</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>The global configuration dictionary containing the layer configurations.</p> </li> <li> <code>n_initial</code>               (<code>int</code>)           \u2013            <p>The number of input features for the first 'Linear' layer.</p> </li> <li> <code>nscreens</code>               (<code>int</code>)           \u2013            <p>The number of output features for any 'Linear' layer with 'out_features' set to 'nscreens'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.nn.Sequential: A sequential container of the configured PyTorch layers.</code>           \u2013            </li> </ul> Source code in <code>src/speckcn2/scnn.py</code> <pre><code>def create_final_block(config: dict, n_initial: int,\n                       nscreens: int) -&gt; nn.Sequential:\n    \"\"\"Creates a fully connected neural network block based on a predefined\n    configuration.\n\n    This function dynamically creates a sequence of PyTorch layers for a fully connected\n    neural network. The configuration for the layers is read from a global `config` dictionary\n    which should contain a 'final_block' key with a list of layer configurations. Each layer\n    configuration is a dictionary that must include a 'type' key with the name of the layer\n    class (e.g., 'Linear', 'Dropout', etc.) and can include additional keys for the layer\n    parameters.\n\n    The first 'Linear' layer in the configuration has its number of input features set to `n_in`,\n    and any 'Linear' layer with 'out_features' set to 'nscreens' has its number of output features\n    set to `nscreens`.\n\n    Args:\n    Parameters\n    ----------\n    config : dict\n        The global configuration dictionary containing the layer configurations.\n    n_initial : int\n        The number of input features for the first 'Linear' layer.\n    nscreens : int\n        The number of output features for any 'Linear' layer with 'out_features' set to 'nscreens'.\n\n    Returns\n    ----------\n    torch.nn.Sequential: A sequential container of the configured PyTorch layers.\n    \"\"\"\n    layers = []\n\n    for layer_config in config['final_block']:\n        layer_type = layer_config.pop('type')\n\n        if layer_type == 'Linear':\n            # The first linear layer has a constrained number of input features\n            if n_initial != -1:\n                layer_config['in_features'] = n_initial\n                n_initial = -1\n            if layer_config['out_features'] == 'nscreens':\n                layer_config['out_features'] = nscreens\n            # Pass the number of features as args\n            n_in = layer_config.pop('in_features')\n            n_out = layer_config.pop('out_features')\n\n            layers.append(torch.nn.Linear(n_in, n_out, **layer_config))\n        else:\n            layer_class = getattr(torch.nn, layer_type)\n            layers.append(layer_class(**layer_config))\n\n    return torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"api/transformations/","title":"Transformations","text":"<p>This module defines several image transformation classes using PyTorch and NumPy.</p> <p>The <code>PolarCoordinateTransform</code> class converts a Cartesian image to polar coordinates, which can be useful for certain types of image analysis. The <code>ShiftRowsTransform</code> class shifts the rows of an image so that the row with the smallest sum is positioned at the bottom, which can help in aligning images for further processing. The <code>ToUnboundTensor</code> class converts an image to a tensor without normalizing it, preserving the original pixel values. Lastly, the <code>SpiderMask</code> class applies a circular mask to the image, simulating the effect of a spider by setting pixels outside the mask to a background value, which can be useful in certain experimental setups.</p>"},{"location":"api/transformations/#speckcn2.transformations.PolarCoordinateTransform","title":"<code>PolarCoordinateTransform()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transform a Cartesian image to polar coordinates.</p> Source code in <code>src/speckcn2/transformations.py</code> <pre><code>def __init__(self):\n    super(PolarCoordinateTransform, self).__init__()\n</code></pre>"},{"location":"api/transformations/#speckcn2.transformations.PolarCoordinateTransform.forward","title":"<code>forward(img)</code>","text":"<p>forward method of the transform Args:     img (PIL Image or Tensor): Image to be scaled.</p> <p>Returns:     PIL Image or Tensor: Rescaled image.</p> Source code in <code>src/speckcn2/transformations.py</code> <pre><code>def forward(self, img):\n    \"\"\" forward method of the transform\n    Args:\n        img (PIL Image or Tensor): Image to be scaled.\n\n    Returns:\n        PIL Image or Tensor: Rescaled image.\n    \"\"\"\n\n    img = np.array(img)  # Convert PIL image to NumPy array\n\n    # Assuming img is a grayscale image\n    height, width = img.shape\n    polar_image = np.zeros_like(img)\n\n    # Center of the image\n    center_x, center_y = width // 2, height // 2\n\n    # Maximum possible value of r\n    max_r = np.sqrt(center_x**2 + center_y**2)\n\n    # Create a grid of (x, y) coordinates\n    y, x = np.ogrid[:height, :width]\n\n    # Shift the grid so that the center of the image is at (0, 0)\n    x = x - center_x\n    y = y - center_y\n\n    # Convert Cartesian to Polar coordinates\n    r = np.sqrt(x**2 + y**2)\n    theta = np.arctan2(y, x)\n\n    # Rescale r to [0, height)\n    r = np.round(r * (height - 1) / max_r).astype(int)\n\n    # Rescale theta to [0, width)\n    theta = np.round((theta + 2 * np.pi) % (2 * np.pi) * (width - 1) /\n                     (2 * np.pi)).astype(int)\n\n    # Use a 2D histogram to accumulate all values that map to the same polar coordinate\n    histogram, _, _ = np.histogram2d(theta.flatten(),\n                                     r.flatten(),\n                                     bins=[height, width],\n                                     range=[[0, height], [0, width]],\n                                     weights=img.flatten())\n\n    # Count how many Cartesian coordinates map to each polar coordinate\n    counts, _, _ = np.histogram2d(theta.flatten(),\n                                  r.flatten(),\n                                  bins=[height, width],\n                                  range=[[0, height], [0, width]])\n\n    # Take the average of all values that map to the same polar coordinate\n    polar_image = histogram / counts\n\n    # Handle any divisions by zero\n    polar_image[np.isnan(polar_image)] = 0\n\n    # Crop the large r part that is not used\n    for x in range(width - 1, -1, -1):\n        # If the column contains at least one non-black pixel\n        if np.any(polar_image[:, x] != 0):\n            # Crop at this x position\n            polar_image = polar_image[:, :x]\n            break\n\n    # reconvert to PIL image before returning\n    return Image.fromarray(polar_image)\n</code></pre>"},{"location":"api/transformations/#speckcn2.transformations.ShiftRowsTransform","title":"<code>ShiftRowsTransform()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Shift the rows of an image such that the row with the smallest sum is at the bottom.</p> Source code in <code>src/speckcn2/transformations.py</code> <pre><code>def __init__(self):\n    super(ShiftRowsTransform, self).__init__()\n</code></pre>"},{"location":"api/transformations/#speckcn2.transformations.SpiderMask","title":"<code>SpiderMask()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Apply a circular mask to the image, representing the effect of the spider.</p> <p>The pixels outside the spider are set to -0.01, such that their value is lower than no light in the detector (0).</p> Source code in <code>src/speckcn2/transformations.py</code> <pre><code>def __init__(self):\n    super(SpiderMask, self).__init__()\n</code></pre>"},{"location":"api/transformations/#speckcn2.transformations.ToUnboundTensor","title":"<code>ToUnboundTensor()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transform the image into a tensor, but do not normalize it like torchvision.ToTensor.</p> Source code in <code>src/speckcn2/transformations.py</code> <pre><code>def __init__(self):\n    super(ToUnboundTensor, self).__init__()\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":"<p>This module provides utility functions for image processing and model optimization.</p> <p>It includes functions to plot original and preprocessed images along with their tags, ensure the existence of specified directories, set up optimizers based on configuration files, and create circular masks with an inner \"spider\" circle removed. These utilities facilitate various tasks in image analysis and machine learning model training.</p>"},{"location":"api/utils/#speckcn2.utils.create_circular_mask_with_spider","title":"<code>create_circular_mask_with_spider(resolution, bkg_value=0)</code>","text":"<p>Creates a circular mask with an inner \"spider\" circle removed.</p> <p>Parameters:</p> <ul> <li> <code>resolution</code>               (<code>int</code>)           \u2013            <p>The resolution of the square mask.</p> </li> <li> <code>bkg_value</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The background value to set for the masked areas. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor : np.ndarray</code>           \u2013            <p>A 2D tensor representing the mask.</p> </li> </ul> Source code in <code>src/speckcn2/utils.py</code> <pre><code>def create_circular_mask_with_spider(resolution: int,\n                                     bkg_value: int = 0) -&gt; torch.Tensor:\n    \"\"\"Creates a circular mask with an inner \"spider\" circle removed.\n\n    Parameters\n    ----------\n    resolution : int\n        The resolution of the square mask.\n    bkg_value : int\n        The background value to set for the masked areas. Defaults to 0.\n\n    Returns\n    -------\n    torch.Tensor : np.ndarray\n        A 2D tensor representing the mask.\n    \"\"\"\n    # Create a circular mask\n    center = (int(resolution / 2), int(resolution / 2))\n    radius = min(center)\n    Y, X = np.ogrid[:resolution, :resolution]\n    mask = (X - center[0])**2 + (Y - center[1])**2 &gt; radius**2\n\n    # Remove the inner circle (spider)\n    spider_radius = int(0.22 * resolution)\n    spider_mask = (X - center[0])**2 + (Y - center[1])**2 &lt; spider_radius**2\n\n    # Apply background value to the mask and spider mask\n    final_mask = np.ones((resolution, resolution), dtype=np.uint8)\n    final_mask[mask] = bkg_value\n    final_mask[spider_mask] = bkg_value\n\n    return torch.Tensor(final_mask)\n</code></pre>"},{"location":"api/utils/#speckcn2.utils.ensure_directory","title":"<code>ensure_directory(data_directory)</code>","text":"<p>Ensure that the directory exists.</p> <p>Parameters:</p> <ul> <li> <code>data_directory</code>               (<code>str</code>)           \u2013            <p>The directory to ensure</p> </li> </ul> Source code in <code>src/speckcn2/utils.py</code> <pre><code>def ensure_directory(data_directory: str) -&gt; None:\n    \"\"\"Ensure that the directory exists.\n\n    Parameters\n    ----------\n    data_directory : str\n        The directory to ensure\n    \"\"\"\n\n    if not os.path.isdir(data_directory):\n        os.mkdir(data_directory)\n</code></pre>"},{"location":"api/utils/#speckcn2.utils.plot_preprocessed_image","title":"<code>plot_preprocessed_image(image_orig, image, tags, counter, datadirectory, mname, file_name, polar=False)</code>","text":"<p>Plots the original and preprocessed image, and the tags.</p> <p>Parameters:</p> <ul> <li> <code>image_orig</code>               (<code>tensor</code>)           \u2013            <p>The original image</p> </li> <li> <code>image</code>               (<code>tensor</code>)           \u2013            <p>The preprocessed image</p> </li> <li> <code>tags</code>               (<code>tensor</code>)           \u2013            <p>The screen tags</p> </li> <li> <code>counter</code>               (<code>int</code>)           \u2013            <p>The counter of the image</p> </li> <li> <code>datadirectory</code>               (<code>str</code>)           \u2013            <p>The directory containing the data</p> </li> <li> <code>mname</code>               (<code>str</code>)           \u2013            <p>The name of the model</p> </li> <li> <code>file_name</code>               (<code>str</code>)           \u2013            <p>The name of the original image</p> </li> <li> <code>polar</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If the image is in polar coordinates, by default False</p> </li> </ul> Source code in <code>src/speckcn2/utils.py</code> <pre><code>def plot_preprocessed_image(image_orig: torch.tensor,\n                            image: torch.tensor,\n                            tags: torch.tensor,\n                            counter: int,\n                            datadirectory: str,\n                            mname: str,\n                            file_name: str,\n                            polar: bool = False) -&gt; None:\n    \"\"\"Plots the original and preprocessed image, and the tags.\n\n    Parameters\n    ----------\n    image_orig : torch.tensor\n        The original image\n    image : torch.tensor\n        The preprocessed image\n    tags : torch.tensor\n        The screen tags\n    counter : int\n        The counter of the image\n    datadirectory : str\n        The directory containing the data\n    mname : str\n        The name of the model\n    file_name : str\n        The name of the original image\n    polar : bool, optional\n        If the image is in polar coordinates, by default False\n    \"\"\"\n\n    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n    # Plot the original image\n    axs[0].imshow(image_orig.squeeze(), cmap='bone')\n    axs[0].set_title(f'Training Image {file_name}')\n    # Plot the preprocessd image\n    axs[1].imshow(image.squeeze(), cmap='bone')\n    axs[1].set_title('Processed as')\n    if polar:\n        axs[1].set_xlabel(r'$r$')\n        axs[1].set_ylabel(r'$\\theta$')\n\n    # Plot the tags\n    axs[2].plot(tags, 'o')\n    axs[2].set_yscale('log')\n    axs[2].set_title('Layer Tags')\n    axs[2].legend()\n\n    fig.subplots_adjust(wspace=0.3)\n    plt.savefig(f'{datadirectory}/imgs_to_{mname}/{counter}.png')\n    plt.close()\n</code></pre>"},{"location":"api/utils/#speckcn2.utils.setup_optimizer","title":"<code>setup_optimizer(config, model)</code>","text":"<p>Returns the optimizer specified in the configuration file.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing the configuration</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>The model to optimize</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>optimizer</code> (              <code>Module</code> )          \u2013            <p>The optimizer with the loaded state</p> </li> </ul> Source code in <code>src/speckcn2/utils.py</code> <pre><code>def setup_optimizer(config: dict, model: nn.Module) -&gt; nn.Module:\n    \"\"\"Returns the optimizer specified in the configuration file.\n\n    Parameters\n    ----------\n    config : dict\n        Dictionary containing the configuration\n    model : torch.nn.Module\n        The model to optimize\n\n    Returns\n    -------\n    optimizer : torch.nn.Module\n        The optimizer with the loaded state\n    \"\"\"\n\n    optimizer_name = config['hyppar']['optimizer']\n    if optimizer_name == 'Adam':\n        return torch.optim.Adam(model.parameters(), lr=config['hyppar']['lr'])\n    elif optimizer_name == 'SGD':\n        return torch.optim.SGD(model.parameters(), lr=config['hyppar']['lr'])\n    else:\n        raise ValueError(f'Unknown optimizer {optimizer_name}')\n</code></pre>"},{"location":"examples/input_data/","title":"Example: Input data","text":"<p>The <code>speckcn2</code> tool expects a set of 2D black-and-white squared images along with their corresponding prediction labels. In its main application, these images are speckle patterns, and the values represent turbulence strength. However, the tool can be adapted for any image regression task, as long as the data is provided in the correct format.</p> <p>Given the large amount of data, we use hdf5 format for optimal storage. You can find a small dataset in the example submodule (<code>./data</code>), that you can use for testing.</p> <p>Each image should be saved in a separate hdf5 file named <code>&lt;NAME&gt;-&lt;ID&gt;.h5</code>. The corresponding prediction value should be stored in a file named <code>&lt;NAME&gt;_label.h5</code>. Images with no corresponding label will be ignored. Here, <code>&lt;NAME&gt;</code> is a common name for the dataset, so we can use <code>&lt;NAME&gt;='speckle_xxx'</code> for speckle patterns. Instead, <code>&lt;ID&gt;</code> is an identifier that takes into account the fact that different input data can correspond to the same turbulence labels, so we have <code>speckle_A-1.h5</code> and <code>speckle_A-2.h5</code> but only a single <code>speckle_A_label.h5</code> that is the turbulence strength for both images.</p> <p>In terms of dimensions, the images are stored as 2D arrays of size \\(N\\times N\\), while the labels are stored as 1D arrays of size \\(M\\). This structure ensures that the data is organized and easily accessible for processing and analysis.</p> <p>\u24d8 config.yaml:  In order to process the data in the correct format, the <code>config.yaml</code> file should contain the following information: <pre><code>speckle:\n  nscreens: M # This is the number of labels that you want to predict for each image\n  original_res: N # This is the resolution of the images\n</code></pre></p>"},{"location":"examples/run/","title":"Example: How to run SpeckleCn2Profiler","text":"<p>Here you find examples on the usage of SpeckleCn2Profiler. This page is divided in two sections, corresponding to the two files that the user has to control to use <code>speckcn2</code>:  </p> <ol> <li> <p>run.py</p> </li> <li> <p>configuration.yaml</p> </li> </ol> <p>Once you have SpeckleCn2Profiler installed, you will run your workflow as:</p> <pre><code>python run.py configuration.yaml\n</code></pre> <p>So let's show you how to set up the <code>run.py</code> and <code>configuration.yaml</code> files.</p>"},{"location":"examples/run/#run-file-explanation","title":"Run File Explanation","text":"<p>The first step of the run is to load the configuration file and the required packages:</p> <p><pre><code>import speckcn2 as sp2\n\nconfig = sp2.load_config(conf_name)\n</code></pre> then you usually want to load and preprocess the data: <pre><code>all_images, all_tags, all_ensemble_ids = sp2.prepare_data(config,\n                                                          nimg_print=15)\nnz = sp2.Normalizer(config)\ntrain_set, test_set = sp2.train_test_split(all_images, all_tags,\n                                           all_ensemble_ids, nz)\n</code></pre> after that you have to define the loss function and the optimizer: <pre><code>criterion = sp2.ComposableLoss(config, nz, device)\ncriterion = criterion.to(device)\noptimizer = sp2.setup_optimizer(config, model)\ncriterion_val = sp2.ComposableLoss(config, nz, device, validation=True)\n</code></pre></p> <p>Then you can load or create the model and train it: <pre><code>model, last_model_state = sp2.setup_model(config)\nmodel, average_loss = sp2.train(model, last_model_state, config, train_set,\n                                test_set, device, optimizer, criterion, criterion_val)\nprint(f'Finished Training, Loss: {average_loss:.5f}', flush=True)\n</code></pre> Once the training is done, you can <code>score</code> the model evaluating its performance on the test set and measuring the desired observables. If you want to do only inference and the model is already trained, you can skip the previous part. An example of a postprocessing pipeline starts with scoring the model: <pre><code>test_tags, test_losses, test_measures, test_cn2_pred, test_cn2_true, test_recovered_tag_pred, test_recovered_tag_true = sp2.score(\n    model, test_set, device, criterion, nz, nimg_plot=0)\n</code></pre></p>"},{"location":"examples/run/#plotting","title":"Plotting","text":"<p>After the model is trained, you can plot the results. Here are some examples:</p> <p><pre><code># Plot the distribution of the screen tags\nsp2.plot_J_error_details(config, test_recovered_tag_true, test_recovered_tag_pred)\n</code></pre> <pre><code># Plot the distribution of the screen tags with bin resolved details\nsp2.screen_errors(config, device, test_recovered_tag_pred, test_recovered_tag_true, nbins=20)\n</code></pre> <pre><code># Overview of the tags distribution and how they are predicted\nsp2.tags_distribution(config,\n                      train_set,\n                      test_tags,\n                      device,\n                      rescale=True,\n                      recover_tag=nz.recover_tag)\n</code></pre> <pre><code># Plot the histograms of the loss function\nsp2.plot_histo_losses(config, test_losses, datadirectory)\n</code></pre> <pre><code># Plot the loss during training\nsp2.plot_loss(config, model, datadirectory)\n</code></pre> <pre><code># Plot the execution time\nsp2.plot_time(config, model, datadirectory)\n</code></pre> <pre><code># Plot histograms of the different parameters\nsp2.plot_param_histo(config, test_losses, datadirectory, test_measures)\n</code></pre> <pre><code># Plot the parameters of the model vs the loss\nsp2.plot_param_vs_loss(config, test_losses, datadirectory, test_measures)\n</code></pre> <pre><code># Test to see if averaging over speckle patterns improves the results\nsp2.average_speckle_input(config, test_set, device, model, criterion, n_ensembles_to_plot=5)\n</code></pre> <pre><code># Test to see if averaging over speckle patterns improves the results\nsp2.average_speckle_output(config, test_set, device, model, criterion, trimming=0.2, n_ensembles_to_plot=20)\n</code></pre> </p> <p>Refer to the documentation or one of the examples if you want to understand and customize your workflow.</p>"},{"location":"examples/run/#configuration-file-explanation","title":"Configuration File Explanation","text":"<p>Here we explain what it is expected in a typical <code>configuration.yaml</code> file. Notice that many fields are optional and have default values, so you can start with a minimal configuration file and add more details as you need them. In the example submodule you can find multiple examples and multiple configuration to take inspiration from.</p> <p>A typical configuration file is divided in the following sections:</p>"},{"location":"examples/run/#speckle","title":"speckle","text":"<ul> <li>nscreens: The number of screens used in the simulation.</li> <li>hArray: array corresponding to the altitudes of the screens.</li> <li>split: The distance from the next screen.</li> <li>lambda: The wavelength of the laser.</li> <li>original_res: The original resolution of the images.</li> <li>datadirectory: The directory where the data files are located.</li> </ul>"},{"location":"examples/run/#preproc","title":"preproc","text":"<ul> <li>polarize: A boolean value indicating whether the images should be transformed into polar coordinate.</li> <li>polresize: The size to which the polarized images are resized.</li> <li>equivariant: A boolean value indicating whether the images should be made pseudo-equivariant, by setting the azimutal angle to the maximum pixel intensity.</li> <li>randomrotate: A boolean value indicating whether the images should be randomly rotated.</li> <li>centercrop: The size of the central crop of the images. Test this value to guarantee that the empty boundaries are removed.</li> <li>resize: The size to which the images are resized.</li> <li>speckreps: The number of times that we want to repeat each speckle pattern in order to augment the data. Use only in combination with random rotations.</li> <li>ensemble: The number of speckle patterns to use as ensemble. This is to train multi-shoot models.</li> <li>ensemble_unif: A boolean value indicating whether the ensemble is uniformly sampled.</li> <li>normalization: How to normalize the tags: <code>unif</code>, <code>lin</code>, <code>log</code> or <code>zscore</code>.</li> <li>img_normalization: if <code>true</code> normalize the pixel values of the images.</li> <li>dataname: The name of the file where the preprocessed images are saved.</li> <li>XXX_details: If <code>true</code> then plot the bin resolved details of <code>XXX</code> metrics</li> </ul>"},{"location":"examples/run/#noise","title":"noise","text":"<ul> <li>D: Fraction of the field.</li> <li>t: Width of the spider.</li> <li>snr: Signal to noise ratio.</li> <li>dT: Telescope diameter.</li> <li>dO: Fraction of obscuration.</li> <li>rn: Amplitude of the noise.</li> <li>fw: Full well capacity.</li> <li>bit: Bit level (sample depth).</li> <li>discretize: A boolean value indicating whether the images should be discretized.</li> <li>rotation_sym: Values in degrees of the rotation symmetry of the pattern. It is defined in the noise section since random rotations multiple of this value are applied to the images.</li> </ul>"},{"location":"examples/run/#model","title":"model","text":"<ul> <li>name: String representing the name of the model. Used to store states and plots. It can be any name.</li> <li>type: The type of the model. We have implemented <code>resnet18</code>, <code>resnet50</code>, <code>resnet152</code> from the ResNet family, and <code>scnnC8</code>, <code>scnnC16</code>, <code>small_scnnC16</code>, which are equivariant CNN.</li> <li>save_every: The frequency (in epochs) at which the model is saved.</li> <li>pretrained: A boolean value indicating whether a pretrained model should be used. It is available only for the ResNet.</li> </ul>"},{"location":"examples/run/#hyppar","title":"hyppar","text":"<ul> <li>maxepochs: The maximum number of epochs for training the model.</li> <li>batch_size: The size of the batches used in training.</li> <li>lr: The learning rate for the optimizer.</li> <li>lr_scheduler: The learning rate scheduler used in training. We have implemented <code>StepLR</code>, <code>ReduceLROnPlateau</code> and <code>CosineAnnealingLR</code>.</li> <li>loss: The loss function used in training. We have implemented <code>MSELoss</code>, <code>BCELoss</code> and <code>Pearson</code>.</li> <li>early_stopping: The number of epochs of plateau to wait before stopping the training.</li> <li>optimizer: The optimizer used in training.</li> </ul>"},{"location":"examples/run/#loss","title":"loss","text":"<ul> <li>XXX: The weight of the loss <code>XXX</code> in the total loss. The total loss is the sum of all the losses weighted by their respective weights. A value of 0 means that the loss is not used. The list of available losses includes:</li> <li>This section of the YAML file defines the configuration for different loss functions used in a machine learning model. Each key represents a specific type of loss function, and the corresponding value indicates whether that loss function is enabled (1) or disabled (0). Here is a brief explanation of each loss function:</li> <li>MAE (Mean Absolute Error): Measures the average magnitude of errors between predicted and actual values.</li> <li>MSE (Mean Squared Error): Measures the average of the squares of the errors between predicted and actual values.</li> <li>JMAE: A variant of MAE that is computed over J, so the screen tag are reconstructed before the evaluation.</li> <li>JMSE: A variant of MSE that is computed over J, so the screen tag are reconstructed before the evaluation.</li> <li>Cn2MAE: Mean Absolute Error specific to Cn2 (a parameter related to atmospheric turbulence).</li> <li>Cn2MSE: Mean Squared Error specific to Cn2.</li> <li>Pearson: Measures the Pearson correlation coefficient between predicted and actual values.</li> <li>Fried: A loss function related to the Fried parameter, which is used in optical turbulence.</li> <li>Isoplanatic: A loss function related to the isoplanatic angle, another parameter in optical turbulence.</li> <li>Rytov: A loss function related to the Rytov variance, which is used in wave propagation.</li> <li>Scintillation_w: A loss function related to the weak scintillation index.</li> <li>Scintillation_ms: A loss function related to the medium-strong scintillation index.</li> </ul>"},{"location":"examples/run/#val_loss","title":"val_loss","text":"<p>Same structure as the loss section, but for the validation loss measured at each step over the test set.</p>"},{"location":"examples/run/#scnn","title":"scnn","text":"<p>This block is used to define the architecture of the SCNN models. The keys are the names of the layers, and the values are the number of filters in each layer. The SCNN models are equivariant CNNs that are used to process the speckle images. A reference example is provided in the <code>configuration.yaml</code> file.</p>"},{"location":"examples/run/#final_block","title":"final_block","text":"<p>This block is used to define the architecture of the final block of any model to get from the image features to the tag predictions. This block has usually a fully connected structure, but here you can control the number of layers, the number of neurons in each layer, regularization, activation, ecc. A reference example is provided in the <code>configuration.yaml</code> file.</p>"}]}